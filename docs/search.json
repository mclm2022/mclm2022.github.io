[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods of Corpus Linguistics",
    "section": "",
    "text": "This website compiles course materials for the Advanced Masters course “Methods of Corpus Linguistics” (September-December 2022, Faculty of Arts, KU Leuven). It includes slides as well as some examples of analyses.\nMost of the code relies on the {mclm} package.\n\n\n\n\nFAQ\nHere are some questions that have come up frequently and that you might have as well. For Git(Hub) issues, please first check the Cheatsheet.\nI cannot run remotes::install_github(\"masterclm/mclm\").\nIt might be that you have not installed the remotes package (which is understandable, since I forgot to say so). Please run install.packages(\"remotes\") and then try again.\nI cannot read the corpus.\nPlease check the path you’re using to read it! If you copied code from slides or notebooks in this website, the path to a corpus directory will be something like here::here(\"studies\", \"_corpora\", \"brown\"), because in the R project of this website the corpora are inside a folder called “_corpora”, inside the folder called “studies”. In your own code, you should adapt the here::here() call to the path to your own corpus. For example, if you use the “brown” corpus and it’s stored in the top level of your project, i.e. not inside any other subfolder, then you should use here::here(\"brown\"). If it’s inside a “corpus” folder instead, you should use here::here(\"corpus\", \"brown\").\nI get a message that R cannot find the function “get_fnames” (or some other function)\nYou should load the library that the function belongs to. For get_fnames(), you need library(mclm). For here(), you need library(here). Alternatively, you can mention the name of the package before the function name, joined by two colons: mclm::get_fnames(), here::here()…\nI don’t see (main) or anything similar in my Terminal when I try to use Git.\nI’m not sure if you can do this on Mac at all, but on Windows, in R Studio, go to Tools > Global Options > Terminal, go to the dropbox menu after “New terminals open with” and choose Git Bash. After you reset R Studio, it should be ok.\n\n\nTIPS\n\nHow to name files"
  },
  {
    "objectID": "slides/assoc.html#outline",
    "href": "slides/assoc.html#outline",
    "title": "Association measures",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nEffect size measures\nStrength of evidence measures\nUsage in practice"
  },
  {
    "objectID": "slides/assoc.html#sources",
    "href": "slides/assoc.html#sources",
    "title": "Association measures",
    "section": "Sources",
    "text": "Sources\n\nCheck the documentation of mclm::assoc_scores().\nEvert (2007) in Toledo.\n\n\nIntroduction"
  },
  {
    "objectID": "slides/assoc.html#terms",
    "href": "slides/assoc.html#terms",
    "title": "Association measures",
    "section": "Terms",
    "text": "Terms\n\nObserved frequenciesExpected frequencies\n\n\n\n\n\n\n \n  \n      \n    Target item \n    Other items \n    Total \n  \n \n\n  \n    Target context \n    O11 \n    O12 \n    R1 \n  \n  \n    Reference context \n    O21 \n    O22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\n\n\n\n\n\n \n  \n      \n    Target item \n    Other items \n    Total \n  \n \n\n  \n    Target context \n    E11 \n    E12 \n    R1 \n  \n  \n    Reference context \n    E21 \n    E22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\nSee slides on Contingency tables.\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/assoc.html#set-up-data",
    "href": "slides/assoc.html#set-up-data",
    "title": "Association measures",
    "section": "Set up data",
    "text": "Set up data\n\nlibrary(tidyverse)\nlibrary(mclm)\ncorpus_folder <- here::here(\"studies\", \"_corpora\", \"brown\") # adapt path!!\nbrown_fnames <- get_fnames(corpus_folder) %>% keep_re(\"/c[a-z]\")\nhot_assoc <- surf_cooc(brown_fnames, \"^hot/jj\", re_token_splitter = \"\\\\s+\") %>% \n  assoc_scores() %>% as_tibble()\nhead(hot_assoc)\n\n# A tibble: 6 × 17\n  type       a     b     c       d   dir exp_a  DP_rows RR_rows    OR       MS\n  <chr>  <dbl> <dbl> <dbl>   <dbl> <dbl> <dbl>    <dbl>   <dbl> <dbl>    <dbl>\n1 ,/,       49   728 58104 1116762     1  38.4  0.0136    1.28  1.29  0.000843\n2 the/at    39   738 68974 1105892    -1  45.6 -0.00851   0.855 0.847 0.000565\n3 ./.       38   739 48774 1126092     1  32.3  0.00739   1.18  1.19  0.000778\n4 and/cc    36   741 28506 1146360     1  18.9  0.0221    1.91  1.95  0.00126 \n5 a/at      28   749 22915 1151951     1  15.2  0.0165    1.85  1.88  0.00122 \n6 of/in     21   756 35007 1139859    -1  23.2 -0.00277   0.907 0.904 0.000600\n# … with 6 more variables: Dice <dbl>, PMI <dbl>, chi2_signed <dbl>,\n#   G_signed <dbl>, t <dbl>, p_fisher_1 <dbl>\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/assoc.html#difference-of-proportions-delta-p",
    "href": "slides/assoc.html#difference-of-proportions-delta-p",
    "title": "Association measures",
    "section": "Difference of proportions (\\(\\Delta p\\))",
    "text": "Difference of proportions (\\(\\Delta p\\))\n\n\n\\[\\frac{O_{11}}{R_1}-\\frac{O_{21}}{R_2}\\]\n\n\\(< 0\\): repulsion\n\\(0\\): neutral\n\\(> 0\\): attraction\n\n\n\nhot_assoc %>% arrange(desc(DP_rows)) %>% \n  select(type, a, b, c, d, DP_rows) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    b \n    c \n    d \n    DP_rows \n  \n \n\n  \n    and/cc \n    36 \n    741 \n    28506 \n    1146360 \n    0.022 \n  \n  \n    a/at \n    28 \n    749 \n    22915 \n    1151951 \n    0.017 \n  \n  \n    water/nn \n    13 \n    764 \n    413 \n    1174453 \n    0.016 \n  \n  \n    ,/, \n    49 \n    728 \n    58104 \n    1116762 \n    0.014 \n  \n  \n    was/bedz \n    13 \n    764 \n    9793 \n    1165073 \n    0.008 \n  \n  \n    cold/jj \n    6 \n    771 \n    133 \n    1174733 \n    0.008 \n  \n  \n    ./. \n    38 \n    739 \n    48774 \n    1126092 \n    0.007 \n  \n  \n    with/in \n    10 \n    767 \n    7251 \n    1167615 \n    0.007 \n  \n  \n    it/pps \n    9 \n    768 \n    5844 \n    1169022 \n    0.007 \n  \n  \n    sun/nn \n    5 \n    772 \n    96 \n    1174770 \n    0.006 \n  \n\n\n\n\n\n\n\n\nOne of the values has to be very high for the value to be high\n\n\nEffect size measures"
  },
  {
    "objectID": "slides/assoc.html#relative-risk",
    "href": "slides/assoc.html#relative-risk",
    "title": "Association measures",
    "section": "Relative risk",
    "text": "Relative risk\n\n\n\\[\\frac{O_{11}/R_1}{O_{21}/R_2}\\]\n\n\\(< 1\\): repulsion\n\\(1\\): neutral\n\\(> 1\\): attraction\n\n\n\nhot_assoc %>% arrange(desc(RR_rows)) %>% \n  select(type, a, b, c, d, RR_rows) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    b \n    c \n    d \n    RR_rows \n  \n \n\n  \n    cereal/nn \n    4 \n    773 \n    12 \n    1174854 \n    504.02 \n  \n  \n    soup/nn \n    3 \n    774 \n    13 \n    1174853 \n    348.94 \n  \n  \n    cup/nn \n    3 \n    774 \n    40 \n    1174826 \n    113.40 \n  \n  \n    coffee/nn \n    4 \n    773 \n    72 \n    1174794 \n    84.00 \n  \n  \n    sun/nn \n    5 \n    772 \n    96 \n    1174770 \n    78.75 \n  \n  \n    weather/nn \n    3 \n    774 \n    63 \n    1174803 \n    72.00 \n  \n  \n    cold/jj \n    6 \n    771 \n    133 \n    1174733 \n    68.21 \n  \n  \n    water/nn \n    13 \n    764 \n    413 \n    1174453 \n    47.59 \n  \n  \n    summer/nn \n    3 \n    774 \n    128 \n    1174738 \n    35.44 \n  \n  \n    day/nn \n    4 \n    773 \n    623 \n    1174243 \n    9.71 \n  \n\n\n\n\n\n\n\n\nHow much higher, e.g. twice as high…\n\n\nEffect size measures"
  },
  {
    "objectID": "slides/assoc.html#odds-ratio",
    "href": "slides/assoc.html#odds-ratio",
    "title": "Association measures",
    "section": "Odds ratio",
    "text": "Odds ratio\n\n\n\\[\\frac{O_{11}/O_{12}}{O_{21}/O_{22}}\\]\n\n\\(< 1\\): repulsion\n\\(1\\): neutral\n\\(> 1\\): attraction\n\n\n\nhot_assoc %>% arrange(desc(OR)) %>% \n  select(type, a, b, c, d, OR) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    b \n    c \n    d \n    OR \n  \n \n\n  \n    cereal/nn \n    4 \n    773 \n    12 \n    1174854 \n    506.62 \n  \n  \n    soup/nn \n    3 \n    774 \n    13 \n    1174853 \n    350.28 \n  \n  \n    cup/nn \n    3 \n    774 \n    40 \n    1174826 \n    113.84 \n  \n  \n    coffee/nn \n    4 \n    773 \n    72 \n    1174794 \n    84.43 \n  \n  \n    sun/nn \n    5 \n    772 \n    96 \n    1174770 \n    79.26 \n  \n  \n    weather/nn \n    3 \n    774 \n    63 \n    1174803 \n    72.28 \n  \n  \n    cold/jj \n    6 \n    771 \n    133 \n    1174733 \n    68.74 \n  \n  \n    water/nn \n    13 \n    764 \n    413 \n    1174453 \n    48.39 \n  \n  \n    summer/nn \n    3 \n    774 \n    128 \n    1174738 \n    35.57 \n  \n  \n    day/nn \n    4 \n    773 \n    623 \n    1174243 \n    9.75 \n  \n\n\n\n\n\n\n\n\nEffect size measures"
  },
  {
    "objectID": "slides/assoc.html#log-odds-ratio",
    "href": "slides/assoc.html#log-odds-ratio",
    "title": "Association measures",
    "section": "log odds ratio",
    "text": "log odds ratio\n\n\n\\[\\log\\left(\\frac{O_{11}/O_{12}}{O_{21}/O_{22}}\\right)\\]\n\n\\(< 0\\): repulsion\n\\(0\\): neutral\n\\(> 0\\): attraction\n\n\n\nhot_assoc %>% mutate(log_OR = log(OR)) %>% arrange(desc(log_OR)) %>% \n  select(type, a, b, c, d, OR, log_OR) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    b \n    c \n    d \n    OR \n    log_OR \n  \n \n\n  \n    cereal/nn \n    4 \n    773 \n    12 \n    1174854 \n    506.62 \n    6.23 \n  \n  \n    soup/nn \n    3 \n    774 \n    13 \n    1174853 \n    350.28 \n    5.86 \n  \n  \n    cup/nn \n    3 \n    774 \n    40 \n    1174826 \n    113.84 \n    4.74 \n  \n  \n    coffee/nn \n    4 \n    773 \n    72 \n    1174794 \n    84.43 \n    4.44 \n  \n  \n    sun/nn \n    5 \n    772 \n    96 \n    1174770 \n    79.26 \n    4.37 \n  \n  \n    weather/nn \n    3 \n    774 \n    63 \n    1174803 \n    72.28 \n    4.28 \n  \n  \n    cold/jj \n    6 \n    771 \n    133 \n    1174733 \n    68.74 \n    4.23 \n  \n  \n    water/nn \n    13 \n    764 \n    413 \n    1174453 \n    48.39 \n    3.88 \n  \n  \n    summer/nn \n    3 \n    774 \n    128 \n    1174738 \n    35.57 \n    3.57 \n  \n  \n    day/nn \n    4 \n    773 \n    623 \n    1174243 \n    9.75 \n    2.28 \n  \n\n\n\n\n\n\n\n\nEffect size measures"
  },
  {
    "objectID": "slides/assoc.html#pmi",
    "href": "slides/assoc.html#pmi",
    "title": "Association measures",
    "section": "PMI",
    "text": "PMI\n\n\n\\[\\log_2 \\left(\\frac{O_{11}}{E_{11}}\\right)\\]\n\n\\(< 0\\): repulsion\n\\(0\\): neutral\n\\(> 0\\): attraction\n\n\n\nhot_assoc %>% arrange(desc(PMI)) %>% \n  select(type, a, exp_a, PMI) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    exp_a \n    PMI \n  \n \n\n  \n    cereal/nn \n    4 \n    0.011 \n    8.56 \n  \n  \n    soup/nn \n    3 \n    0.011 \n    8.15 \n  \n  \n    cup/nn \n    3 \n    0.028 \n    6.72 \n  \n  \n    coffee/nn \n    4 \n    0.050 \n    6.32 \n  \n  \n    sun/nn \n    5 \n    0.067 \n    6.23 \n  \n  \n    weather/nn \n    3 \n    0.044 \n    6.10 \n  \n  \n    cold/jj \n    6 \n    0.092 \n    6.03 \n  \n  \n    water/nn \n    13 \n    0.282 \n    5.53 \n  \n  \n    summer/nn \n    3 \n    0.087 \n    5.12 \n  \n  \n    day/nn \n    4 \n    0.414 \n    3.27 \n  \n\n\n\n\n\n\n\n\nEffect size measures"
  },
  {
    "objectID": "slides/assoc.html#dice-coefficient",
    "href": "slides/assoc.html#dice-coefficient",
    "title": "Association measures",
    "section": "DICE coefficient",
    "text": "DICE coefficient\n\n\n\\[\\frac{2O_{11}}{R_1+C_1}\\]\nHarmonic mean of \\(\\frac{O_{11}}{R_1}\\) and \\(\\frac{O_{11}}{C_1}\\)\n\nRange: 0-1\n\n\n\nhot_assoc %>% arrange(desc(Dice)) %>% \n  select(type, a, b, c, Dice) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    b \n    c \n    Dice \n  \n \n\n  \n    water/nn \n    13 \n    764 \n    413 \n    0.022 \n  \n  \n    cold/jj \n    6 \n    771 \n    133 \n    0.013 \n  \n  \n    sun/nn \n    5 \n    772 \n    96 \n    0.011 \n  \n  \n    cereal/nn \n    4 \n    773 \n    12 \n    0.010 \n  \n  \n    coffee/nn \n    4 \n    773 \n    72 \n    0.009 \n  \n  \n    soup/nn \n    3 \n    774 \n    13 \n    0.008 \n  \n  \n    cup/nn \n    3 \n    774 \n    40 \n    0.007 \n  \n  \n    weather/nn \n    3 \n    774 \n    63 \n    0.007 \n  \n  \n    summer/nn \n    3 \n    774 \n    128 \n    0.007 \n  \n  \n    day/nn \n    4 \n    773 \n    623 \n    0.006 \n  \n\n\n\n\n\n\n\n\nThe harmonic mean of two proportions grows high only if both proportions are high -> so good for catching proper nouns. Combining: “if we have the node, how often do we have the collocate” and “if we have the collocate, how often do we have the node” basically: is this the probability of being the first column bigger when you are in the first row than when you are in the second\n\n\nEffect size measures"
  },
  {
    "objectID": "slides/assoc.html#sum-up-effect-size-measures",
    "href": "slides/assoc.html#sum-up-effect-size-measures",
    "title": "Association measures",
    "section": "Sum up effect size measures",
    "text": "Sum up effect size measures\n\nIntuitive\n\n\n\nFragile, especially with low frequencies (which we often have)\n\n\nEffect size measures"
  },
  {
    "objectID": "slides/assoc.html#about-these-measures",
    "href": "slides/assoc.html#about-these-measures",
    "title": "Association measures",
    "section": "About these measures",
    "text": "About these measures\n\nHow certain are we that there is a difference?\nTake into amount effect size and amount of information.\n\nIf you have a big difference you don’t need much data.\nIf you have a subtle difference you need a lot of data.\n\n\n\n\nBUT they combine attraction and repulsion!\n\n\nStrength of evidence measures"
  },
  {
    "objectID": "slides/assoc.html#chi-square-chi2",
    "href": "slides/assoc.html#chi-square-chi2",
    "title": "Association measures",
    "section": "Chi-square (\\(\\chi^2\\))",
    "text": "Chi-square (\\(\\chi^2\\))\n\n\n\n\\(\\sum_{i=1}^{2}\\sum_{j=1}^{2}\\frac{(O_{ij}-E_{ij})^2}{E_{ij}}\\)\nNot a good test for low frequency items (one of the expected frequencies is below 5)\n\n\n\nhot_assoc %>% arrange(desc(chi2_signed)) %>% \n  select(type, a, exp_a, b, c, d, chi2_signed) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    exp_a \n    b \n    c \n    d \n    chi2_signed \n  \n \n\n  \n    cereal/nn \n    4 \n    0.011 \n    773 \n    12 \n    1174854 \n    1506.1 \n  \n  \n    soup/nn \n    3 \n    0.011 \n    774 \n    13 \n    1174853 \n    845.7 \n  \n  \n    water/nn \n    13 \n    0.282 \n    764 \n    413 \n    1174453 \n    575.1 \n  \n  \n    cold/jj \n    6 \n    0.092 \n    771 \n    133 \n    1174733 \n    380.3 \n  \n  \n    sun/nn \n    5 \n    0.067 \n    772 \n    96 \n    1174770 \n    364.9 \n  \n  \n    cup/nn \n    3 \n    0.028 \n    774 \n    40 \n    1174826 \n    310.9 \n  \n  \n    coffee/nn \n    4 \n    0.050 \n    773 \n    72 \n    1174794 \n    310.8 \n  \n  \n    weather/nn \n    3 \n    0.044 \n    774 \n    63 \n    1174803 \n    200.5 \n  \n  \n    summer/nn \n    3 \n    0.087 \n    774 \n    128 \n    1174738 \n    98.1 \n  \n  \n    day/nn \n    4 \n    0.414 \n    773 \n    623 \n    1174243 \n    31.1 \n  \n\n\n\n\n\n\n\n\nchecks sum of squared differences between each observed value and its expected counterpart (divided by the expected value) Under \\(H_0\\) this test statistic has a \\(\\chi^2\\) distribution with one df.\n\n\nStrength of evidence measures"
  },
  {
    "objectID": "slides/assoc.html#log-likelihood-ratio-g-or-g2",
    "href": "slides/assoc.html#log-likelihood-ratio-g-or-g2",
    "title": "Association measures",
    "section": "Log-likelihood ratio: \\(G\\) or \\(G^2\\)",
    "text": "Log-likelihood ratio: \\(G\\) or \\(G^2\\)\n\n\n\\[2\\sum_{i=1}^{2}\\sum_{j=1}^{2}\\left(O_{ij} \\times log\\left(\\frac{O_{ij}}{E_{ij}}\\right) \\right)\\]\n\nAlso problematic with low frequency but not as much as \\(\\chi^2\\) (expected values can be as low as 3).\n\n\n\nhot_assoc %>% arrange(desc(G_signed)) %>% \n  select(type, a, exp_a, b, c, G_signed) %>% head(10) %>% \n  kbl() %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    exp_a \n    b \n    c \n    G_signed \n  \n \n\n  \n    water/nn \n    13 \n    0.282 \n    764 \n    413 \n    74.8 \n  \n  \n    cereal/nn \n    4 \n    0.011 \n    773 \n    12 \n    40.6 \n  \n  \n    cold/jj \n    6 \n    0.092 \n    771 \n    133 \n    38.6 \n  \n  \n    sun/nn \n    5 \n    0.067 \n    772 \n    96 \n    33.6 \n  \n  \n    soup/nn \n    3 \n    0.011 \n    774 \n    13 \n    28.5 \n  \n  \n    coffee/nn \n    4 \n    0.050 \n    773 \n    72 \n    27.3 \n  \n  \n    cup/nn \n    3 \n    0.028 \n    774 \n    40 \n    22.2 \n  \n  \n    weather/nn \n    3 \n    0.044 \n    774 \n    63 \n    19.6 \n  \n  \n    summer/nn \n    3 \n    0.087 \n    774 \n    128 \n    15.5 \n  \n  \n    and/cc \n    36 \n    18.864 \n    741 \n    28506 \n    12.7 \n  \n\n\n\n\n\n\n\n\n\nLike \\(\\chi^2\\) but instead of taking the difference between observed and expected, it’s a division with a log transformation\nUnder \\(H_0\\) this test statistic has a \\(\\chi^2\\) distribution with one df.\nWe calculate \\(L\\), difference between proportions\n\nNumerator: maximum likelihood estimation (MLE) respecting the null hypothesis\nDenominator: maximum likelihood estimation allowing the proportion to be different from the observed one\nThen this computes the probability of encountering the proportion that we have\n\\(G^2 = -2 log(L)\\)\n\nHigher values of \\(G^2\\) (=lower values of \\(L\\)) show stronger evidence\n\n\n\nStrength of evidence measures"
  },
  {
    "objectID": "slides/assoc.html#fisher-exact-test",
    "href": "slides/assoc.html#fisher-exact-test",
    "title": "Association measures",
    "section": "Fisher exact test",
    "text": "Fisher exact test\n\n\n\nWe’ll use the p-value: the lower the better.\nUsed for low frequencies - accurate but computationally expensive.\n\n\n\nhot_assoc %>% arrange(p_fisher_1) %>% \n  select(type, a, b, c, d, p_fisher_1) %>% head(10) %>% \n  kbl(digits = 5) %>% kable_paper(font_size = 22)\n\n\n\n \n  \n    type \n    a \n    b \n    c \n    d \n    p_fisher_1 \n  \n \n\n  \n    water/nn \n    13 \n    764 \n    413 \n    1174453 \n    0.00000 \n  \n  \n    cereal/nn \n    4 \n    773 \n    12 \n    1174854 \n    0.00000 \n  \n  \n    cold/jj \n    6 \n    771 \n    133 \n    1174733 \n    0.00000 \n  \n  \n    sun/nn \n    5 \n    772 \n    96 \n    1174770 \n    0.00000 \n  \n  \n    soup/nn \n    3 \n    774 \n    13 \n    1174853 \n    0.00000 \n  \n  \n    coffee/nn \n    4 \n    773 \n    72 \n    1174794 \n    0.00000 \n  \n  \n    cup/nn \n    3 \n    774 \n    40 \n    1174826 \n    0.00000 \n  \n  \n    weather/nn \n    3 \n    774 \n    63 \n    1174803 \n    0.00001 \n  \n  \n    summer/nn \n    3 \n    774 \n    128 \n    1174738 \n    0.00010 \n  \n  \n    and/cc \n    36 \n    741 \n    28506 \n    1146360 \n    0.00024 \n  \n\n\n\n\n\n\n\n\n\nCan work for low frequencies, so it is mostly used in those cases (since it’s also computationally more demanding - sometimes it never ends)\nThe position of the observed \\(O_{11}\\) in a hypergeometric distribution (given by the marginal frequencies \\(R_1\\), \\(R_2\\) and \\(C_1\\)?) is computed\n\n\n\nStrength of evidence measures"
  },
  {
    "objectID": "slides/assoc.html#assoc-object",
    "href": "slides/assoc.html#assoc-object",
    "title": "Association measures",
    "section": "1. assoc object",
    "text": "1. assoc object\nObtain a dataframe with association measures between different items and the target context.\n\ncollocations: e.g. between different words and hot/jj\nkeywords: e.g. between different words and the academic files of the corpus.\n\nCode: mclm::assoc_scores() or mclm::assoc_abcd().\n\nUsage in practice"
  },
  {
    "objectID": "slides/assoc.html#what-counts-as-association",
    "href": "slides/assoc.html#what-counts-as-association",
    "title": "Association measures",
    "section": "What counts as association?",
    "text": "What counts as association?\n\nSet a threshold (always arbitrary, may be theoretically informed).\nUse a ranking.\nCombine: choose the n-best elements.\n\n\n\n\n\n\n\nTip\n\n\nThis can also be combined with different measures, e.g. use frequency threshold, a minimum value of PMI and of \\(G^2\\) and rank by either PMI or \\(G^2\\).\n\n\n\n\nUsage in practice"
  },
  {
    "objectID": "slides/assoc.html#how-to-choose-a-measure",
    "href": "slides/assoc.html#how-to-choose-a-measure",
    "title": "Association measures",
    "section": "How to choose a measure?",
    "text": "How to choose a measure?\n\nIt depends on the goal, previous literature…\nFor the final paper, comparing measures is a valid objective.\n\n\n\n\n\n\n\nSuggestion\n\n\nCombine an effect-size measure and a strength-of-evidence measure. Use them for thresholds and compare the rankings. A good pair is PMI and \\(G^2\\).\n\n\n\n\nUsage in practice"
  },
  {
    "objectID": "slides/contingency-tables.html#outline",
    "href": "slides/contingency-tables.html#outline",
    "title": "Contingency tables",
    "section": "Outline",
    "text": "Outline\n\nHot example\nObserved frequencies\nExpected frequencies\nTarget and reference contexts\nSumming up"
  },
  {
    "objectID": "slides/contingency-tables.html#set-up-data",
    "href": "slides/contingency-tables.html#set-up-data",
    "title": "Contingency tables",
    "section": "Set up data",
    "text": "Set up data\n\nlibrary(tidyverse)\nlibrary(mclm)\ncorpus_folder <- here::here(\"studies\", \"_corpora\", \"brown\")\nbrown_fnames <- get_fnames(corpus_folder) %>% keep_re(\"/c[a-z]\")\nflist <- freqlist(brown_fnames, re_token_splitter = re(\"\\\\s+\"))\nprint(flist, n = 5)\n\nFrequency list (types in list: 63517, tokens in list: 1162192)\nrank   type abs_freq nrm_freq\n---- ------ -------- --------\n   1 the/at    69013  593.818\n   2    ,/,    58153  500.373\n   3    ./.    48812  419.999\n   4  of/in    35028  301.396\n   5 and/cc    28542  245.588\n...\n\n\n\nHot example"
  },
  {
    "objectID": "slides/contingency-tables.html#frequency-of-hot",
    "href": "slides/contingency-tables.html#frequency-of-hot",
    "title": "Contingency tables",
    "section": "Frequency of hot",
    "text": "Frequency of hot\n\nflist %>% keep_re(\"^hot/\")\n\nFrequency list (types in list: 3, tokens in list: 130)\n<total number of tokens: 1162192>\nrank orig_rank      type abs_freq nrm_freq\n---- --------- --------- -------- --------\n   1       805    hot/jj      123    1.058\n   2     14852 hot/jj-tl        5    0.043\n   3     28147 hot/jj-hl        2    0.017\n\n\n\n\nHot example"
  },
  {
    "objectID": "slides/contingency-tables.html#concordance-of-hot",
    "href": "slides/contingency-tables.html#concordance-of-hot",
    "title": "Contingency tables",
    "section": "Concordance of hot",
    "text": "Concordance of hot\n\nhot <- conc(brown_fnames, \"\\\\bhot/jj\")\nhot\n\nConcordance-based data frame (number of observations: 118)\nidx                             left|match |right                           \n  1 .../-- After/in a/at long/jj ,/,|hot/jj|controversy/nn ,/, Miller/np-...\n  2 ...$ Dave/np Mills/np in/in a/at|hot/jj|duel/nn in/in 1.10.1/cd ./. K...\n  3 ...opped/vbd this/dt suddenly/rb|hot/jj|potato/nn in/in a/at very/ql ...\n  4 ...wps got/vbd off/rp to/in a/at|hot/jj|start/nn and/cc made/vbd a/at...\n  5 .../at cup/nn of/in steaming/vbg|hot/jj|coffee/nn awaiting/vbg him/pp...\n  6 ...A/at measure/nn of/in how/wrb|hot/jj|the/at stock/nn was/bedz ,/, ...\n  7 ...oods/nns-tl issue/nn was/bedz|hot/jj|long/rb before/cs it/pps was/...\n  8 ...ock/nn issue/vb such/abl a/at|hot/jj|one/cd ?/. ?/. The/at answer/...\n  9 ... Foods/nns-tl stock/nn the/at|hot/jj|issue/nn that/cs it/pps was/b...\n 10 ..., introduced/vbd the/at ``/``|hot/jj|dog/nn ''/'' and/cc paved/vbd...\n 11 ...g/vbg temperatures/nns and/cc|hot/jj|summer/nn pavements/nns are/b...\n 12 ...der/rbr ./. An/at ordinary/jj|hot/jj|bath/nn or/cc shower/nn will/...\n 13 ...smell/nn and/cc feel/nn of/in|hot/jj|,/, wet/jj woolen/jj sleeves/...\n 14 ...ns ./. No/at matter/nn how/ql|hot/jj|the/at day/nn ,/, they/ppss a...\n 15 ... who/wps was/bedz blowing/vbg|hot/jj|and/cc cold/jj ,/, exalting/v...\n 16 ...der/in sunny/jj skies/nns ,/,|hot/jj|sun/nn ,/, and/cc a/at fresh/...\n 17 ...nd/vb the/at summer/nn too/ql|hot/jj|for/in comfort/nn ./. And/cc ...\n 18 ...n't/doz* mean/vb that/cs a/at|hot/jj|rodder/nn must/md necessarily...\n 19 ...ctive/jj and/cc successful/jj|hot/jj|rodder/nn for/in years/nns wi...\n 20 ...d/vbn to/in its/pp$ normal/jj|hot/jj|operating/vbg pressure/nn and...\n 21 ... rated/vbn at/in a/at very/ql|hot/jj|2,460/cd fps/nn ,/, and/cc it...\n 22 ...napkins/nns that/wps kept/vbd|hot/jj|a/at platter/nn of/in oyster/...\n 23 ...nn that/wps kept/vbd them/ppo|hot/jj|when/wrb served/vbn --/-- was...\n 24 ...n ./. Heat/vb and/cc serve/vb|hot/jj|on/in toast/nn ./. The/at ome...\n 25 ...f/cs they/ppss are/ber too/ql|hot/jj|,/, and/cc to/to stop/vb flam...\n 26 ...nns ./. And/cc lots/nns of/in|hot/jj|pads/nns !/. !/. Do/do keep/v...\n 27 .../nn chilled/vbn or/cc soup/nn|hot/jj|./. Be/be sure/jj to/to get/v...\n 28 ...e/at chili/nn and/cc kraut/nn|hot/jj|with/in the/at franks/nns ./....\n 29 ...p$ daytime/jj naps/nns and/cc|hot/jj|meals/nns ,/, and/cc be/be pu...\n 30 ...shed/vbn and/cc free/jj of/in|hot/jj|weather/nn nerves/nns ./. You...\n...\n...\n\nThis data frame has 6 columns:\n   column\n1 glob_id\n2      id\n3  source\n4    left\n5   match\n6   right\n\n\n\nHot example"
  },
  {
    "objectID": "slides/contingency-tables.html#concordance-of-hot-1",
    "href": "slides/contingency-tables.html#concordance-of-hot-1",
    "title": "Contingency tables",
    "section": "Concordance of hot",
    "text": "Concordance of hot\n\nhot %>% \n  arrange(right)\n\nConcordance-based data frame (number of observations: 118)\nidx                             left|match |right                           \n  1 ...t was/bedz the/at word/nn ,/,|hot/jj|!/. !/. Hair/nn like/cs a/at ...\n  2 .../vbz in/in the/at long/jj (/(|hot/jj|)/) run/nn to/to take/vb good...\n  3 ...f/cs they/ppss are/ber too/ql|hot/jj|,/, and/cc to/to stop/vb flam...\n  4 ...d/cc the/at dice/nns were/bed|hot/jj|,/, but/cc he/pps couldn't/md...\n  5 ...pps had/hvd been/ben still/rb|hot/jj|,/, she/pps might/md even/rb ...\n  6 ...e/pps felt/vbd cold/jj and/cc|hot/jj|,/, sticky/jj and/cc chilly/j...\n  7 ...ed/vbn on/in sensation/nn (/(|hot/jj|,/, sweet/jj ,/, bitter/jj ,/...\n  8 ...dz unusually/rb dry/jj and/cc|hot/jj|,/, the/at spring/nn produced...\n  9 ...smell/nn and/cc feel/nn of/in|hot/jj|,/, wet/jj woolen/jj sleeves/...\n 10 .../nn chilled/vbn or/cc soup/nn|hot/jj|./. Be/be sure/jj to/to get/v...\n 11 ...n and/cc felt/vbd a/at bit/nn|hot/jj|./. He/pps stayed/vbd home/nr...\n 12 ...d away/rb ./. It/pps was/bedz|hot/jj|./. The/at dogs/nns were/bed ...\n 13 ...and/cc the/at sun/nn was/bedz|hot/jj|./. The/at new/jj Riverside/n...\n 14 ... rated/vbn at/in a/at very/ql|hot/jj|2,460/cd fps/nn ,/, and/cc it...\n 15 ...napkins/nns that/wps kept/vbd|hot/jj|a/at platter/nn of/in oyster/...\n 16 ...vbd around/rb ,/, suddenly/rb|hot/jj|all/ql over/rp ,/, finding/vb...\n 17 ... who/wps was/bedz blowing/vbg|hot/jj|and/cc cold/jj ,/, exalting/v...\n 18 ...onal/jj wars/nns ,/, both/abx|hot/jj|and/cc cold/jj ./. In/in ever...\n 19 .... His/pp$ prescription/nn :/:|hot/jj|and/cc cold/jj compresses/nns...\n 20 ...heated/jj in/in winter/nn ,/,|hot/jj|and/cc damp/jj under/in the/a...\n 21 .../cs your/pp$ blood/nn got/vbd|hot/jj|and/cc danced/vbd with/in the...\n 22 .../. The/at theatre/nn was/bedz|hot/jj|and/cc they/ppss were/bed dru...\n 23 ...The/at sun/nn ,/, blazing/vbg|hot/jj|as/cs prophesied/vbn ,/, was/...\n 24 ...rb it/pps was/bedz already/rb|hot/jj|at/in 7:30/cd A.M./rb ,/, and...\n 25 ...d out/rp pink/jj from/in a/at|hot/jj|bath/nn ,/, and/cc I/ppss gav...\n 26 ...der/rbr ./. An/at ordinary/jj|hot/jj|bath/nn or/cc shower/nn will/...\n 27 ...at drifting/vbg odor/nn of/in|hot/jj|biscuits/nns ./. The/at old/j...\n 28 ...pss want/vb to/to buy/vb a/at|hot/jj|Bodhisattva/np ./. Additional...\n 29 ...t/nn of/in Mrs./np Fogg's/np$|hot/jj|broth/nn before/cs starting/v...\n 30 .../nn and/cc wiping/vbg his/pp$|hot/jj|brow/nn ./. It/pps may/md app...\n...\n...\n\nThis data frame has 6 columns:\n   column\n1 glob_id\n2      id\n3  source\n4    left\n5   match\n6   right\n\n\n\nHot example"
  },
  {
    "objectID": "slides/contingency-tables.html#concordance-of-hot-2",
    "href": "slides/contingency-tables.html#concordance-of-hot-2",
    "title": "Contingency tables",
    "section": "Concordance of hot",
    "text": "Concordance of hot\n\nhot %>% \n  arrange(right) %>% \n  print_kwic(from = 25, n = 15)\n\nidx                             left|match |right                           \n...\n 25 ...d out/rp pink/jj from/in a/at|hot/jj|bath/nn ,/, and/cc I/ppss gav...\n 26 ...der/rbr ./. An/at ordinary/jj|hot/jj|bath/nn or/cc shower/nn will/...\n 27 ...at drifting/vbg odor/nn of/in|hot/jj|biscuits/nns ./. The/at old/j...\n 28 ...pss want/vb to/to buy/vb a/at|hot/jj|Bodhisattva/np ./. Additional...\n 29 ...t/nn of/in Mrs./np Fogg's/np$|hot/jj|broth/nn before/cs starting/v...\n 30 .../nn and/cc wiping/vbg his/pp$|hot/jj|brow/nn ./. It/pps may/md app...\n 31 ...eakfast/nn of/in fruit/nn ,/,|hot/jj|cereal/nn ,/, milk/nn ,/, hon...\n 32 ...sed/vbd to/to like/vb any/dti|hot/jj|cereal/nn ,/, now/rb that's/d...\n 33 .../nns they/ppss miss/vb the/at|hot/jj|cereal/nn ./. The/at school/n...\n 34 ...to/to come/vb down/rp with/in|hot/jj|chills/nns and/cc puzzling/jj...\n 35 ... sprinkled/vbd sugar/nn on/in|hot/jj|coals/nns and/cc held/vbd the...\n 36 ...t nonexistent/jj cup/nn of/in|hot/jj|coffee/nn ,/, and/cc that/cs ...\n 37 ...h/nn for/in a/at cup/nn of/in|hot/jj|coffee/nn ./. They/ppss are/b...\n 38 .../at cup/nn of/in steaming/vbg|hot/jj|coffee/nn awaiting/vbg him/pp...\n 39 ...n boiled/vbn ,/, applying/vbg|hot/jj|compresses/nns throughout/in ...\n...\n\n\n\nHot example"
  },
  {
    "objectID": "slides/contingency-tables.html#co-occurrences-with-hot",
    "href": "slides/contingency-tables.html#co-occurrences-with-hot",
    "title": "Contingency tables",
    "section": "Co-occurrences with hot",
    "text": "Co-occurrences with hot\n\nhot_cooc <- surf_cooc(brown_fnames, \"^hot/jj\", re_token_splitter = \"\\\\s+\")\n\n\n\n\n\nhead(hot_cooc$target_freqlist, 7)\n\nFrequency list (types in list: 7, tokens in list: 226)\n<total number of tokens: 777>\nrank orig_rank   type abs_freq nrm_freq\n---- --------- ------ -------- --------\n   1         1    ,/,       49  630.631\n   2         2 the/at       39  501.931\n   3         3    ./.       38  489.060\n   4         4 and/cc       36  463.320\n   5         5   a/at       28  360.360\n   6         6  of/in       21  270.270\n   7         7  in/in       15  193.050\n\n\n\n\nhead(hot_cooc$ref_freqlist, 7)\n\nFrequency list (types in list: 7, tokens in list: 282996)\n<total number of tokens: 1174866>\nrank orig_rank   type abs_freq nrm_freq\n---- --------- ------ -------- --------\n   1         1 the/at    68974  587.080\n   2         2    ,/,    58104  494.559\n   3         3    ./.    48774  415.145\n   4         4  of/in    35007  297.966\n   5         5 and/cc    28506  242.632\n   6         6   a/at    22915  195.044\n   7         7  in/in    20716  176.326\n\n\n\n\n\nHot example"
  },
  {
    "objectID": "slides/contingency-tables.html#hot-coffee",
    "href": "slides/contingency-tables.html#hot-coffee",
    "title": "Contingency tables",
    "section": "Hot coffee",
    "text": "Hot coffee\n\nmap(hot_cooc, keep_re, \"^coffee/\")\n\n$target_freqlist\nFrequency list (types in list: 1, tokens in list: 4)\n<total number of tokens: 777>\nrank orig_rank      type abs_freq nrm_freq\n---- --------- --------- -------- --------\n   1        24 coffee/nn        4    51.48\n\n$ref_freqlist\nFrequency list (types in list: 2, tokens in list: 74)\n<total number of tokens: 1174866>\nrank orig_rank         type abs_freq nrm_freq\n---- --------- ------------ -------- --------\n   1      1450    coffee/nn       72    0.613\n   2     25804 coffee/nn-tl        2    0.017\n\n\n\nHot example"
  },
  {
    "objectID": "slides/contingency-tables.html#important-frequencies",
    "href": "slides/contingency-tables.html#important-frequencies",
    "title": "Contingency tables",
    "section": "Important frequencies",
    "text": "Important frequencies\n\nFrequency of hot/jj context = \\(f(n)\\) = 7771\nFrequency of coffee/nn = \\(f(c)\\) = 76\nFrequency of coffee/nn in the context of hot/jj = \\(f(n,c)\\) = 4\nSum of the hot/jj context and not-hot/jj context = \\(N\\) = 1175643\n\n\nObserved frequencies\n\nNot the frequency of hot/jj (123)."
  },
  {
    "objectID": "slides/contingency-tables.html#a-b-c-d",
    "href": "slides/contingency-tables.html#a-b-c-d",
    "title": "Contingency tables",
    "section": "a, b, c, d…",
    "text": "a, b, c, d…\n\n\n\n\n \n  \n      \n    Coffee \n    Not coffee \n    Total \n  \n \n\n  \n    Context of hot \n    a \n    b \n    m = a + b \n  \n  \n    Not context of hot \n    c \n    d \n    n = c + d \n  \n  \n    Total \n    k = a + c \n    l = b + d \n    N = m + n \n  \n\n\n\n\n\n\nObserved frequencies"
  },
  {
    "objectID": "slides/contingency-tables.html#o-r-c-n",
    "href": "slides/contingency-tables.html#o-r-c-n",
    "title": "Contingency tables",
    "section": "O, R, C, N",
    "text": "O, R, C, N\n\n\n\n\n \n  \n      \n    Coffee \n    Not coffee \n    Total \n  \n \n\n  \n    Context of hot \n    O11 \n    O12 \n    R1 \n  \n  \n    Not context of hot \n    O21 \n    O22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\nObserved frequencies"
  },
  {
    "objectID": "slides/contingency-tables.html#building-the-contingency-table",
    "href": "slides/contingency-tables.html#building-the-contingency-table",
    "title": "Contingency tables",
    "section": "Building the contingency table",
    "text": "Building the contingency table\n\n\n\n\n \n  \n      \n    Coffee \n    Not coffee \n    Total \n  \n \n\n  \n    Context of hot \n    4 \n    773 \n    777 \n  \n  \n    Not context of hot \n    72 \n    1 174 794 \n    1 174 866 \n  \n  \n    Total \n    76 \n    1 175 567 \n    1 175 643 \n  \n\n\n\n\n\n\n\nHow important is it that coffee occurs 4 times in the context of hot when\n\nthere are 773 other events in the context of hot\nand coffee also occurs 72 times outside the context of hot?\n\n\n\n\nObserved frequencies"
  },
  {
    "objectID": "slides/contingency-tables.html#expected-a-b-c-d",
    "href": "slides/contingency-tables.html#expected-a-b-c-d",
    "title": "Contingency tables",
    "section": "Expected a, b, c, d…",
    "text": "Expected a, b, c, d…\n\n\n\n\n \n  \n      \n    Coffee \n    Not coffee \n    Total \n  \n \n\n  \n    Context of hot \n    (m * k)/N \n    (m * l)/N \n    m \n  \n  \n    Not context of hot \n    (n * k)/N \n    (n * l)/N \n    n \n  \n  \n    Total \n    k \n    l \n    N \n  \n\n\n\n\n\n\nExpected frequencies"
  },
  {
    "objectID": "slides/contingency-tables.html#e-r-c-n",
    "href": "slides/contingency-tables.html#e-r-c-n",
    "title": "Contingency tables",
    "section": "E, R, C, N",
    "text": "E, R, C, N\n\n\n\n\n \n  \n      \n    Coffee \n    Not coffee \n    Total \n  \n \n\n  \n    Context of hot \n    E11 \n    E12 \n    R1 \n  \n  \n    Not context of hot \n    E21 \n    E22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\nExpected frequencies"
  },
  {
    "objectID": "slides/contingency-tables.html#expected-frequencies-1",
    "href": "slides/contingency-tables.html#expected-frequencies-1",
    "title": "Contingency tables",
    "section": "Expected frequencies",
    "text": "Expected frequencies\n\n\n\n\n \n  \n      \n    Coffee \n    Not coffee \n    Total \n  \n \n\n  \n    Context of hot \n    0.05 \n    776.95 \n    777 \n  \n  \n    Not context of hot \n    75.95 \n    1 174 790.05 \n    1 174 866 \n  \n  \n    Total \n    76.00 \n    1 175 567.00 \n    1 175 643 \n  \n\n\n\n\n\n\nCf. Observed frequencies.\n\n\n\n\n \n  \n      \n    Coffee \n    Not coffee \n    Total \n  \n \n\n  \n    Context of hot \n    4 \n    773 \n    777 \n  \n  \n    Not context of hot \n    72 \n    1 174 794 \n    1 174 866 \n  \n  \n    Total \n    76 \n    1 175 567 \n    1 175 643 \n  \n\n\n\n\n\n\nExpected frequencies"
  },
  {
    "objectID": "slides/contingency-tables.html#contexts-in-the-contingency-table",
    "href": "slides/contingency-tables.html#contexts-in-the-contingency-table",
    "title": "Contingency tables",
    "section": "Contexts in the contingency table",
    "text": "Contexts in the contingency table\n\n\n\n\n \n  \n      \n    Target item \n    Other items \n    Total \n  \n \n\n  \n    Target context \n    O11 \n    O12 \n    R1 \n  \n  \n    Reference context \n    O21 \n    O22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#collocation-analysis",
    "href": "slides/contingency-tables.html#collocation-analysis",
    "title": "Contingency tables",
    "section": "Collocation analysis",
    "text": "Collocation analysis\n\nThe target context is the surroundings of an item, the node, e.g. hot.\n\nIn surface collocations, a certain window around the node, e.g. 3 tokens to either side.\nIn textual collocations, the text in which the node occurs.\nIn syntactic collocations, a certain syntactic relationship with the node.\n\nThe reference context is all the contexts not surrounding the node.\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#collocation-analysis---table",
    "href": "slides/contingency-tables.html#collocation-analysis---table",
    "title": "Contingency tables",
    "section": "Collocation analysis - table",
    "text": "Collocation analysis - table\n\n\n\n\n \n  \n      \n    Collocate \n    Other items \n    Total \n  \n \n\n  \n    Context of node \n    O11 \n    O12 \n    R1 \n  \n  \n    Not context of node \n    O21 \n    O22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#distinctive-collocation-analysis",
    "href": "slides/contingency-tables.html#distinctive-collocation-analysis",
    "title": "Contingency tables",
    "section": "Distinctive collocation analysis",
    "text": "Distinctive collocation analysis\n\nThe target context is the surroundings of a node, e.g. hot.\nThe reference context is the surroundings of a second node for contrast, e.g. cold.\n\n\n\n\n\n\n\nNote\n\n\nIn (distinctive) collocation analysis, the “target” item is the collocate.\n\n\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#distinctive-collocation-analysis---table",
    "href": "slides/contingency-tables.html#distinctive-collocation-analysis---table",
    "title": "Contingency tables",
    "section": "Distinctive collocation analysis - table",
    "text": "Distinctive collocation analysis - table\n\n\n\n\n \n  \n      \n    Collocate \n    Other items \n    Total \n  \n \n\n  \n    Context of node \n    O11 \n    O12 \n    R1 \n  \n  \n    Context of alternative node \n    O21 \n    O22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#keyword-analysis",
    "href": "slides/contingency-tables.html#keyword-analysis",
    "title": "Contingency tables",
    "section": "Keyword analysis",
    "text": "Keyword analysis\n\nThe target context is a text or corpus.\nThe reference context is another bigger, reference corpus.\n\n\n\n\n\n\n\nNote\n\n\nIn distinctive keyword analysis the reference context is another target corpus.\n\n\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#keyword-analysis---table",
    "href": "slides/contingency-tables.html#keyword-analysis---table",
    "title": "Contingency tables",
    "section": "Keyword analysis - table",
    "text": "Keyword analysis - table\n\n\n\n\n \n  \n      \n    Target item \n    Other items \n    Total \n  \n \n\n  \n    Target text/corpus \n    O11 \n    O12 \n    R1 \n  \n  \n    Reference corpus \n    O21 \n    O22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#collostructional-analysis",
    "href": "slides/contingency-tables.html#collostructional-analysis",
    "title": "Contingency tables",
    "section": "Collostructional analysis",
    "text": "Collostructional analysis\n\nThe target context is a certain slot in a construction.\nThe reference context is a comparable slot in a comparable construction.\n\n\n\n\n\n\n\nNote\n\n\nWe talk about distinctive collexeme analysis when that second construction is very specific.\n\n\n\n\nFor example, ditransitive construction versus transitive or intransitive. Distinctive: between ditransitive and prepositional alternative – you need enough data.\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#collostructional-analysis---table",
    "href": "slides/contingency-tables.html#collostructional-analysis---table",
    "title": "Contingency tables",
    "section": "Collostructional analysis - table",
    "text": "Collostructional analysis - table\n\n\n\n\n \n  \n      \n    Target item \n    Other items \n    Total \n  \n \n\n  \n    Target construction \n    O11 \n    O12 \n    R1 \n  \n  \n    Comparable construction \n    O21 \n    O22 \n    R2 \n  \n  \n    Total \n    C1 \n    C2 \n    N \n  \n\n\n\n\n\n\nTarget and reference contexts"
  },
  {
    "objectID": "slides/contingency-tables.html#procedure",
    "href": "slides/contingency-tables.html#procedure",
    "title": "Contingency tables",
    "section": "Procedure",
    "text": "Procedure\n\nFor each type in the corpus, compute their frequencies in the target and reference contexts.\nWe obtain two frequency lists from which we can obtain all necessary values of the contingency table.\nBased on the contingency tables, compute the association strength between each type and the target context.\nRank and/or filter.\n\n\nSumming up"
  },
  {
    "objectID": "slides/contingency-tables.html#code",
    "href": "slides/contingency-tables.html#code",
    "title": "Contingency tables",
    "section": "Code",
    "text": "Code\nMain function mclm::assoc_scores().\n\nObtain two frequency lists:\n\n\n“Manually” (with freqlist() and different corpora, for example.)\nWith surf_cooc() for surface collocations.\nWith text_cooc() for textual collocations.\n\n\nGive the frequency lists to assoc_scores().1\n\n\nSumming up\n\nAlternatively, give the values of a, b, c and d to assoc_abcd()."
  },
  {
    "objectID": "slides/contingency-tables.html#hot-example-1",
    "href": "slides/contingency-tables.html#hot-example-1",
    "title": "Contingency tables",
    "section": "Hot example",
    "text": "Hot example\n\nassoc_scores(hot_cooc)\n\nAssociation scores (types in list: 44)\n       type  a    PMI G_signed|  b     c       d dir  exp_a DP_rows\n 1      ,/, 49  0.350    2.824|728 58104 1116762   1 38.434   0.014\n 2   the/at 39 -0.226   -1.069|738 68974 1105892  -1 45.612  -0.009\n 3      ./. 38  0.236    1.010|739 48774 1126092   1 32.261   0.007\n 4   and/cc 36  0.932   12.660|741 28506 1146360   1 18.864   0.022\n 5     a/at 28  0.885    8.898|749 22915 1151951   1 15.163   0.017\n 6    of/in 21 -0.141   -0.213|756 35007 1139859  -1 23.151  -0.003\n 7    in/in 15  0.131    0.122|762 20716 1154150   1 13.701   0.002\n 8 was/bedz 13  1.004    5.120|764  9793 1165073   1  6.481   0.008\n 9 water/nn 13  5.529   74.799|764   413 1174453   1  0.282   0.016\n10  with/in 10  1.059    4.321|767  7251 1167615   1  4.799   0.007\n11   it/pps  9  1.218    4.975|768  5844 1169022   1  3.868   0.007\n12    to/to  8 -0.301   -0.380|769 14909 1159957  -1  9.859  -0.002\n13    on/in  7  0.795    1.796|770  6098 1168768   1  4.035   0.004\n14  cold/jj  6  6.029   38.634|771   133 1174733   1  0.092   0.008\n15    to/in  6 -0.283   -0.249|771 11040 1163826  -1  7.300  -0.002\n16    at/in  5  0.499    0.538|772  5348 1169518   1  3.538   0.002\n17  from/in  5  0.801    1.299|772  4337 1170529   1  2.870   0.003\n18   sun/nn  5  6.227   33.572|772    96 1174770   1  0.067   0.006\n19  this/dt  5  0.558    0.664|772  5133 1169733   1  3.396   0.002\n20           4 -1.166   -3.520|773 13577 1161289  -1  8.976  -0.006\n...\n<number of extra columns to the right: 7>\n\n\n\nSumming up"
  },
  {
    "objectID": "slides/contingency-tables.html#hot-coffee-association",
    "href": "slides/contingency-tables.html#hot-coffee-association",
    "title": "Contingency tables",
    "section": "Hot-coffee association",
    "text": "Hot-coffee association\n\nassoc_scores(hot_cooc)[\"coffee/nn\",]\n\nAssociation scores (types in list: 1)\n       type a   PMI G_signed|  b  c       d dir exp_a DP_rows RR_rows\n1 coffee/nn 4 6.315   27.349|773 72 1174794   1  0.05   0.005  84.003\n<number of extra columns to the right: 6>\n\n\n\nSumming up"
  },
  {
    "objectID": "slides/correspondence-analysis.html#outline",
    "href": "slides/correspondence-analysis.html#outline",
    "title": "Correspondence analysis",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nTerminology\nInterpretation\nLinguistic example"
  },
  {
    "objectID": "slides/correspondence-analysis.html#correspondence-analysis",
    "href": "slides/correspondence-analysis.html#correspondence-analysis",
    "title": "Correspondence analysis",
    "section": "Correspondence Analysis",
    "text": "Correspondence Analysis\n\nDimension reduction technique for “count” data.\n\nVisualization: biplot, showing relationships:\n\nbetween rows\nbetween columns\nbetween rows and columns (kind of).\n\n\nIntroduction"
  },
  {
    "objectID": "slides/correspondence-analysis.html#linguistic-example",
    "href": "slides/correspondence-analysis.html#linguistic-example",
    "title": "Correspondence analysis",
    "section": "Linguistic example",
    "text": "Linguistic example\n\n\nIntroduction"
  },
  {
    "objectID": "slides/correspondence-analysis.html#classic-non-linguistic-example",
    "href": "slides/correspondence-analysis.html#classic-non-linguistic-example",
    "title": "Correspondence analysis",
    "section": "Classic (non-linguistic) example1",
    "text": "Classic (non-linguistic) example1\n\n\n\nlibrary(tidyverse)\nlibrary(mclm)\ndata(smoke)\nprint_matrix(smoke)\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n  \n \n\n  \n    SM \n    4 \n    2 \n    3 \n    2 \n  \n  \n    JM \n    4 \n    3 \n    7 \n    4 \n  \n  \n    SE \n    25 \n    10 \n    12 \n    4 \n  \n  \n    JE \n    18 \n    24 \n    33 \n    13 \n  \n  \n    SC \n    10 \n    6 \n    7 \n    2 \n  \n\n\n\n\n\n\n\nRows are types of employees (Senior/Junior manager, Senior/Junior Employee, Secretary).\nColumns are types of smokers.\nValues in the cells are counts.\n\n\n\n\nIntroduction\n\nThe smoke dataset comes with the {ca} package."
  },
  {
    "objectID": "slides/correspondence-analysis.html#smoke-plot",
    "href": "slides/correspondence-analysis.html#smoke-plot",
    "title": "Correspondence analysis",
    "section": "Smoke plot",
    "text": "Smoke plot\n\nsmoke_ca <- ca(smoke)\nplot(smoke_ca)\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/correspondence-analysis.html#smoke-ca",
    "href": "slides/correspondence-analysis.html#smoke-ca",
    "title": "Correspondence analysis",
    "section": "Smoke CA",
    "text": "Smoke CA\n\nprint()summary()\n\n\n\n\nCode\nsmoke_ca\n\n\n\n Principal inertias (eigenvalues):\n           1        2        3       \nValue      0.074759 0.010017 0.000414\nPercentage 87.76%   11.76%   0.49%   \n\n\n Rows:\n              SM      JM      SE     JE       SC\nMass     0.05699  0.0933  0.2642 0.4560  0.12953\nChiDist  0.21656  0.3569  0.3808 0.2400  0.21617\nInertia  0.00267  0.0119  0.0383 0.0263  0.00605\nDim. 1  -0.24054  0.9471 -1.3920 0.8520 -0.73546\nDim. 2  -1.93571 -2.4310 -0.1065 0.5769  0.78843\n\n\n Columns:\n           none   light medium   heavy\nMass     0.3161 0.23316 0.3212  0.1295\nChiDist  0.3945 0.17400 0.1981  0.3551\nInertia  0.0492 0.00706 0.0126  0.0163\nDim. 1  -1.4385 0.36375 0.7180  1.0744\nDim. 2  -0.3047 1.40943 0.0735 -1.9760\n\n\n\n\n\n\nCode\nsummary(smoke_ca)\n\n\n\nPrincipal inertias (eigenvalues):\n\n dim    value      %   cum%   scree plot               \n 1      0.074759  87.8  87.8  **********************   \n 2      0.010017  11.8  99.5  ***                      \n 3      0.000414   0.5 100.0                           \n        -------- -----                                 \n Total: 0.085190 100.0                                 \n\n\nRows:\n    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  \n1 |   SM |   57  893   31 |  -66  92   3 | -194 800 214 |\n2 |   JM |   93  991  139 |  259 526  84 | -243 465 551 |\n3 |   SE |  264 1000  450 | -381 999 512 |  -11   1   3 |\n4 |   JE |  456 1000  308 |  233 942 331 |   58  58 152 |\n5 |   SC |  130  999   71 | -201 865  70 |   79 133  81 |\n\nColumns:\n    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  \n1 | none |  316 1000  577 | -393 994 654 |  -30   6  29 |\n2 | lght |  233  984   83 |   99 327  31 |  141 657 463 |\n3 | medm |  321  983  148 |  196 982 166 |    7   1   2 |\n4 | hevy |  130  995  192 |  294 684 150 | -198 310 506 |\n\n\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/correspondence-analysis.html#original-matrix-n",
    "href": "slides/correspondence-analysis.html#original-matrix-n",
    "title": "Correspondence analysis",
    "section": "Original matrix \\(N\\)",
    "text": "Original matrix \\(N\\)\n\\(n \\times m\\) matrix with frequency counts \\(n_{ij}\\)\n\n\n\n\\(n\\) = 5\n\\(m\\) = 4\n\\(n_{\\mathrm{SE, light}}\\) = 10\n\n\n\n\nCode\nsmoke_mtx <- as.matrix(smoke)\nprint_matrix(smoke_mtx)\n\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n  \n \n\n  \n    SM \n    4 \n    2 \n    3 \n    2 \n  \n  \n    JM \n    4 \n    3 \n    7 \n    4 \n  \n  \n    SE \n    25 \n    10 \n    12 \n    4 \n  \n  \n    JE \n    18 \n    24 \n    33 \n    13 \n  \n  \n    SC \n    10 \n    6 \n    7 \n    2 \n  \n\n\n\n\n\n\n\n\nTerminology"
  },
  {
    "objectID": "slides/correspondence-analysis.html#correspondence-matrix-p",
    "href": "slides/correspondence-analysis.html#correspondence-matrix-p",
    "title": "Correspondence analysis",
    "section": "Correspondence matrix \\(P\\)",
    "text": "Correspondence matrix \\(P\\)\n\\(n \\times m\\) matrix with overall proportions \\(p_{ij}\\)\n\n\nMass\n\n\\(p_{\\mathrm{SE,light}}\\) = 0.052\n\\(p_{\\mathrm{SE,.}}\\) = 0.264\n\\(p_{\\mathrm{.,light}}\\) = 0.233\n\n\n\n\n\n\nCode\nprop.table(smoke_mtx) %>% \n  addmargins() %>% \n  print_matrix()\n\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n    Sum \n  \n \n\n  \n    SM \n    0.021 \n    0.010 \n    0.016 \n    0.010 \n    0.057 \n  \n  \n    JM \n    0.021 \n    0.016 \n    0.036 \n    0.021 \n    0.093 \n  \n  \n    SE \n    0.130 \n    0.052 \n    0.062 \n    0.021 \n    0.264 \n  \n  \n    JE \n    0.093 \n    0.124 \n    0.171 \n    0.067 \n    0.456 \n  \n  \n    SC \n    0.052 \n    0.031 \n    0.036 \n    0.010 \n    0.130 \n  \n  \n    Sum \n    0.316 \n    0.233 \n    0.321 \n    0.130 \n    1.000 \n  \n\n\n\n\n\n\n\n\nTerminology"
  },
  {
    "objectID": "slides/correspondence-analysis.html#row-profiles",
    "href": "slides/correspondence-analysis.html#row-profiles",
    "title": "Correspondence analysis",
    "section": "Row profiles",
    "text": "Row profiles\n\\(n \\times m\\) matrix with overall row proportions \\(r_{ij}\\)\n\n\n\n\\(r_{\\mathrm{SE,light}}\\) = 0.1961\ncolumns are dimensions in the row points cloud\nIn bold: the row centroid (vector of column masses!)\n\n\n\n\nCode\naddmargins(smoke_mtx, 1) %>% \n  prop.table(1) %>% \n  print_matrix()\n\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n  \n \n\n  \n    SM \n    0.364 \n    0.182 \n    0.273 \n    0.182 \n  \n  \n    JM \n    0.222 \n    0.167 \n    0.389 \n    0.222 \n  \n  \n    SE \n    0.490 \n    0.196 \n    0.235 \n    0.078 \n  \n  \n    JE \n    0.205 \n    0.273 \n    0.375 \n    0.148 \n  \n  \n    SC \n    0.400 \n    0.240 \n    0.280 \n    0.080 \n  \n  \n    Sum \n    0.316 \n    0.233 \n    0.321 \n    0.130 \n  \n\n\n\n\n\n\n\n\nTerminology\n\nProportion of senior employees that are light smokers."
  },
  {
    "objectID": "slides/correspondence-analysis.html#what-do-you-mean-row-point-cloud",
    "href": "slides/correspondence-analysis.html#what-do-you-mean-row-point-cloud",
    "title": "Correspondence analysis",
    "section": "What do you mean, row point cloud?",
    "text": "What do you mean, row point cloud?\n\n\n\n\nCode\nrow_cloud <- addmargins(smoke_mtx, 1) %>% prop.table(1) %>% \n  as_tibble(rownames = \"Employee\") %>%\n  mutate(Employee = if_else(Employee == \"Sum\", \"Centroid\", Employee))\nrow_cloud %>% ggplot(aes(x = none, y = light, label = Employee)) +\n  geom_text(size = 10) +\n  theme_minimal(base_size = 20) +\n  annotate(\"point\", x = row_cloud$none[[nrow(row_cloud)]], y = row_cloud$light[[nrow(row_cloud)]], shape = 1, size = 25)\n\n\n\n\n\n\n\nBUT more than two dimensions\nWe then compute \\(\\chi^2\\) distances instead of euclidean distances\nHow far are points from the centroid?\n\n\n\n\nTerminology"
  },
  {
    "objectID": "slides/correspondence-analysis.html#column-profiles",
    "href": "slides/correspondence-analysis.html#column-profiles",
    "title": "Correspondence analysis",
    "section": "Column profiles",
    "text": "Column profiles\n\\(n \\times m\\) matrix with overall row proportions \\(c_{ij}\\)\n\n\n\n\\(c_{\\mathrm{SE,light}}\\) = 0.2221\nrows are dimensions in the column points cloud\nIn bold: the column centroid (vector of row masses!)\n\n\n\n\nCode\naddmargins(smoke_mtx, 2) %>% \n  prop.table(2) %>% \n  print_matrix()\n\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n    Sum \n  \n \n\n  \n    SM \n    0.066 \n    0.044 \n    0.048 \n    0.08 \n    0.057 \n  \n  \n    JM \n    0.066 \n    0.067 \n    0.113 \n    0.16 \n    0.093 \n  \n  \n    SE \n    0.410 \n    0.222 \n    0.194 \n    0.16 \n    0.264 \n  \n  \n    JE \n    0.295 \n    0.533 \n    0.532 \n    0.52 \n    0.456 \n  \n  \n    SC \n    0.164 \n    0.133 \n    0.113 \n    0.08 \n    0.130 \n  \n\n\n\n\n\n\n\n\nTerminology\n\nProportion of light smokers that are senior employees."
  },
  {
    "objectID": "slides/correspondence-analysis.html#also-a-column-point-cloud",
    "href": "slides/correspondence-analysis.html#also-a-column-point-cloud",
    "title": "Correspondence analysis",
    "section": "Also a column point cloud?",
    "text": "Also a column point cloud?\n\n\nCode\nrow_cloud %>% ggplot(aes(x = none, y = light, label = Employee)) +\n  geom_text(size = 8) +\n  theme_minimal(base_size = 20) +\n  annotate(\"point\", x = row_cloud$none[[nrow(row_cloud)]], y = row_cloud$light[[nrow(row_cloud)]], shape = 1, size = 25) +\n  labs(title = \"Two dimensions of the row cloud.\")\ncol_cloud <- addmargins(smoke_mtx, 2) %>% prop.table(2) %>% \n  t() %>% \n  as_tibble(rownames = \"Smoker\") %>%\n  mutate(Smoker = if_else(Smoker == \"Sum\", \"Centroid\", Smoker))\ncol_cloud %>% ggplot(aes(x = SM, y = SC, label = Smoker)) +\n  geom_text(size = 8) +\n  theme_minimal(base_size = 20) +\n  annotate(\"point\", x = col_cloud$SM[[nrow(col_cloud)]], y = col_cloud$SC[[nrow(col_cloud)]], shape = 1, size = 25) +\n  labs(title = \"Two dimensions of the column cloud.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerminology"
  },
  {
    "objectID": "slides/correspondence-analysis.html#smoke-ca---interpretation",
    "href": "slides/correspondence-analysis.html#smoke-ca---interpretation",
    "title": "Correspondence analysis",
    "section": "Smoke CA - interpretation",
    "text": "Smoke CA - interpretation\n\n\n\nsmoke_ca\n\n\n Principal inertias (eigenvalues):\n           1        2        3       \nValue      0.074759 0.010017 0.000414\nPercentage 87.76%   11.76%   0.49%   \n\n\n Rows:\n              SM      JM      SE     JE       SC\nMass     0.05699  0.0933  0.2642 0.4560  0.12953\nChiDist  0.21656  0.3569  0.3808 0.2400  0.21617\nInertia  0.00267  0.0119  0.0383 0.0263  0.00605\nDim. 1  -0.24054  0.9471 -1.3920 0.8520 -0.73546\nDim. 2  -1.93571 -2.4310 -0.1065 0.5769  0.78843\n\n\n Columns:\n           none   light medium   heavy\nMass     0.3161 0.23316 0.3212  0.1295\nChiDist  0.3945 0.17400 0.1981  0.3551\nInertia  0.0492 0.00706 0.0126  0.0163\nDim. 1  -1.4385 0.36375 0.7180  1.0744\nDim. 2  -0.3047 1.40943 0.0735 -1.9760\n\n\n\n\nsmoke_ca$rowmass\n\n[1] 0.0570 0.0933 0.2642 0.4560 0.1295\n\nsmoke_ca$rowdist\n\n[1] 0.217 0.357 0.381 0.240 0.216\n\nsmoke_ca$rowinertia\n\n[1] 0.00267 0.01188 0.03831 0.02627 0.00605\n\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-look-at-rowmass",
    "href": "slides/correspondence-analysis.html#lets-look-at-rowmass",
    "title": "Correspondence analysis",
    "section": "Let’s look at rowmass",
    "text": "Let’s look at rowmass\n\n\nCode\naddmargins(smoke_mtx, 1) %>% \n  prop.table(1) %>% \n  print_matrix()\naddmargins(smoke_mtx, 2) %>% \n  prop.table(2) %>% \n  print_matrix()\n\n\n\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n  \n \n\n  \n    SM \n    0.364 \n    0.182 \n    0.273 \n    0.182 \n  \n  \n    JM \n    0.222 \n    0.167 \n    0.389 \n    0.222 \n  \n  \n    SE \n    0.490 \n    0.196 \n    0.235 \n    0.078 \n  \n  \n    JE \n    0.205 \n    0.273 \n    0.375 \n    0.148 \n  \n  \n    SC \n    0.400 \n    0.240 \n    0.280 \n    0.080 \n  \n  \n    Sum \n    0.316 \n    0.233 \n    0.321 \n    0.130 \n  \n\n\n\n\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n    Sum \n  \n \n\n  \n    SM \n    0.066 \n    0.044 \n    0.048 \n    0.08 \n    0.057 \n  \n  \n    JM \n    0.066 \n    0.067 \n    0.113 \n    0.16 \n    0.093 \n  \n  \n    SE \n    0.410 \n    0.222 \n    0.194 \n    0.16 \n    0.264 \n  \n  \n    JE \n    0.295 \n    0.533 \n    0.532 \n    0.52 \n    0.456 \n  \n  \n    SC \n    0.164 \n    0.133 \n    0.113 \n    0.08 \n    0.130 \n  \n\n\n\n\n\n\n\n\nsmoke_ca$colmass\nsmoke_ca$rowmass\n\n\n\n[1] 0.316 0.233 0.321 0.130\n\n\n[1] 0.0570 0.0933 0.2642 0.4560 0.1295\n\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-look-at-rowdist",
    "href": "slides/correspondence-analysis.html#lets-look-at-rowdist",
    "title": "Correspondence analysis",
    "section": "Let’s look at rowdist",
    "text": "Let’s look at rowdist\n\nprop.table(smoke_mtx,1)[\"SM\",]\nsmoke_ca$colmass\n\n\n\n  none  light medium  heavy \n 0.364  0.182  0.273  0.182 \n\n\n[1] 0.316 0.233 0.321 0.130\n\n\n\n\n\nprop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass\n\n   none   light  medium   heavy \n 0.0476 -0.0513 -0.0485  0.0523 \n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-look-at-rowdist-1",
    "href": "slides/correspondence-analysis.html#lets-look-at-rowdist-1",
    "title": "Correspondence analysis",
    "section": "Let’s look at rowdist",
    "text": "Let’s look at rowdist\n\nprop.table(smoke_mtx,1)[\"SM\",]\nsmoke_ca$colmass\n\n\n\n  none  light medium  heavy \n 0.364  0.182  0.273  0.182 \n\n\n[1] 0.316 0.233 0.321 0.130\n\n\n\n\n\nprop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass\n\n   none   light  medium   heavy \n 0.0476 -0.0513 -0.0485  0.0523 \n\n(prop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass)^2\n\n   none   light  medium   heavy \n0.00226 0.00264 0.00235 0.00273 \n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-look-at-rowdist-2",
    "href": "slides/correspondence-analysis.html#lets-look-at-rowdist-2",
    "title": "Correspondence analysis",
    "section": "Let’s look at rowdist",
    "text": "Let’s look at rowdist\n\nprop.table(smoke_mtx,1)[\"SM\",]\nsmoke_ca$colmass\n\n\n\n  none  light medium  heavy \n 0.364  0.182  0.273  0.182 \n\n\n[1] 0.316 0.233 0.321 0.130\n\n\n\n\n\nprop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass\n\n   none   light  medium   heavy \n 0.0476 -0.0513 -0.0485  0.0523 \n\n(prop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass)^2\n\n   none   light  medium   heavy \n0.00226 0.00264 0.00235 0.00273 \n\n# Euclidean distance\nsqrt(sum((prop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass)^2))\n\n[1] 0.0999\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-look-at-rowdist-3",
    "href": "slides/correspondence-analysis.html#lets-look-at-rowdist-3",
    "title": "Correspondence analysis",
    "section": "Let’s look at rowdist",
    "text": "Let’s look at rowdist\n\nprop.table(smoke_mtx,1)[\"SM\",]\nsmoke_ca$colmass\n\n\n\n  none  light medium  heavy \n 0.364  0.182  0.273  0.182 \n\n\n[1] 0.316 0.233 0.321 0.130\n\n\n\n\n\nprop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass\n\n   none   light  medium   heavy \n 0.0476 -0.0513 -0.0485  0.0523 \n\n(prop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass)^2\n\n   none   light  medium   heavy \n0.00226 0.00264 0.00235 0.00273 \n\n(prop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass)^2/smoke_ca$colmass\n\n   none   light  medium   heavy \n0.00716 0.01131 0.00733 0.02110 \n\n# Chi-squared distance\nsqrt(sum((prop.table(smoke_mtx,1)[\"SM\",]-smoke_ca$colmass)^2/smoke_ca$colmass))\n\n[1] 0.217\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-look-at-rowdist-4",
    "href": "slides/correspondence-analysis.html#lets-look-at-rowdist-4",
    "title": "Correspondence analysis",
    "section": "Let’s look at rowdist",
    "text": "Let’s look at rowdist\n\nrow_dists <- map_dbl(smoke_ca$rownames, function(row) {\n  sqrt(sum((prop.table(smoke_mtx,1)[row,]-smoke_ca$colmass)^2/smoke_ca$colmass))\n})\nrow_dists\n\n[1] 0.217 0.357 0.381 0.240 0.216\n\nsmoke_ca$rowdist\n\n[1] 0.217 0.357 0.381 0.240 0.216\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-look-at-rowinertia",
    "href": "slides/correspondence-analysis.html#lets-look-at-rowinertia",
    "title": "Correspondence analysis",
    "section": "Let’s look at rowinertia",
    "text": "Let’s look at rowinertia\n\nsmoke_ca$rowdist\n\n[1] 0.217 0.357 0.381 0.240 0.216\n\nsmoke_ca$rowdist ^ 2\n\n[1] 0.0469 0.1274 0.1450 0.0576 0.0467\n\nsmoke_ca$rowdist ^ 2 * smoke_ca$rowmass\n\n[1] 0.00267 0.01188 0.03831 0.02627 0.00605\n\nsmoke_ca$rowinertia\n\n[1] 0.00267 0.01188 0.03831 0.02627 0.00605\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#reading-inertias",
    "href": "slides/correspondence-analysis.html#reading-inertias",
    "title": "Correspondence analysis",
    "section": "Reading inertias",
    "text": "Reading inertias\n\n\n\nsmoke_ca\n\n\n Principal inertias (eigenvalues):\n           1        2        3       \nValue      0.074759 0.010017 0.000414\nPercentage 87.76%   11.76%   0.49%   \n\n\n Rows:\n              SM      JM      SE     JE       SC\nMass     0.05699  0.0933  0.2642 0.4560  0.12953\nChiDist  0.21656  0.3569  0.3808 0.2400  0.21617\nInertia  0.00267  0.0119  0.0383 0.0263  0.00605\nDim. 1  -0.24054  0.9471 -1.3920 0.8520 -0.73546\nDim. 2  -1.93571 -2.4310 -0.1065 0.5769  0.78843\n\n\n Columns:\n           none   light medium   heavy\nMass     0.3161 0.23316 0.3212  0.1295\nChiDist  0.3945 0.17400 0.1981  0.3551\nInertia  0.0492 0.00706 0.0126  0.0163\nDim. 1  -1.4385 0.36375 0.7180  1.0744\nDim. 2  -0.3047 1.40943 0.0735 -1.9760\n\n\n\n\nsum(smoke_ca$rowinertia)\n\n[1] 0.0852\n\nsum(smoke_ca$colinertia)\n\n[1] 0.0852\n\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#interpretation-of-summary",
    "href": "slides/correspondence-analysis.html#interpretation-of-summary",
    "title": "Correspondence analysis",
    "section": "Interpretation of summary()",
    "text": "Interpretation of summary()\n\nsmoke_sum <- summary(smoke_ca)\nsmoke_sum\n\n\nPrincipal inertias (eigenvalues):\n\n dim    value      %   cum%   scree plot               \n 1      0.074759  87.8  87.8  **********************   \n 2      0.010017  11.8  99.5  ***                      \n 3      0.000414   0.5 100.0                           \n        -------- -----                                 \n Total: 0.085190 100.0                                 \n\n\nRows:\n    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  \n1 |   SM |   57  893   31 |  -66  92   3 | -194 800 214 |\n2 |   JM |   93  991  139 |  259 526  84 | -243 465 551 |\n3 |   SE |  264 1000  450 | -381 999 512 |  -11   1   3 |\n4 |   JE |  456 1000  308 |  233 942 331 |   58  58 152 |\n5 |   SC |  130  999   71 | -201 865  70 |   79 133  81 |\n\nColumns:\n    name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  \n1 | none |  316 1000  577 | -393 994 654 |  -30   6  29 |\n2 | lght |  233  984   83 |   99 327  31 |  141 657 463 |\n3 | medm |  321  983  148 |  196 982 166 |    7   1   2 |\n4 | hevy |  130  995  192 |  294 684 150 | -198 310 506 |\n\n\n\nround(smoke_ca$rowmass*1000)\nround(smoke_ca$rowinertia/sum(smoke_ca$rowinertia)*1000)\nround(smoke_ca$colinertia/sum(smoke_ca$colinertia)*1000)\n\n\n\n[1]  57  93 264 456 130\n\n\n[1]  31 139 450 308  71\n\n\n[1] 577  83 148 192\n\n\n\n\n:::\n::::\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#zooming-in-on-rows",
    "href": "slides/correspondence-analysis.html#zooming-in-on-rows",
    "title": "Correspondence analysis",
    "section": "Zooming in on rows",
    "text": "Zooming in on rows\n\nrows <- summary(smoke_ca)$rows\ncolnames(rows) <- c(\"row\", \"mass\", \"quality\", \"inertia\",\n                    paste0(rep(c(\"k\", \"cor\", \"ctr\"), 2), rep(c(1, 2), each = 3)))\nrows <- as_tibble(rows)\n# left table\nrows %>% print_matrix()\n# right table\nrows %>% select(row, quality, cor1, cor2) %>% \n  mutate(cors = cor1+cor2, qdiff = quality-cors) %>% \n  print_matrix()\n\n\n\n\n\n \n  \n    row \n    mass \n    quality \n    inertia \n    k1 \n    cor1 \n    ctr1 \n    k2 \n    cor2 \n    ctr2 \n  \n \n\n  \n    SM \n    57 \n    893 \n    31 \n    -66 \n    92 \n    3 \n    -194 \n    800 \n    214 \n  \n  \n    JM \n    93 \n    991 \n    139 \n    259 \n    526 \n    84 \n    -243 \n    465 \n    551 \n  \n  \n    SE \n    264 \n    1000 \n    450 \n    -381 \n    999 \n    512 \n    -11 \n    1 \n    3 \n  \n  \n    JE \n    456 \n    1000 \n    308 \n    233 \n    942 \n    331 \n    58 \n    58 \n    152 \n  \n  \n    SC \n    130 \n    999 \n    71 \n    -201 \n    865 \n    70 \n    79 \n    133 \n    81 \n  \n\n\n\n\n\n\n\n \n  \n    row \n    quality \n    cor1 \n    cor2 \n    cors \n    qdiff \n  \n \n\n  \n    SM \n    893 \n    92 \n    800 \n    892 \n    1 \n  \n  \n    JM \n    991 \n    526 \n    465 \n    991 \n    0 \n  \n  \n    SE \n    1000 \n    999 \n    1 \n    1000 \n    0 \n  \n  \n    JE \n    1000 \n    942 \n    58 \n    1000 \n    0 \n  \n  \n    SC \n    999 \n    865 \n    133 \n    998 \n    1 \n  \n\n\n\n\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#lets-see-that-plot-again",
    "href": "slides/correspondence-analysis.html#lets-see-that-plot-again",
    "title": "Correspondence analysis",
    "section": "Let’s see that plot again?",
    "text": "Let’s see that plot again?\n\n\nCode\nplot(smoke_ca)\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#checking-residuals",
    "href": "slides/correspondence-analysis.html#checking-residuals",
    "title": "Correspondence analysis",
    "section": "Checking residuals",
    "text": "Checking residuals\n\nsmoke_chisq <- chisq.test(smoke)\nsmoke_chisq\n\n\n    Pearson's Chi-squared test\n\ndata:  smoke\nX-squared = 16, df = 12, p-value = 0.2\n\n\n\n\n\nprint_matrix(smoke_chisq$expected)\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n  \n \n\n  \n    SM \n    3.48 \n    2.56 \n    3.53 \n    1.42 \n  \n  \n    JM \n    5.69 \n    4.20 \n    5.78 \n    2.33 \n  \n  \n    SE \n    16.12 \n    11.89 \n    16.38 \n    6.61 \n  \n  \n    JE \n    27.81 \n    20.52 \n    28.27 \n    11.40 \n  \n  \n    SC \n    7.90 \n    5.83 \n    8.03 \n    3.24 \n  \n\n\n\n\n\n\n\nprint_matrix(smoke_chisq$residuals)\n\n\n\n \n  \n      \n    none \n    light \n    medium \n    heavy \n  \n \n\n  \n    SM \n    0.281 \n    -0.353 \n    -0.284 \n    0.482 \n  \n  \n    JM \n    -0.708 \n    -0.584 \n    0.506 \n    1.093 \n  \n  \n    SE \n    2.212 \n    -0.548 \n    -1.083 \n    -1.014 \n  \n  \n    JE \n    -1.861 \n    0.769 \n    0.890 \n    0.474 \n  \n  \n    SC \n    0.747 \n    0.071 \n    -0.364 \n    -0.688 \n  \n\n\n\n\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/correspondence-analysis.html#contingency-table",
    "href": "slides/correspondence-analysis.html#contingency-table",
    "title": "Correspondence analysis",
    "section": "Contingency table",
    "text": "Contingency table\n\n\nCode\n# get corpus documents\ncorpus_folder <- here::here(\"studies\", \"_corpora\", \"clinton_trump\")\nfnames <- get_fnames(corpus_folder)\nshort_fnames <- short_names(fnames)\n# get possible features\nstop_list <- read_types(here::here(\"studies\", \"assets\", \"ca-trump-clinton\", \"stop_list.txt\"))\nfeatures <- freqlist(fnames) %>% drop_types(stop_list) %>%\n  keep_pos(1:150) %>% as_types()\n# create contingency table (a matrix)\nd <- map(setNames(fnames, short_fnames), ~ freqlist(.x)[features]) %>%\n  bind_cols() %>% data.frame(row.names = features) %>% \n  as.matrix() %>% t() %>% drop_empty_rc()\nkbl(d[1:10, 1:10]) %>% kable_paper(font_size = 20)\n\n\n\n\n \n  \n      \n    again \n    ago \n    also \n    america \n    american \n    americans \n    applause \n    are \n    audience \n    back \n  \n \n\n  \n    Clinton_2016.07.28 \n    4 \n    5 \n    6 \n    24 \n    3 \n    11 \n    113 \n    36 \n    6 \n    6 \n  \n  \n    Clinton_2016.07.29 \n    2 \n    3 \n    5 \n    13 \n    3 \n    1 \n    72 \n    18 \n    3 \n    2 \n  \n  \n    Clinton_2016.08.01 \n    0 \n    1 \n    2 \n    7 \n    2 \n    4 \n    22 \n    11 \n    1 \n    3 \n  \n  \n    Clinton_2016.08.05 \n    4 \n    1 \n    9 \n    11 \n    10 \n    10 \n    11 \n    37 \n    0 \n    7 \n  \n  \n    Clinton_2016.08.10 \n    0 \n    0 \n    5 \n    6 \n    4 \n    3 \n    41 \n    19 \n    0 \n    4 \n  \n  \n    Clinton_2016.08.11 \n    7 \n    1 \n    11 \n    25 \n    13 \n    15 \n    51 \n    40 \n    0 \n    16 \n  \n  \n    Clinton_2016.08.15 \n    2 \n    4 \n    3 \n    19 \n    9 \n    5 \n    52 \n    30 \n    8 \n    16 \n  \n  \n    Clinton_2016.08.16 \n    3 \n    0 \n    4 \n    10 \n    2 \n    3 \n    41 \n    20 \n    2 \n    6 \n  \n  \n    Clinton_2016.08.17 \n    3 \n    2 \n    5 \n    12 \n    6 \n    12 \n    64 \n    34 \n    0 \n    2 \n  \n  \n    Clinton_2016.08.25 \n    6 \n    1 \n    3 \n    19 \n    10 \n    5 \n    18 \n    17 \n    1 \n    2 \n  \n\n\n\n\n\n\nLinguistic example"
  },
  {
    "objectID": "slides/correspondence-analysis.html#correspondence-analysis-1",
    "href": "slides/correspondence-analysis.html#correspondence-analysis-1",
    "title": "Correspondence analysis",
    "section": "Correspondence Analysis",
    "text": "Correspondence Analysis\n\nd_ca <- ca(d)\n\n\nprint()summary()\n\n\n\nd_ca\n\n\n Principal inertias (eigenvalues):\n           1        2        3        4        5      6        7       \nValue      0.053657 0.040702 0.022246 0.019935 0.0143 0.013147 0.011372\nPercentage 14.09%   10.69%   5.84%    5.23%    3.75%  3.45%    2.99%   \n           8        9        10       11       12       13       14      \nValue      0.010335 0.008489 0.007962 0.007012 0.006627 0.006017 0.005841\nPercentage 2.71%    2.23%    2.09%    1.84%    1.74%    1.58%    1.53%   \n           15       16       17      18       19       20       21      \nValue      0.005661 0.005539 0.00512 0.004972 0.004851 0.004673 0.004558\nPercentage 1.49%    1.45%    1.34%   1.31%    1.27%    1.23%    1.2%    \n           22       23       24       25       26      27       28      \nValue      0.004145 0.004037 0.003968 0.003885 0.00385 0.003568 0.003367\nPercentage 1.09%    1.06%    1.04%    1.02%    1.01%   0.94%    0.88%   \n           29       30       31       32       33       34       35      \nValue      0.003301 0.003141 0.003067 0.002921 0.002839 0.002717 0.002664\nPercentage 0.87%    0.82%    0.81%    0.77%    0.75%    0.71%    0.7%    \n           36      37       38       39       40       41       42      \nValue      0.00258 0.002534 0.002443 0.002341 0.002303 0.002214 0.002181\nPercentage 0.68%   0.67%    0.64%    0.61%    0.6%     0.58%    0.57%   \n           43       44       45       46       47       48       49      \nValue      0.002148 0.002031 0.001955 0.001873 0.001861 0.001794 0.001711\nPercentage 0.56%    0.53%    0.51%    0.49%    0.49%    0.47%    0.45%   \n           50       51       52       53       54       55       56      \nValue      0.001662 0.001653 0.001595 0.001496 0.001442 0.001406 0.001361\nPercentage 0.44%    0.43%    0.42%    0.39%    0.38%    0.37%    0.36%   \n           57       58       59       60       61       62       63      \nValue      0.001311 0.001267 0.001257 0.001183 0.001174 0.001111 0.001072\nPercentage 0.34%    0.33%    0.33%    0.31%    0.31%    0.29%    0.28%   \n           64       65       66      67       68       69       70      \nValue      0.001038 0.001011 0.00097 0.000924 0.000905 0.000842 0.000825\nPercentage 0.27%    0.27%    0.25%   0.24%    0.24%    0.22%    0.22%   \n           71       72       73       74       75       76       77      \nValue      0.000774 0.000768 0.000747 0.000718 0.000704 0.000671 0.000659\nPercentage 0.2%     0.2%     0.2%     0.19%    0.18%    0.18%    0.17%   \n           78       79       80       81       82       83       84     \nValue      0.000605 0.000595 0.000568 0.000536 0.000505 0.000481 0.00045\nPercentage 0.16%    0.16%    0.15%    0.14%    0.13%    0.13%    0.12%  \n           85       86       87       88       89       90       91      \nValue      0.000431 0.000422 0.000389 0.000381 0.000343 0.000335 0.000315\nPercentage 0.11%    0.11%    0.1%     0.1%     0.09%    0.09%    0.08%   \n           92       93       94       95       96       97       98      \nValue      0.000292 0.000275 0.000265 0.000243 0.000238 0.000217 0.000205\nPercentage 0.08%    0.07%    0.07%    0.06%    0.06%    0.06%    0.05%   \n           99       100      101      102      103      104      105     \nValue      0.000183 0.000173 0.000164 0.000155 0.000152 0.000148 0.000128\nPercentage 0.05%    0.05%    0.04%    0.04%    0.04%    0.04%    0.03%   \n           106      107      108     109     110     111     112   113    \nValue      0.000121 0.000113 8.9e-05 7.4e-05 6.8e-05 5.9e-05 5e-05 4.5e-05\nPercentage 0.03%    0.03%    0.02%   0.02%   0.02%   0.02%   0.01% 0.01%  \n           114     115     116 117\nValue      3.7e-05 2.3e-05 0   0  \nPercentage 0.01%   0.01%   0%  0% \n\n\n Rows:\n        Clinton_2016.07.28 Clinton_2016.07.29 Clinton_2016.08.01\nMass               0.00787            0.00516            0.00225\nChiDist            0.72854            0.65694            0.97131\nInertia            0.00418            0.00223            0.00213\nDim. 1            -2.02389           -1.16774           -1.77022\nDim. 2             0.31847            0.87857            1.00231\n        Clinton_2016.08.05 Clinton_2016.08.10 Clinton_2016.08.11\nMass               0.00841            0.00329            0.00770\nChiDist            0.67367            0.84070            0.85071\nInertia            0.00382            0.00233            0.00557\nDim. 1            -1.11288           -2.12166           -2.06907\nDim. 2             1.52271            1.01558            0.48095\n        Clinton_2016.08.15 Clinton_2016.08.16 Clinton_2016.08.17\nMass               0.01033            0.00465            0.00613\nChiDist            0.62627            0.68516            0.77281\nInertia            0.00405            0.00218            0.00366\nDim. 1            -0.61023           -1.10783           -1.82868\nDim. 2             1.32985            0.92825            1.01634\n        Clinton_2016.08.25 Clinton_2016.08.31 Clinton_2016.09.05.A\nMass               0.00434            0.00473              0.00946\nChiDist            0.87991            0.97231              0.62874\nInertia            0.00336            0.00448              0.00374\nDim. 1            -1.67873           -1.99464             -1.27886\nDim. 2             1.23535            0.29216              1.19872\n        Clinton_2016.09.05.B Clinton_2016.09.06 Clinton_2016.09.08.A\nMass                 0.00517            0.00797              0.00286\nChiDist              0.71890            0.55881              1.05714\nInertia              0.00267            0.00249              0.00320\nDim. 1              -1.60773           -1.16248             -0.65555\nDim. 2               1.46085            0.75815              1.49847\n        Clinton_2016.09.08.B Clinton_2016.09.08.C Clinton_2016.09.29\nMass                 0.00419              0.00521            0.00249\nChiDist              0.81674              0.63028            0.93480\nInertia              0.00279              0.00207            0.00217\nDim. 1              -1.81649             -1.21676           -1.38177\nDim. 2               0.81885              1.31835            1.19489\n        Clinton_2016.09.30 Clinton_2016.10.03 Clinton_2016.10.24\nMass               0.00509            0.00659            0.01141\nChiDist            0.78489            0.63694            0.59666\nInertia            0.00313            0.00267            0.00406\nDim. 1            -2.22845           -1.23714           -1.62939\nDim. 2             0.37197            1.19205            0.83067\n        Clinton_2016.10.26 Clinton_2016.10.31.A Clinton_2016.10.31.B\nMass               0.00473              0.00458              0.00462\nChiDist            0.71761              0.90953              0.74975\nInertia            0.00244              0.00379              0.00260\nDim. 1            -1.58816             -1.75994             -1.90898\nDim. 2             1.08970              1.58702              1.38698\n        Clinton_2016.11.01.A Clinton_2016.11.01.B Clinton_2016.11.04.A\nMass                 0.00605              0.00342              0.00607\nChiDist              0.74790              1.02463              0.61072\nInertia              0.00338              0.00359              0.00226\nDim. 1              -1.29248             -2.24480             -1.38148\nDim. 2               1.37490              0.88133              0.77066\n        Clinton_2016.11.04.B Clinton_2016.11.05 Clinton_2016.11.06.A\nMass                 0.00596            0.00130              0.00429\nChiDist              0.70280            1.13518              0.82288\nInertia              0.00294            0.00168              0.00290\nDim. 1              -1.31183           -1.94122             -2.54299\nDim. 2               1.57636            1.22924              0.89944\n        Clinton_2016.11.06.B Clinton_2016.11.07.A Clinton_2016.11.07.B\nMass                 0.00470              0.00467              0.00580\nChiDist              0.83614              1.27873              0.68038\nInertia              0.00328              0.00763              0.00268\nDim. 1              -2.06126             -1.67631             -1.75604\nDim. 2               1.26015              1.23829              1.08815\n        Clinton_2016.11.07.C Clinton_2016.11.08 Clinton_2016.11.09\nMass                 0.00236            0.00228            0.00291\nChiDist              1.11338            1.16716            0.96174\nInertia              0.00292            0.00310            0.00269\nDim. 1              -2.83205           -2.98628           -1.83965\nDim. 2               0.78706            0.84589            0.41832\n        Trump_2016.07.22 Trump_2016.07.25 Trump_2016.07.26 Trump_2016.07.27.A\nMass             0.01219          0.01690          0.00424            0.01707\nChiDist          0.69509          0.47903          0.78165            0.74738\nInertia          0.00589          0.00388          0.00259            0.00954\nDim. 1           0.69341          0.79917         -0.73954            1.16718\nDim. 2           2.27359          1.00803         -0.70112            1.73993\n        Trump_2016.07.27.B Trump_2016.08.01 Trump_2016.08.02 Trump_2016.08.04\nMass               0.01936          0.01814          0.02013          0.01438\nChiDist            0.45108          0.59678          0.52218          0.42709\nInertia            0.00394          0.00646          0.00549          0.00262\nDim. 1             0.97719          1.26662          1.50007          0.68727\nDim. 2             1.06519          0.76711          0.54820          0.61284\n        Trump_2016.08.05 Trump_2016.08.08 Trump_2016.08.10 Trump_2016.08.12.A\nMass             0.01671          0.00668          0.01773            0.01699\nChiDist          0.44280          1.07661          0.53492            0.44378\nInertia          0.00328          0.00774          0.00507            0.00335\nDim. 1           0.64611         -1.54888          1.45535            1.10950\nDim. 2           0.69654         -2.21025          0.88880            0.70871\n        Trump_2016.08.12.B Trump_2016.08.15 Trump_2016.08.16 Trump_2016.08.17\nMass               0.01563          0.00589          0.00616          0.01222\nChiDist            0.48574          1.00223          0.68532          0.67146\nInertia            0.00369          0.00592          0.00290          0.00551\nDim. 1             1.28463         -1.24694         -1.13363          0.48849\nDim. 2             0.17365         -0.14256         -1.57701          1.19286\n        Trump_2016.08.18 Trump_2016.08.19 Trump_2016.08.22 Trump_2016.08.23\nMass             0.00646          0.00728          0.00785          0.00729\nChiDist          0.71089          0.67168          0.51183          0.91938\nInertia          0.00327          0.00329          0.00206          0.00616\nDim. 1          -1.47111         -0.67781         -0.43326         -0.80832\nDim. 2          -1.75252         -1.90305         -1.24625         -1.99774\n        Trump_2016.08.24.A Trump_2016.08.24.B Trump_2016.08.25 Trump_2016.08.30\nMass               0.01321            0.00721          0.00782          0.00732\nChiDist            0.45717            0.70808          0.61256          0.54672\nInertia            0.00276            0.00362          0.00293          0.00219\nDim. 1             0.30969           -1.06200         -0.35949         -0.48429\nDim. 2            -1.39983           -1.93394         -1.48243         -0.99443\n        Trump_2016.08.31 Trump_2016.09.01.A Trump_2016.09.01.B\nMass             0.01058            0.00495            0.00262\nChiDist          0.52025            0.75350            1.00512\nInertia          0.00286            0.00281            0.00265\nDim. 1          -0.31778           -0.33167           -1.74930\nDim. 2          -0.68690           -2.01326           -1.75483\n        Trump_2016.09.06.A Trump_2016.09.06.B Trump_2016.09.07.A\nMass               0.01347            0.00939            0.00451\nChiDist            0.56006            0.43420            0.90409\nInertia            0.00422            0.00177            0.00369\nDim. 1             0.98309            0.06941           -0.93760\nDim. 2             1.07593           -0.96084           -1.17754\n        Trump_2016.09.07.B Trump_2016.09.08 Trump_2016.09.09.A\nMass               0.00625          0.00616            0.00934\nChiDist            0.78935          0.67009            0.46204\nInertia            0.00390          0.00277            0.00199\nDim. 1             0.81699          0.02700            0.37344\nDim. 2             1.15679         -0.13474           -0.03671\n        Trump_2016.09.09.B Trump_2016.09.09 Trump_2016.09.12.A\nMass               0.00329          0.00934            0.00912\nChiDist            0.76311          0.46204            0.51056\nInertia            0.00192          0.00199            0.00238\nDim. 1             0.34694          0.37344           -0.19505\nDim. 2            -1.12001         -0.03671           -0.73983\n        Trump_2016.09.12.B Trump_2016.09.13.A Trump_2016.09.13.B\nMass               0.00309            0.00688            0.00314\nChiDist            0.97906            0.58271            1.62463\nInertia            0.00296            0.00233            0.00830\nDim. 1            -1.14776           -0.14949           -1.89710\nDim. 2            -1.07806           -1.45653           -0.40451\n        Trump_2016.09.13 Trump_2016.09.16 Trump_2016.09.17 Trump_2016.09.29\nMass             0.00688          0.00957          0.00444          0.00790\nChiDist          0.58271          0.48383          0.79010          0.52557\nInertia          0.00233          0.00224          0.00277          0.00218\nDim. 1          -0.14949          0.25199         -0.73154          0.20163\nDim. 2          -1.45653         -0.73164          0.06891         -0.95380\n        Trump_2016.09.30 Trump_2016.10.03.A Trump_2016.10.03.B Trump_2016.10.05\nMass             0.01137            0.01270            0.01131          0.01167\nChiDist          0.53608            0.69566            0.46560          0.37763\nInertia          0.00327            0.00614            0.00245          0.00166\nDim. 1           0.07976            0.54541           -0.16163         -0.07123\nDim. 2          -1.11114            0.82933           -0.54760         -0.67281\n        Trump_2016.10.06 Trump_2016.10.10.A Trump_2016.10.10.B Trump_2016.10.11\nMass             0.01621            0.01286            0.01505          0.01321\nChiDist          0.46661            0.46752            0.41148          0.39338\nInertia          0.00353            0.00281            0.00255          0.00204\nDim. 1           0.92687            0.67948            0.93722          0.37293\nDim. 2           1.07343           -0.38991           -0.18854         -0.30168\n        Trump_2016.10.12 Trump_2016.10.13 Trump_2016.10.14.A Trump_2016.10.14.B\nMass              0.0142          0.00725            0.00986            0.01447\nChiDist           0.4966          0.63648            0.44339            0.40301\nInertia           0.0035          0.00294            0.00194            0.00235\nDim. 1            0.6583         -0.45674            0.81191            0.68835\nDim. 2           -0.0311         -0.48487           -0.35958           -0.13796\n        Trump_2016.10.17 Trump_2016.10.18 Trump_2016.10.21.A Trump_2016.10.21.B\nMass             0.00999          0.00936            0.00600            0.01024\nChiDist          0.75894          0.48127            0.75496            0.51578\nInertia          0.00575          0.00217            0.00342            0.00272\nDim. 1           0.28669          0.08980            1.10636            0.52937\nDim. 2          -0.68317         -0.71455            0.92033           -0.87859\n        Trump_2016.10.21.C Trump_2016.10.27 Trump_2016.10.28 Trump_2016.10.31\nMass               0.00641          0.00722          0.00986          0.01258\nChiDist            0.60011          0.53977          0.50616          0.45557\nInertia            0.00231          0.00210          0.00253          0.00261\nDim. 1            -0.14318          0.22554          0.55159          0.25615\nDim. 2            -1.44395         -0.87037         -0.37745         -0.85616\n        Trump_2016.11.01.A Trump_2016.11.01.B Trump_2016.11.02.A\nMass               0.00875            0.00403            0.00888\nChiDist            0.47734            0.81191            0.57400\nInertia            0.00199            0.00266            0.00293\nDim. 1            -0.21015           -0.63058            0.24179\nDim. 2            -1.17654           -0.96360           -1.31010\n        Trump_2016.11.02.B Trump_2016.11.02.C Trump_2016.11.03.A\nMass               0.01078            0.00696            0.01122\nChiDist            0.44223            0.45933            0.48062\nInertia            0.00211            0.00147            0.00259\nDim. 1             0.43943            0.25498            0.33105\nDim. 2            -0.38575           -0.71532           -0.89609\n        Trump_2016.11.03.B Trump_2016.11.04.A Trump_2016.11.04.B\nMass               0.01030            0.01010            0.00943\nChiDist            0.40904            0.50267            0.42586\nInertia            0.00172            0.00255            0.00171\nDim. 1             0.59619            0.41608            0.23570\nDim. 2            -0.74128           -0.41939           -0.49207\n        Trump_2016.11.04.C Trump_2016.11.05.A Trump_2016.11.05.B\nMass               0.01235            0.01250            0.00973\nChiDist            0.46700            0.44782            0.53837\nInertia            0.00269            0.00251            0.00282\nDim. 1             0.86480            0.81109            0.00957\nDim. 2            -0.35621           -0.44001           -0.72408\n        Trump_2016.11.06 Trump_2016.11.07.A Trump_2016.11.07.B\nMass             0.00769             0.0115            0.01174\nChiDist          0.54634             0.5817            0.45662\nInertia          0.00230             0.0039            0.00245\nDim. 1           0.55748             0.4638            0.69168\nDim. 2          -0.71745            -0.5945           -0.50859\n        Trump_2016.11.07.C Trump_2016.11.07.D Trump_2016.11.08 Trump_2016.11.09\nMass               0.00684            0.00934          0.00819          0.00307\nChiDist            0.70994            0.46062          0.54275          0.95258\nInertia            0.00345            0.00198          0.00241          0.00279\nDim. 1             0.32096           -0.11529          0.12273         -0.83664\nDim. 2            -1.13683           -0.71337         -1.09737          0.50210\n\n\n Columns:\n           again      ago     also  america american americans applause\nMass     0.00520  0.00239  0.00441  0.00728  0.00623   0.00298   0.0417\nChiDist  0.59271  0.67304  0.69401  0.74844  1.00224   1.01361   0.5077\nInertia  0.00183  0.00108  0.00213  0.00407  0.00626   0.00306   0.0107\nDim. 1  -0.04653  1.42091 -1.32777 -2.51700 -1.95816  -3.08274  -1.3551\nDim. 2  -1.91206 -0.14646 -1.12235 -0.39190 -2.55831  -0.80550  -0.8807\n             are audience     back      bad     been     being  believe\nMass     0.02665  0.00398  0.00789  0.00337  0.00913  0.002714  0.00574\nChiDist  0.27224  1.31493  0.45793  0.74207  0.40025  0.580570  0.66284\nInertia  0.00198  0.00688  0.00165  0.00186  0.00146  0.000915  0.00252\nDim. 1  -0.29785 -0.82076  0.16778  1.97468  0.01503 -0.143460 -1.34014\nDim. 2  -0.33262 -3.10009 -0.65682 -0.55942 -0.16116 -0.042899  0.28557\n          better       big   booing    bring    build campaign   can't     care\nMass     0.00333  0.004299  0.00398  0.00304  0.00310  0.00319 0.00413  0.00393\nChiDist  0.56943  0.455725  1.32978  0.70156  0.67447  0.94575 0.57161  0.92892\nInertia  0.00108  0.000893  0.00704  0.00150  0.00141  0.00286 0.00135  0.00339\nDim. 1  -0.32014  0.674739  0.13660 -0.25543  0.30632 -2.23064 0.45155 -0.40253\nDim. 2  -0.03827 -0.033457 -3.15327 -1.65955 -0.88945  0.53180 0.57872 -0.24827\n          change  clinton    come countries  country      day     deal     did\nMass     0.00265  0.00995 0.00421   0.00244  0.01409  0.00310  0.00289 0.00494\nChiDist  1.03829  0.66937 0.57192   0.70936  0.41172  0.68683  0.79254 0.56255\nInertia  0.00286  0.00446 0.00138   0.00123  0.00239  0.00146  0.00181 0.00156\nDim. 1  -0.63910 -0.95596 0.46960   0.68959 -0.45816 -0.69089  1.22428 0.55516\nDim. 2  -1.89876 -0.76519 0.11646  -1.12318 -1.28842  0.97911 -0.84514 1.17934\n         didn't  doesn't   doing   don't   donald     done election    even\nMass    0.00293  0.00272 0.00434 0.01298  0.00429  0.00281  0.00227 0.00488\nChiDist 0.70625  0.70162 0.53789 0.42538  1.01695  0.76950  1.33125 0.49803\nInertia 0.00146  0.00134 0.00126 0.00235  0.00444  0.00166  0.00401 0.00121\nDim. 1  1.03604 -0.09742 1.01116 1.21532 -2.10531 -0.75504 -3.41121 0.19051\nDim. 2  1.08141  0.65338 0.25113 0.45578  2.23348  0.86540  2.17855 0.39306\n            ever everybody    folks     four      get     give      go    going\nMass     0.00465   0.00344  0.00403  0.00234  0.01169  0.00271 0.00637  0.03682\nChiDist  0.52537   0.65586  0.68399  0.74628  0.34888  0.74148 0.54306  0.39574\nInertia  0.00128   0.00148  0.00189  0.00130  0.00142  0.00149 0.00188  0.00577\nDim. 1   0.21894  -0.65236  1.49939  0.75926 -0.18967 -0.18898 0.08363  1.06378\nDim. 2  -1.24393   0.57027 -1.15309 -1.40534  0.51270  0.90447 1.38814 -0.63649\n           gonna    good      got government    great     had   happen      has\nMass     0.00261 0.00590  0.00708    0.00238  0.01249 0.00674  0.00326  0.00903\nChiDist  1.97901 0.50567  0.61873    0.89879  0.42992 0.59454  0.65432  0.53272\nInertia  0.01024 0.00151  0.00271    0.00193  0.00231 0.00238  0.00140  0.00256\nDim. 1   1.74616 0.20730 -0.22118   -0.38390  0.75507 0.11124  0.97980 -1.05348\nDim. 2  -0.72854 0.99638  1.39320   -2.75894 -0.27813 1.57321 -1.20735  0.18450\n           he's     help  hillary    i'll     i'm     i've important inaudible\nMass    0.00369  0.00231  0.01102 0.00237 0.00949  0.00441   0.00238   0.00301\nChiDist 0.87298  1.14192  0.53795 0.67733 0.43234  0.64769   0.71351   2.50099\nInertia 0.00281  0.00301  0.00319 0.00109 0.00177  0.00185   0.00121   0.01881\nDim. 1  0.52917 -3.53872  0.00955 0.83712 0.32248 -0.23131  -0.99611   0.12751\nDim. 2  2.59097  1.73090 -1.46143 0.63621 0.35250  1.52470   0.56070   1.38804\n              is     isis     it's      job     jobs     just     keep    know\nMass     0.03485  0.00246  0.01963  0.00301  0.00926  0.01176  0.00237 0.01936\nChiDist  0.28776  1.37982  0.33181  0.79021  0.67097  0.34990  0.81525 0.37735\nInertia  0.00288  0.00468  0.00216  0.00188  0.00417  0.00144  0.00158 0.00276\nDim. 1  -0.70550  0.56684  0.92092 -0.56673 -0.33775 -0.19619 -0.96505 0.32623\nDim. 2   0.24544 -0.23310 -0.11132  0.08972 -2.12776  0.87747 -0.18902 1.41148\n        laughter     let    like    look     lot     love     made     make\nMass     0.00235 0.00334 0.01013 0.00658 0.00591  0.00374  0.00279  0.00860\nChiDist  1.07469 0.61233 0.34748 0.62405 0.52565  0.70683  0.68018  0.47955\nInertia  0.00271 0.00125 0.00122 0.00256 0.00163  0.00187  0.00129  0.00198\nDim. 1   0.12941 0.07260 0.78828 1.24481 0.80809  0.52054 -0.15455 -1.25143\nDim. 2   2.16122 0.13605 0.15329 0.70219 1.22086 -0.01538 -0.06180  0.16861\n           mean   mexico military million    money     need      new      not\nMass    0.00355  0.00230  0.00245 0.00249  0.00398  0.00362  0.00638  0.01767\nChiDist 0.82322  0.94229  1.35259 0.85416  0.61222  0.78488  0.83331  0.29258\nInertia 0.00241  0.00204  0.00449 0.00182  0.00149  0.00223  0.00443  0.00151\nDim. 1  1.69345  1.91787  0.18755 0.14550  1.39654 -1.44421 -0.97159 -0.60096\nDim. 2  1.76899 -1.36846 -0.48560 0.32542 -0.44203  1.00628 -0.92025  0.43422\n           obama      ok     only     out      pay  people  percent      ph\nMass     0.00288 0.00363  0.00326  0.0116  0.00308 0.02483  0.00455 0.00232\nChiDist  0.82744 0.92138  0.73986  0.3476  0.86535 0.27217  0.70893 1.29998\nInertia  0.00197 0.00308  0.00179  0.0014  0.00231 0.00184  0.00229 0.00392\nDim. 1   0.38621 2.77864 -1.13106 -0.1124 -1.02251 0.07947  0.88837 1.52291\nDim. 2  -0.33202 1.50829 -1.05026  0.4996  1.00648 0.09766 -2.07962 1.61024\n           place     plan president       put   really remember   right    said\nMass     0.00227  0.00237   0.00591  0.003444  0.00565  0.00318 0.01144 0.01234\nChiDist  0.69791  1.07109   0.80612  0.521433  0.61536  0.71638 0.37559 0.56708\nInertia  0.00110  0.00272   0.00384  0.000936  0.00214  0.00163 0.00161 0.00397\nDim. 1  -0.05062 -1.55305  -2.08299 -0.392729 -0.55240  0.28864 0.86696 1.18909\nDim. 2  -0.23084 -1.41184   1.70011 -0.149998  1.89038 -1.08873 0.17230 1.61726\n            say     see     seen      she    she's    state   states     stop\nMass    0.00847 0.00582  0.00238  0.01542  0.00367  0.00411  0.00449  0.00235\nChiDist 0.37242 0.50491  0.76344  0.57268  0.73651  0.68547  0.49413  0.78727\nInertia 0.00118 0.00148  0.00138  0.00506  0.00199  0.00193  0.00110  0.00145\nDim. 1  0.60954 0.68700  0.37797  0.90144  1.54675 -0.61681 -0.09087  0.69153\nDim. 2  0.65385 0.91515 -0.17770 -1.18574 -0.69465 -1.31118 -0.51880 -1.66137\n           take     talk      tax     tell    thank  that's there's    these\nMass    0.00571  0.00238  0.00225  0.00549  0.00881 0.01031 0.00228  0.00734\nChiDist 0.46795  0.83659  1.36466  0.47710  0.75745 0.37205 0.73098  0.51526\nInertia 0.00125  0.00167  0.00418  0.00125  0.00505 0.00143 0.00122  0.00195\nDim. 1  0.54986 -0.31465 -1.08180 -0.05398 -1.08654 0.19579 0.28661  0.67819\nDim. 2  0.00448  1.34102 -0.33456  0.55411 -0.92050 0.48380 0.54697 -0.42013\n         they're   thing   things   think    those     time together      too\nMass     0.00977 0.00321 0.003849 0.00900  0.00454  0.00660  0.00307  0.00231\nChiDist  0.62821 0.65050 0.503108 0.51260  0.57317  0.45555  1.21279  0.78833\nInertia  0.00386 0.00136 0.000974 0.00237  0.00149  0.00137  0.00451  0.00144\nDim. 1   2.14070 1.26334 0.559702 0.42815 -1.18586 -0.29843 -4.07737 -0.99743\nDim. 2  -0.06408 1.27331 0.620920 1.61146  0.23093 -0.24678  1.51046  0.36597\n           trade    trump   united       up     very     vote     wall     want\nMass     0.00346  0.01794  0.00379  0.01013  0.01174  0.00471  0.00311  0.01230\nChiDist  0.94017  0.59361  0.60364  0.35919  0.54847  0.89731  0.86193  0.47207\nInertia  0.00305  0.00632  0.00138  0.00131  0.00353  0.00379  0.00231  0.00274\nDim. 1   1.18794  0.32062 -0.44590 -0.31289  0.78273 -1.42633  1.06967 -0.65314\nDim. 2  -1.90379 -0.04606 -0.91234  0.59942 -0.35590 -0.48455 -2.04138  0.88367\n           wants     was     way    we'll    we're     well     were   what's\nMass     0.00283 0.01763 0.00633  0.00258  0.02226  0.00521  0.00554  0.00357\nChiDist  0.64370 0.47071 0.37498  0.81727  0.44595  0.65637  0.63980  0.59124\nInertia  0.00117 0.00391 0.00089  0.00172  0.00443  0.00224  0.00227  0.00125\nDim. 1   0.55184 0.35683 0.56292  0.86412  1.07606 -0.15334 -0.32015  1.46406\nDim. 2  -0.77502 1.40292 0.12552 -1.16637 -0.54869  2.18059  0.97466 -0.12975\n             why      win     work  working    world    years  you're     your\nMass     0.00339  0.00437  0.00482  0.00265  0.00444  0.00598 0.00457  0.01139\nChiDist  0.65960  0.84503  1.02405  0.89373  0.52063  0.45065 0.60997  0.47686\nInertia  0.00147  0.00312  0.00505  0.00212  0.00120  0.00121 0.00170  0.00259\nDim. 1  -0.47816  1.28319 -3.13882 -2.07193 -0.36458  0.39163 1.03397 -0.91277\nDim. 2   0.74921 -1.31044  1.11788  0.40909 -0.48495 -0.53551 0.52202 -0.65435\n\n\n\n\n\nsummary(d_ca)\n\n\nPrincipal inertias (eigenvalues):\n\n dim    value      %   cum%   scree plot               \n 1      0.053657  14.1  14.1  ****                     \n 2      0.040702  10.7  24.8  ***                      \n 3      0.022246   5.8  30.6  *                        \n 4      0.019935   5.2  35.8  *                        \n 5      0.014300   3.8  39.6  *                        \n 6      0.013147   3.5  43.1  *                        \n 7      0.011372   3.0  46.0  *                        \n 8      0.010335   2.7  48.8  *                        \n 9      0.008489   2.2  51.0  *                        \n 10     0.007962   2.1  53.1  *                        \n 11     0.007012   1.8  54.9                           \n 12     0.006627   1.7  56.7                           \n 13     0.006017   1.6  58.2                           \n 14     0.005841   1.5  59.8                           \n 15     0.005661   1.5  61.3                           \n 16     0.005539   1.5  62.7                           \n 17     0.005120   1.3  64.1                           \n 18     0.004972   1.3  65.4                           \n 19     0.004851   1.3  66.6                           \n 20     0.004673   1.2  67.9                           \n 21     0.004558   1.2  69.1                           \n 22     0.004145   1.1  70.1                           \n 23     0.004037   1.1  71.2                           \n 24     0.003968   1.0  72.2                           \n 25     0.003885   1.0  73.3                           \n 26     0.003850   1.0  74.3                           \n 27     0.003568   0.9  75.2                           \n 28     0.003367   0.9  76.1                           \n 29     0.003301   0.9  77.0                           \n 30     0.003141   0.8  77.8                           \n 31     0.003067   0.8  78.6                           \n 32     0.002921   0.8  79.4                           \n 33     0.002839   0.7  80.1                           \n 34     0.002717   0.7  80.8                           \n 35     0.002664   0.7  81.5                           \n 36     0.002580   0.7  82.2                           \n 37     0.002534   0.7  82.9                           \n 38     0.002443   0.6  83.5                           \n 39     0.002341   0.6  84.1                           \n 40     0.002303   0.6  84.7                           \n 41     0.002214   0.6  85.3                           \n 42     0.002181   0.6  85.9                           \n 43     0.002148   0.6  86.4                           \n 44     0.002031   0.5  87.0                           \n 45     0.001955   0.5  87.5                           \n 46     0.001873   0.5  88.0                           \n 47     0.001861   0.5  88.5                           \n 48     0.001794   0.5  88.9                           \n 49     0.001711   0.4  89.4                           \n 50     0.001662   0.4  89.8                           \n 51     0.001653   0.4  90.3                           \n 52     0.001595   0.4  90.7                           \n 53     0.001496   0.4  91.1                           \n 54     0.001442   0.4  91.4                           \n 55     0.001406   0.4  91.8                           \n 56     0.001361   0.4  92.2                           \n 57     0.001311   0.3  92.5                           \n 58     0.001267   0.3  92.9                           \n 59     0.001257   0.3  93.2                           \n 60     0.001183   0.3  93.5                           \n 61     0.001174   0.3  93.8                           \n 62     0.001111   0.3  94.1                           \n 63     0.001072   0.3  94.4                           \n 64     0.001038   0.3  94.6                           \n 65     0.001011   0.3  94.9                           \n 66     0.000970   0.3  95.2                           \n 67     0.000924   0.2  95.4                           \n 68     0.000905   0.2  95.6                           \n 69     0.000842   0.2  95.9                           \n 70     0.000825   0.2  96.1                           \n 71     0.000774   0.2  96.3                           \n 72     0.000768   0.2  96.5                           \n 73     0.000747   0.2  96.7                           \n 74     0.000718   0.2  96.9                           \n 75     0.000704   0.2  97.1                           \n 76     0.000671   0.2  97.2                           \n 77     0.000659   0.2  97.4                           \n 78     0.000605   0.2  97.6                           \n 79     0.000595   0.2  97.7                           \n 80     0.000568   0.1  97.9                           \n 81     0.000536   0.1  98.0                           \n 82     0.000505   0.1  98.1                           \n 83     0.000481   0.1  98.3                           \n 84     0.000450   0.1  98.4                           \n 85     0.000431   0.1  98.5                           \n 86     0.000422   0.1  98.6                           \n 87     0.000389   0.1  98.7                           \n 88     0.000381   0.1  98.8                           \n 89     0.000343   0.1  98.9                           \n 90     0.000335   0.1  99.0                           \n 91     0.000315   0.1  99.1                           \n 92     0.000292   0.1  99.2                           \n 93     0.000275   0.1  99.2                           \n 94     0.000265   0.1  99.3                           \n 95     0.000243   0.1  99.4                           \n 96     0.000238   0.1  99.4                           \n 97     0.000217   0.1  99.5                           \n 98     0.000205   0.1  99.5                           \n 99     0.000183   0.0  99.6                           \n 100    0.000173   0.0  99.6                           \n 101    0.000164   0.0  99.7                           \n 102    0.000155   0.0  99.7                           \n 103    0.000152   0.0  99.7                           \n 104    0.000148   0.0  99.8                           \n 105    0.000128   0.0  99.8                           \n 106    0.000121   0.0  99.9                           \n 107    0.000113   0.0  99.9                           \n 108    8.9e-050   0.0  99.9                           \n 109    7.4e-050   0.0  99.9                           \n 110    6.8e-050   0.0  99.9                           \n 111    5.9e-050   0.0 100.0                           \n 112    5e-05000   0.0 100.0                           \n 113    4.5e-050   0.0 100.0                           \n 114    3.7e-050   0.0 100.0                           \n 115    2.3e-050   0.0 100.0                           \n 116    00000000   0.0 100.0                           \n 117    00000000   0.0 100.0                           \n        -------- -----                                 \n Total: 0.380871 100.0                                 \n\n\nRows:\n             name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  \n1   |  C_20160728 |    8  422   11 | -469 414  32 |   64   8   1 |\n2   |  C_20160729 |    5  242    6 | -270 170   7 |  177  73   4 |\n3   |  C_20160801 |    2  222    6 | -410 178   7 |  202  43   2 |\n4   |  C_20160805 |    8  354   10 | -258 146  10 |  307 208  20 |\n5   |  C_20160810 |    3  401    6 | -491 342  15 |  205  59   3 |\n6   |  C_20160811 |    8  330   15 | -479 317  33 |   97  13   2 |\n7   |  C_20160815 |   10  234   11 | -141  51   4 |  268 184  18 |\n8   |  C_20160816 |    5  215    6 | -257 140   6 |  187  75   4 |\n9   |  C_20160817 |    6  371   10 | -424 300  21 |  205  70   6 |\n10  |   C_2016082 |    4  276    9 | -389 195  12 |  249  80   7 |\n11  |   C_2016083 |    5  229   12 | -462 226  19 |   59   4   0 |\n12  | C_20160905A |    9  370   10 | -296 222  15 |  242 148  14 |\n13  | C_20160905B |    5  436    7 | -372 268  13 |  295 168  11 |\n14  |  C_20160906 |    8  307    7 | -269 232  11 |  153  75   5 |\n15  | C_20160908A |    3  102    8 | -152  21   1 |  302  82   6 |\n16  | C_20160908B |    4  306    7 | -421 265  14 |  165  41   3 |\n17  | C_20160908C |    5  378    5 | -282 200   8 |  266 178   9 |\n18  |   C_2016092 |    2  184    6 | -320 117   5 |  241  67   4 |\n19  |   C_2016093 |    5  442    8 | -516 433  25 |   75   9   1 |\n20  |   C_2016100 |    7  345    7 | -287 202  10 |  240 143   9 |\n21  |  C_20161024 |   11  479   11 | -377 400  30 |  168  79   8 |\n22  |  C_20161026 |    5  357    6 | -368 263  12 |  220  94   6 |\n23  | C_20161031A |    5  325   10 | -408 201  14 |  320 124  12 |\n24  | C_20161031B |    5  487    7 | -442 348  17 |  280 139   9 |\n25  | C_20161101A |    6  298    9 | -299 160  10 |  277 138  11 |\n26  | C_20161101B |    3  288    9 | -520 258  17 |  178  30   3 |\n27  | C_20161104A |    6  339    6 | -320 275  12 |  155  65   4 |\n28  | C_20161104B |    6  392    8 | -304 187  10 |  318 205  15 |\n29  |  C_20161105 |    1  205    4 | -450 157   5 |  248  48   2 |\n30  | C_20161106A |    4  561    8 | -589 512  28 |  181  49   3 |\n31  | C_20161106B |    5  419    9 | -477 326  20 |  254  92   7 |\n32  | C_20161107A |    5  130   20 | -388  92  13 |  250  38   7 |\n33  | C_20161107B |    6  462    7 | -407 357  18 |  220 104   7 |\n34  | C_20161107C |    2  368    8 | -656 347  19 |  159  20   1 |\n35  |  C_20161108 |    2  373    8 | -692 351  20 |  171  21   2 |\n36  |  C_20161109 |    3  204    7 | -426 196  10 |   84   8   1 |\n37  |  T_20160722 |   12  489   15 |  161  53   6 |  459 435  63 |\n38  |  T_20160725 |   17  330   10 |  185 149  11 |  203 180  17 |\n39  |  T_20160726 |    4   81    7 | -171  48   2 | -141  33   2 |\n40  | T_20160727A |   17  351   25 |  270 131  23 |  351 221  52 |\n41  | T_20160727B |   19  479   10 |  226 252  18 |  215 227  22 |\n42  |  T_20160801 |   18  309   17 |  293 242  29 |  155  67  11 |\n43  |  T_20160802 |   20  488   14 |  347 443  45 |  111  45   6 |\n44  |  T_20160804 |   14  223    7 |  159 139   7 |  124  84   5 |\n45  |  T_20160805 |   17  215    9 |  150 114   7 |  141 101   8 |\n46  |  T_20160808 |    7  283   20 | -359 111  16 | -446 172  33 |\n47  |  T_20160810 |   18  510   13 |  337 397  38 |  179 112  14 |\n48  | T_20160812A |   17  439    9 |  257 335  21 |  143 104   9 |\n49  | T_20160812B |   16  380   10 |  298 375  26 |   35   5   0 |\n50  |  T_20160815 |    6   84   16 | -289  83   9 |  -29   1   0 |\n51  |  T_20160816 |    6  362    8 | -263 147   8 | -318 216  15 |\n52  |  T_20160817 |   12  157   14 |  113  28   3 |  241 128  17 |\n53  |  T_20160818 |    6  477    9 | -341 230  14 | -354 247  20 |\n54  |  T_20160819 |    7  381    9 | -157  55   3 | -384 327  26 |\n55  |  T_20160822 |    8  280    5 | -100  38   1 | -251 241  12 |\n56  |  T_20160823 |    7  234   16 | -187  41   5 | -403 192  29 |\n57  | T_20160824A |   13  406    7 |   72  25   1 | -282 382  26 |\n58  | T_20160824B |    7  424    9 | -246 121   8 | -390 304  27 |\n59  |  T_20160825 |    8  257    8 |  -83  18   1 | -299 238  17 |\n60  |  T_20160830 |    7  177    6 | -112  42   2 | -201 135   7 |\n61  |  T_20160831 |   11   91    8 |  -74  20   1 | -139  71   5 |\n62  | T_20160901A |    5  301    7 |  -77  10   1 | -406 291  20 |\n63  | T_20160901B |    3  287    7 | -405 163   8 | -354 124   8 |\n64  | T_20160906A |   13  316   11 |  228 165  13 |  217 150  16 |\n65  | T_20160906B |    9  201    5 |   16   1   0 | -194 199   9 |\n66  | T_20160907A |    5  127   10 | -217  58   4 | -238  69   6 |\n67  | T_20160907B |    6  145   10 |  189  57   4 |  233  87   8 |\n68  |  T_20160908 |    6    2    7 |    6   0   0 |  -27   2   0 |\n69  | T_20160909A |    9   35    5 |   87  35   1 |   -7   0   0 |\n70  | T_20160909B |    3   99    5 |   80  11   0 | -226  88   4 |\n71  | Tr_20160909 |    9   35    5 |   87  35   1 |   -7   0   0 |\n72  | T_20160912A |    9   93    6 |  -45   8   0 | -149  85   5 |\n73  | T_20160912B |    3  123    8 | -266  74   4 | -217  49   4 |\n74  | T_20160913A |    7  258    6 |  -35   4   0 | -294 254  15 |\n75  | T_20160913B |    3   76   22 | -439  73  11 |  -82   3   1 |\n76  | Tr_20160913 |    7  258    6 |  -35   4   0 | -294 254  15 |\n77  |  T_20160916 |   10  108    6 |   58  15   1 | -148  93   5 |\n78  |  T_20160917 |    4   46    7 | -169  46   2 |   14   0   0 |\n79  |   T_2016092 |    8  142    6 |   47   8   0 | -192 134   7 |\n80  |   T_2016093 |   11  176    9 |   18   1   0 | -224 175  14 |\n81  | T_20161003A |   13   91   16 |  126  33   4 |  167  58   9 |\n82  | T_20161003B |   11   63    6 |  -37   6   0 | -110  56   3 |\n83  |  T_20161005 |   12  131    4 |  -16   2   0 | -136 129   5 |\n84  |  T_20161006 |   16  427    9 |  215 212  14 |  217 215  19 |\n85  | T_20161010A |   13  142    7 |  157 113   6 |  -79  28   2 |\n86  | T_20161010B |   15  287    7 |  217 278  13 |  -38   9   1 |\n87  |  T_20161011 |   13   72    5 |   86  48   2 |  -61  24   1 |\n88  |  T_20161012 |   14   94    9 |  152  94   6 |   -6   0   0 |\n89  |  T_20161013 |    7   51    8 | -106  28   2 |  -98  24   2 |\n90  | T_20161014A |   10  207    5 |  188 180   6 |  -73  27   1 |\n91  | T_20161014B |   14  161    6 |  159 157   7 |  -28   5   0 |\n92  |  T_20161017 |   10   41   15 |   66   8   1 | -138  33   5 |\n93  |  T_20161018 |    9   92    6 |   21   2   0 | -144  90   5 |\n94  | T_20161021A |    6  176    9 |  256 115   7 |  186  60   5 |\n95  | T_20161021B |   10  175    7 |  123  57   3 | -177 118   8 |\n96  | T_20161021C |    6  239    6 |  -33   3   0 | -291 236  13 |\n97  |  T_20161027 |    7  115    6 |   52   9   0 | -176 106   5 |\n98  |  T_20161028 |   10   86    7 |  128  64   3 |  -76  23   1 |\n99  |   T_2016103 |   13  161    7 |   59  17   1 | -173 144   9 |\n100 | T_20161101A |    9  258    5 |  -49  10   0 | -237 247  12 |\n101 | T_20161101B |    4   90    7 | -146  32   2 | -194  57   4 |\n102 | T_20161102A |    9  222    8 |   56  10   1 | -264 212  15 |\n103 | T_20161102B |   11   84    6 |  102  53   2 |  -78  31   2 |\n104 | T_20161102C |    7  115    4 |   59  17   0 | -144  99   4 |\n105 | T_20161103A |   11  167    7 |   77  25   1 | -181 141   9 |\n106 | T_20161103B |   10  248    5 |  138 114   4 | -150 134   6 |\n107 | T_20161104A |   10   65    7 |   96  37   2 |  -85  28   2 |\n108 | T_20161104B |    9   71    4 |   55  16   1 |  -99  54   2 |\n109 | T_20161104C |   12  208    7 |  200 184   9 |  -72  24   2 |\n110 | T_20161105A |   12  215    7 |  188 176   8 |  -89  39   2 |\n111 | T_20161105B |   10   74    7 |    2   0   0 | -146  74   5 |\n112 |  T_20161106 |    8  126    6 |  129  56   2 | -145  70   4 |\n113 | T_20161107A |   12   77   10 |  107  34   2 | -120  43   4 |\n114 | T_20161107B |   12  174    6 |  160 123   6 | -103  50   3 |\n115 | T_20161107C |    7  115    9 |   74  11   1 | -229 104   9 |\n116 | T_20161107D |    9  101    5 |  -27   3   0 | -144  98   5 |\n117 |  T_20161108 |    8  169    6 |   28   3   0 | -221 166  10 |\n118 |  T_20161109 |    3   53    7 | -194  41   2 |  101  11   1 |\n\nColumns:\n        name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  \n1   |   agan |    5  424    5 |  -11   0   0 | -386 424  19 |\n2   |    ago |    2  241    3 |  329 239   5 |  -30   2   0 |\n3   |   also |    4  303    6 | -308 196   8 | -226 106   6 |\n4   |  amerc |    7  618   11 | -583 607  46 |  -79  11   1 |\n5   | amercn |    6  470   16 | -454 205  24 | -516 265  41 |\n6   | amrcns |    3  522    8 | -714 496  28 | -163  26   2 |\n7   |   appl |   42  505   28 | -314 382  77 | -178 122  32 |\n8   |    are |   27  125    5 |  -69  64   2 |  -67  61   3 |\n9   |   adnc |    4  247   18 | -190  21   3 | -625 226  38 |\n10  |   back |    8   91    4 |   39   7   0 | -133  84   3 |\n11  |    bad |    3  403    5 |  457 380  13 | -113  23   1 |\n12  |   been |    9    7    4 |    3   0   0 |  -33   7   0 |\n13  |   beng |    3    3    2 |  -33   3   0 |   -9   0   0 |\n14  |   belv |    6  227    7 | -310 219  10 |   58   8   0 |\n15  |   bttr |    3   17    3 |  -74  17   0 |   -8   0   0 |\n16  |    big |    4  118    2 |  156 118   2 |   -7   0   0 |\n17  |   bong |    4  229   18 |   32   1   0 | -636 229  40 |\n18  |   brng |    3  235    4 |  -59   7   0 | -335 228   8 |\n19  |   buld |    3   82    4 |   71  11   0 | -179  71   2 |\n20  |   cmpg |    3  311    8 | -517 298  16 |  107  13   1 |\n21  |   cant |    4   75    4 |  105  33   1 |  117  42   1 |\n22  |   care |    4   13    9 |  -93  10   1 |  -50   3   0 |\n23  |   chng |    3  156    8 | -148  20   1 | -383 136  10 |\n24  |   clnt |   10  163   12 | -221 109   9 | -154  53   6 |\n25  |   come |    4   38    4 |  109  36   1 |   23   2   0 |\n26  |  cntrs |    2  153    3 |  160  51   1 | -227 102   3 |\n27  |  cntry |   14  465    6 | -106  66   3 | -260 399  23 |\n28  |    day |    3  137    4 | -160  54   1 |  198  83   3 |\n29  |   deal |    3  174    5 |  284 128   4 | -171  46   2 |\n30  |    did |    5  231    4 |  129  52   2 |  238 179   7 |\n31  |   ddnt |    3  211    4 |  240 115   3 |  218  95   3 |\n32  |   dsnt |    3   36    4 |  -23   1   0 |  132  35   1 |\n33  |   dong |    4  198    3 |  234 190   4 |   51   9   0 |\n34  |   dont |   13  485    6 |  282 438  19 |   92  47   3 |\n35  |   dnld |    4  426   12 | -488 230  19 |  451 196  21 |\n36  |   done |    3  103    4 | -175  52   2 |  175  51   2 |\n37  |   elct |    2  461   11 | -790 352  26 |  440 109  11 |\n38  |   even |    5   33    3 |   44   8   0 |   79  25   1 |\n39  |   ever |    5  237    3 |   51   9   0 | -251 228   7 |\n40  |   evry |    3   84    4 | -151  53   1 |  115  31   1 |\n41  |   flks |    4  374    5 |  347 258   9 | -233 116   5 |\n42  |   four |    2  200    3 |  176  56   1 | -284 144   5 |\n43  |    get |   12  104    4 |  -44  16   0 |  103  88   3 |\n44  |   give |    3   64    4 |  -44   3   0 |  182  61   2 |\n45  |     go |    6  267    5 |   19   1   0 |  280 266  12 |\n46  |   gong |   37  493   15 |  246 388  42 | -128 105  15 |\n47  |   gonn |    3   47   27 |  404  42   8 | -147   6   1 |\n48  |   good |    6  167    4 |   48   9   0 |  201 158   6 |\n49  |    got |    7  213    7 |  -51   7   0 |  281 206  14 |\n50  |   gvrn |    2  393    5 |  -89  10   0 | -557 384  18 |\n51  |   gret |   12  183    6 |  175 166   7 |  -56  17   1 |\n52  |    had |    7  287    6 |   26   2   0 |  317 285  17 |\n53  |   hppn |    3  259    4 |  227 120   3 | -244 139   5 |\n54  |    has |    9  215    7 | -244 210  10 |   37   5   0 |\n55  |    hes |    4  378    7 |  123  20   1 |  523 359  25 |\n56  |   help |    2  609    8 | -820 515  29 |  349  94   7 |\n57  |   hllr |   11  300    8 |    2   0   0 | -295 300  24 |\n58  |    ill |    2  118    3 |  194  82   2 |  128  36   1 |\n59  |     im |    9   57    5 |   75  30   1 |   71  27   1 |\n60  |    ive |    4  232    5 |  -54   7   0 |  308 226  10 |\n61  |   impr |    2  130    3 | -231 105   2 |  113  25   1 |\n62  |   indb |    3   13   49 |   30   0   0 |  280  13   6 |\n63  |     is |   35  352    8 | -163 323  17 |   50  30   2 |\n64  |   isis |    2   10   12 |  131   9   1 |  -47   1   0 |\n65  |    its |   20  418    6 |  213 413  17 |  -22   5   0 |\n66  |    job |    3   28    5 | -131  28   1 |   18   1   0 |\n67  |   jobs |    9  423   11 |  -78  14   1 | -429 409  42 |\n68  |   just |   12  273    4 |  -45  17   0 |  177 256   9 |\n69  |   keep |    2   77    4 | -224  75   2 |  -38   2   0 |\n70  |   know |   19  610    7 |   76  40   2 |  285 569  39 |\n71  |   lght |    2  165    7 |   30   1   0 |  436 165  11 |\n72  |    let |    3    3    3 |   17   1   0 |   27   2   0 |\n73  |   like |   10  284    3 |  183 276   6 |   31   8   0 |\n74  |   look |    7  265    7 |  288 214  10 |  142  52   3 |\n75  |    lot |    6  346    4 |  187 127   4 |  246 220   9 |\n76  |   love |    4   29    5 |  121  29   1 |   -3   0   0 |\n77  |   made |    3    3    3 |  -36   3   0 |  -12   0   0 |\n78  |   make |    9  370    5 | -290 365  13 |   34   5   0 |\n79  |   mean |    4  415    6 |  392 227  10 |  357 188  11 |\n80  |   mexc |    2  308    5 |  444 222   8 | -276  86   4 |\n81  |   mltr |    2    6   12 |   43   1   0 |  -98   5   1 |\n82  |   mlln |    2    7    5 |   34   2   0 |   66   6   0 |\n83  |   mony |    4  300    4 |  323 279   8 |  -89  21   1 |\n84  |   need |    4  249    6 | -335 182   8 |  203  67   4 |\n85  |    new |    6  123   12 | -225  73   6 | -186  50   5 |\n86  |    not |   18  316    4 | -139 226   6 |   88  90   3 |\n87  |   obam |    3   18    5 |   89  12   0 |  -67   7   0 |\n88  |     ok |    4  597    8 |  644 488  28 |  304 109   8 |\n89  |   only |    3  207    5 | -262 125   4 | -212  82   4 |\n90  |    out |   12   90    4 |  -26   6   0 |  101  84   3 |\n91  |    pay |    3  130    6 | -237  75   3 |  203  55   3 |\n92  |   pepl |   25   10    5 |   18   5   0 |   20   5   0 |\n93  |   prcn |    5  435    6 |  206  84   4 | -420 350  20 |\n94  |     ph |    2  136   10 |  353  74   5 |  325  62   6 |\n95  |   plac |    2    5    3 |  -12   0   0 |  -47   4   0 |\n96  |   plan |    2  184    7 | -360 113   6 | -285  71   5 |\n97  |   prsd |    6  539   10 | -483 358  26 |  343 181  17 |\n98  |    put |    3   34    2 |  -91  30   1 |  -30   3   0 |\n99  |   rlly |    6  427    6 | -128  43   2 |  381 384  20 |\n100 |   rmmb |    3  103    4 |   67   9   0 | -220  94   4 |\n101 |   rght |   11  294    4 |  201 286   9 |   35   9   0 |\n102 |   said |   12  567   10 |  275 236  17 |  326 331  32 |\n103 |    say |    8  269    3 |  141 144   3 |  132 125   4 |\n104 |    see |    6  233    4 |  159  99   3 |  185 134   5 |\n105 |   seen |    2   15    4 |   88  13   0 |  -36   2   0 |\n106 |    she |   15  307   13 |  209 133  13 | -239 174  22 |\n107 |   shes |    4  273    5 |  358 237   9 | -140  36   2 |\n108 |   stat |    4  192    5 | -143  43   2 | -265 149   7 |\n109 |   stts |    4   47    3 |  -21   2   0 | -105  45   1 |\n110 |   stop |    2  223    4 |  160  41   1 | -335 181   6 |\n111 |   take |    6   74    3 |  127  74   2 |    1   0   0 |\n112 |   talk |    2  112    4 |  -73   8   0 |  271 105   4 |\n113 |    tax |    2   36   11 | -251  34   3 |  -67   2   0 |\n114 |   tell |    5   56    3 |  -13   1   0 |  112  55   2 |\n115 |  thank |    9  171   13 | -252 110  10 | -186  60   7 |\n116 |   thts |   10   84    4 |   45  15   0 |   98  69   2 |\n117 |   thrs |    2   31    3 |   66   8   0 |  110  23   1 |\n118 |   thes |    7  120    5 |  157  93   3 |  -85  27   1 |\n119 |   thyr |   10  623   10 |  496 623  45 |  -13   0   0 |\n120 |  thing |    3  358    4 |  293 202   5 |  257 156   5 |\n121 |  thngs |    4  128    3 |  130  66   1 |  125  62   1 |\n122 |  think |    9  440    6 |   99  37   2 |  325 402  23 |\n123 |   thos |    5  236    4 | -275 230   6 |   47   7   0 |\n124 |   time |    7   35    4 |  -69  23   1 |  -50  12   0 |\n125 |   tgth |    3  670   12 | -944 606  51 |  305  63   7 |\n126 |    too |    2   95    4 | -231  86   2 |   74   9   0 |\n127 |   trad |    3  253    8 |  275  86   5 | -384 167  13 |\n128 |   trmp |   18   16   17 |   74  16   2 |   -9   0   0 |\n129 |   untd |    4  122    4 | -103  29   1 | -184  93   3 |\n130 |     up |   10  154    3 |  -72  41   1 |  121 113   4 |\n131 |   very |   12  126    9 |  181 109   7 |  -72  17   1 |\n132 |   vote |    5  147   10 | -330 136  10 |  -98  12   1 |\n133 |   wall |    3  311    6 |  248  83   4 | -412 228  13 |\n134 |   want |   12  245    7 | -151 103   5 |  178 143  10 |\n135 |   wnts |    3   98    3 |  128  39   1 | -156  59   2 |\n136 |    was |   18  392   10 |   83  31   2 |  283 362  35 |\n137 |    way |    6  125    2 |  130 121   2 |   25   5   0 |\n138 |   well |    3  143    5 |  200  60   2 | -235  83   4 |\n139 |   were |   22  374   12 |  249 312  26 | -111  62   7 |\n140 |   well |    5  452    6 |  -36   3   0 |  440 449  25 |\n141 |   were |    6  108    6 |  -74  13   1 |  197  94   5 |\n142 |   whts |    4  331    3 |  339 329   8 |  -26   2   0 |\n143 |    why |    3   81    4 | -111  28   1 |  151  53   2 |\n144 |    win |    4  222    8 |  297 124   7 | -264  98   8 |\n145 |   work |    5  553   13 | -727 504  47 |  226  49   6 |\n146 |   wrkn |    3  297    6 | -480 288  11 |   83   9   0 |\n147 |   wrld |    4   62    3 |  -84  26   1 |  -98  35   1 |\n148 |   yers |    6   98    3 |   91  41   1 | -108  57   2 |\n149 |  youre |    5  184    4 |  240 154   5 |  105  30   1 |\n150 |   your |   11  273    7 | -211 197   9 | -132  77   5 |\n\n\n\n\n\n\nLinguistic example"
  },
  {
    "objectID": "slides/correspondence-analysis.html#basic-plot",
    "href": "slides/correspondence-analysis.html#basic-plot",
    "title": "Correspondence analysis",
    "section": "Basic plot",
    "text": "Basic plot\n\nplot(d_ca)\n\n\n\nLinguistic example"
  },
  {
    "objectID": "slides/correspondence-analysis.html#customized-plot",
    "href": "slides/correspondence-analysis.html#customized-plot",
    "title": "Correspondence analysis",
    "section": "Customized plot",
    "text": "Customized plot\n\n\nCode\ntexts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"text\") %>% \n  mutate(Subcorpus = re_retrieve_first(fnames, \"/clinton_trump/([^/]+)\", requested_group = 1))\n\nwords_df <- col_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"word\")\n\ndim_1 <- sprintf(\"Dimension 1 (%.2f %%)\", summary(d_ca)$scree[1,3])\ndim_2 <- sprintf(\"Dimension 2 (%.2f %%)\", summary(d_ca)$scree[2,3])\n\nggplot(words_df, aes(x = V1, y = V2)) +\n  geom_text(aes(label = word), color = \"gray60\") +\n  geom_point(data = texts_df, aes(color = Subcorpus)) +\n  scale_color_manual(values = c(\"#0000CD\",\"#DC143C\")) +\n  geom_hline(yintercept = 0, color = \"darkgray\") +\n  geom_vline(xintercept = 0, color = \"darkgray\") +\n  theme_bw(base_size = 12) +\n  labs(x = dim_1, y = dim_2) +\n  coord_fixed()\n\n\n\n\nLinguistic example"
  },
  {
    "objectID": "slides/factor-analysis.html#outline",
    "href": "slides/factor-analysis.html#outline",
    "title": "Factor analysis",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nInterpretation\nData extraction"
  },
  {
    "objectID": "slides/factor-analysis.html#objective",
    "href": "slides/factor-analysis.html#objective",
    "title": "Factor analysis",
    "section": "Objective",
    "text": "Objective\n\nFind correlations between (numeric) variables in a dataset.\nInterpret the underlying (theoretically-relevant) dimensions in the dataset.\nDescribe groups of texts based on these dimensions.\n\n\nIntroduction"
  },
  {
    "objectID": "slides/factor-analysis.html#in-register-analysis",
    "href": "slides/factor-analysis.html#in-register-analysis",
    "title": "Factor analysis",
    "section": "In register analysis",
    "text": "In register analysis\nBiber (1988) finds 7 underlying dimensions to 69 variables and describes textual registers based on them.\nE.g. Narrative emphasis (past tense, 3PP, perfect aspect, public verbs … against adjectives, present tense.)\n\n\nCan we identify interpretable dimensions of register variation on the basis of the data and what are the relationships between the metaregisters, as well as more specific subsections, with regard to these dimensions?\n(Levshina 2015, 353)\n\n\nIntroduction"
  },
  {
    "objectID": "slides/factor-analysis.html#unlike-correspondence-analysis",
    "href": "slides/factor-analysis.html#unlike-correspondence-analysis",
    "title": "Factor analysis",
    "section": "Unlike Correspondence Analysis",
    "text": "Unlike Correspondence Analysis\n\n\n\n\n\n\n\nCorrespondence Analysis\nFactor Analysis\n\n\n\n\nCount data\nNumeric variables\n\n\nRelationships between columns and between rows\nCorrelations between variables to find underlying structure\n\n\nBiplot\nScatterplot of items based on factors\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/factor-analysis.html#our-setup",
    "href": "slides/factor-analysis.html#our-setup",
    "title": "Factor analysis",
    "section": "Our setup",
    "text": "Our setup\n\nTechnical/theoretical description based on Biber (1988).\nDataset based on the Brown corpus, see study for how the variables were created.\nBase R factanal() to run Factor Analysis and custom functions to examine it.\n\n\nlibrary(tidyverse)\nlibrary(GGally) # to plot correlogram\nlibrary(xml2) # to read examples from files\npath <- here::here(\"studies\", \"register-analysis.tsv\")\ndataset <- read_tsv(path, show_col_types = FALSE)\nfa <- dataset %>% data.frame(row.names = \"filename\") %>% \n  as.matrix() %>% factanal(factors = 4, scores = \"regression\")\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/factor-analysis.html#dataset-500-rows-14-columns",
    "href": "slides/factor-analysis.html#dataset-500-rows-14-columns",
    "title": "Factor analysis",
    "section": "Dataset (500 rows, 14 columns)",
    "text": "Dataset (500 rows, 14 columns)\n\n\nCode\ndataset %>% kbl() %>% kable_paper(font_size = 22) %>% \n  scroll_box(height = \"500px\", width = \"100vw\")\n\n\n\n\n \n  \n    filename \n    ttr \n    word_len \n    p_bigr \n    p_pobi \n    p_mw \n    p_c \n    p_nomin \n    p_noun \n    p_ppss \n    p_adj \n    p_neg \n    p_adv \n    p_qual \n  \n \n\n  \n    a01 \n    0.357 \n    4.99 \n    0.824 \n    0.207 \n    13.38 \n    0.113 \n    356.82 \n    0.244 \n    35.68 \n    464 \n    13.38 \n    205.2 \n    26.76 \n  \n  \n    a02 \n    0.381 \n    4.95 \n    0.836 \n    0.185 \n    17.57 \n    0.118 \n    210.80 \n    0.250 \n    39.53 \n    457 \n    8.78 \n    193.2 \n    13.18 \n  \n  \n    a03 \n    0.340 \n    4.88 \n    0.808 \n    0.187 \n    17.58 \n    0.110 \n    224.18 \n    0.256 \n    30.77 \n    541 \n    13.19 \n    162.6 \n    39.56 \n  \n  \n    a04 \n    0.383 \n    5.19 \n    0.840 \n    0.226 \n    4.51 \n    0.088 \n    370.04 \n    0.225 \n    18.05 \n    731 \n    4.51 \n    311.4 \n    99.28 \n  \n  \n    a05 \n    0.320 \n    4.76 \n    0.765 \n    0.206 \n    0.00 \n    0.094 \n    240.64 \n    0.223 \n    62.39 \n    584 \n    0.00 \n    196.1 \n    26.74 \n  \n  \n    a06 \n    0.350 \n    4.89 \n    0.813 \n    0.202 \n    53.03 \n    0.114 \n    304.90 \n    0.226 \n    128.15 \n    535 \n    8.84 \n    163.5 \n    13.26 \n  \n  \n    a07 \n    0.370 \n    5.13 \n    0.828 \n    0.202 \n    17.62 \n    0.105 \n    303.96 \n    0.224 \n    48.46 \n    604 \n    8.81 \n    163.0 \n    26.43 \n  \n  \n    a08 \n    0.356 \n    4.91 \n    0.810 \n    0.212 \n    4.57 \n    0.084 \n    251.49 \n    0.220 \n    45.73 \n    590 \n    4.57 \n    306.4 \n    77.73 \n  \n  \n    a09 \n    0.379 \n    4.94 \n    0.823 \n    0.179 \n    17.91 \n    0.097 \n    255.15 \n    0.269 \n    44.76 \n    389 \n    8.95 \n    179.1 \n    22.38 \n  \n  \n    a10 \n    0.368 \n    4.82 \n    0.832 \n    0.216 \n    4.38 \n    0.120 \n    153.37 \n    0.208 \n    100.79 \n    482 \n    4.38 \n    184.0 \n    48.20 \n  \n  \n    a11 \n    0.393 \n    4.67 \n    0.853 \n    0.194 \n    17.71 \n    0.110 \n    30.99 \n    0.191 \n    48.69 \n    527 \n    0.00 \n    301.0 \n    35.41 \n  \n  \n    a12 \n    0.338 \n    4.50 \n    0.828 \n    0.225 \n    119.76 \n    0.139 \n    55.60 \n    0.192 \n    141.15 \n    423 \n    38.49 \n    329.3 \n    68.44 \n  \n  \n    a13 \n    0.382 \n    4.48 \n    0.847 \n    0.213 \n    58.01 \n    0.107 \n    58.01 \n    0.187 \n    107.09 \n    424 \n    26.77 \n    312.4 \n    26.77 \n  \n  \n    a14 \n    0.380 \n    4.72 \n    0.838 \n    0.196 \n    34.35 \n    0.126 \n    115.93 \n    0.199 \n    55.82 \n    520 \n    4.29 \n    283.4 \n    34.35 \n  \n  \n    a15 \n    0.374 \n    4.53 \n    0.858 \n    0.228 \n    82.14 \n    0.129 \n    90.79 \n    0.169 \n    125.38 \n    406 \n    30.26 \n    337.2 \n    99.44 \n  \n  \n    a16 \n    0.417 \n    4.69 \n    0.862 \n    0.199 \n    88.46 \n    0.149 \n    80.03 \n    0.197 \n    50.55 \n    468 \n    21.06 \n    345.4 \n    21.06 \n  \n  \n    a17 \n    0.388 \n    4.56 \n    0.801 \n    0.183 \n    8.72 \n    0.126 \n    82.86 \n    0.198 \n    65.42 \n    388 \n    4.36 \n    170.1 \n    17.44 \n  \n  \n    a18 \n    0.360 \n    4.68 \n    0.774 \n    0.168 \n    21.61 \n    0.131 \n    133.97 \n    0.187 \n    64.82 \n    380 \n    4.32 \n    146.9 \n    4.32 \n  \n  \n    a19 \n    0.381 \n    4.92 \n    0.838 \n    0.208 \n    13.27 \n    0.114 \n    199.12 \n    0.243 \n    30.97 \n    385 \n    8.85 \n    261.1 \n    26.55 \n  \n  \n    a20 \n    0.377 \n    4.84 \n    0.844 \n    0.186 \n    0.00 \n    0.129 \n    116.58 \n    0.211 \n    51.81 \n    522 \n    0.00 \n    302.2 \n    17.27 \n  \n  \n    a21 \n    0.398 \n    4.97 \n    0.870 \n    0.200 \n    13.36 \n    0.108 \n    138.09 \n    0.213 \n    44.54 \n    388 \n    8.91 \n    245.0 \n    8.91 \n  \n  \n    a22 \n    0.362 \n    4.99 \n    0.817 \n    0.185 \n    0.00 \n    0.125 \n    268.51 \n    0.231 \n    38.98 \n    364 \n    0.00 \n    203.6 \n    17.32 \n  \n  \n    a23 \n    0.402 \n    4.93 \n    0.845 \n    0.177 \n    0.00 \n    0.135 \n    197.68 \n    0.230 \n    34.38 \n    486 \n    0.00 \n    159.0 \n    25.78 \n  \n  \n    a24 \n    0.393 \n    4.79 \n    0.827 \n    0.171 \n    0.00 \n    0.095 \n    182.06 \n    0.254 \n    31.08 \n    373 \n    0.00 \n    226.5 \n    4.44 \n  \n  \n    a25 \n    0.400 \n    4.95 \n    0.849 \n    0.184 \n    8.60 \n    0.139 \n    189.09 \n    0.231 \n    25.78 \n    451 \n    4.30 \n    219.2 \n    21.49 \n  \n  \n    a26 \n    0.360 \n    4.78 \n    0.824 \n    0.213 \n    21.95 \n    0.108 \n    232.66 \n    0.234 \n    74.63 \n    474 \n    0.00 \n    280.9 \n    39.51 \n  \n  \n    a27 \n    0.396 \n    4.92 \n    0.863 \n    0.213 \n    48.08 \n    0.118 \n    152.97 \n    0.226 \n    113.64 \n    734 \n    26.22 \n    240.4 \n    61.19 \n  \n  \n    a28 \n    0.363 \n    4.88 \n    0.841 \n    0.200 \n    18.02 \n    0.096 \n    283.78 \n    0.288 \n    45.05 \n    527 \n    9.01 \n    234.2 \n    45.05 \n  \n  \n    a29 \n    0.434 \n    4.96 \n    0.888 \n    0.197 \n    25.55 \n    0.141 \n    127.77 \n    0.234 \n    68.14 \n    664 \n    17.04 \n    225.7 \n    34.07 \n  \n  \n    a30 \n    0.386 \n    4.51 \n    0.842 \n    0.219 \n    92.35 \n    0.115 \n    74.76 \n    0.189 \n    167.11 \n    471 \n    52.77 \n    334.2 \n    39.58 \n  \n  \n    a31 \n    0.404 \n    4.79 \n    0.864 \n    0.205 \n    43.29 \n    0.127 \n    129.87 \n    0.223 \n    43.29 \n    515 \n    8.66 \n    281.4 \n    60.61 \n  \n  \n    a32 \n    0.404 \n    4.69 \n    0.845 \n    0.221 \n    38.78 \n    0.137 \n    146.49 \n    0.203 \n    180.96 \n    707 \n    17.23 \n    254.2 \n    103.40 \n  \n  \n    a33 \n    0.370 \n    4.83 \n    0.823 \n    0.203 \n    39.41 \n    0.110 \n    91.94 \n    0.212 \n    113.83 \n    591 \n    21.89 \n    236.4 \n    35.03 \n  \n  \n    a34 \n    0.328 \n    4.94 \n    0.784 \n    0.204 \n    4.47 \n    0.089 \n    250.56 \n    0.208 \n    13.42 \n    756 \n    0.00 \n    295.3 \n    31.32 \n  \n  \n    a35 \n    0.319 \n    5.15 \n    0.788 \n    0.211 \n    0.00 \n    0.095 \n    491.65 \n    0.228 \n    27.06 \n    970 \n    0.00 \n    360.8 \n    153.36 \n  \n  \n    a36 \n    0.376 \n    5.12 \n    0.838 \n    0.194 \n    17.95 \n    0.099 \n    332.14 \n    0.233 \n    58.35 \n    700 \n    17.95 \n    255.8 \n    80.79 \n  \n  \n    a37 \n    0.387 \n    5.09 \n    0.864 \n    0.204 \n    4.30 \n    0.130 \n    215.15 \n    0.199 \n    47.33 \n    818 \n    4.30 \n    284.0 \n    43.03 \n  \n  \n    a38 \n    0.318 \n    4.44 \n    0.795 \n    0.217 \n    9.01 \n    0.097 \n    90.09 \n    0.160 \n    45.05 \n    536 \n    0.00 \n    432.4 \n    112.61 \n  \n  \n    a39 \n    0.340 \n    4.56 \n    0.803 \n    0.217 \n    57.12 \n    0.183 \n    48.96 \n    0.167 \n    167.28 \n    510 \n    12.24 \n    322.3 \n    73.44 \n  \n  \n    a40 \n    0.416 \n    4.81 \n    0.885 \n    0.215 \n    29.91 \n    0.133 \n    141.03 \n    0.187 \n    94.02 \n    590 \n    8.55 \n    329.1 \n    64.10 \n  \n  \n    a41 \n    0.355 \n    5.03 \n    0.816 \n    0.213 \n    0.00 \n    0.134 \n    255.41 \n    0.167 \n    60.61 \n    632 \n    0.00 \n    281.4 \n    108.22 \n  \n  \n    a42 \n    0.380 \n    4.77 \n    0.834 \n    0.190 \n    12.83 \n    0.141 \n    89.82 \n    0.195 \n    89.82 \n    453 \n    8.55 \n    363.6 \n    47.05 \n  \n  \n    a43 \n    0.329 \n    4.93 \n    0.811 \n    0.207 \n    43.65 \n    0.122 \n    152.77 \n    0.239 \n    52.38 \n    646 \n    17.46 \n    257.5 \n    69.84 \n  \n  \n    a44 \n    0.342 \n    5.01 \n    0.824 \n    0.197 \n    35.13 \n    0.117 \n    232.76 \n    0.253 \n    79.05 \n    698 \n    21.96 \n    250.3 \n    87.83 \n  \n  \n    b01 \n    0.414 \n    5.10 \n    0.880 \n    0.215 \n    9.09 \n    0.085 \n    272.73 \n    0.247 \n    50.00 \n    759 \n    4.54 \n    286.4 \n    86.36 \n  \n  \n    b02 \n    0.391 \n    4.96 \n    0.869 \n    0.231 \n    4.48 \n    0.092 \n    192.48 \n    0.213 \n    35.81 \n    833 \n    4.48 \n    335.7 \n    116.38 \n  \n  \n    b03 \n    0.386 \n    4.66 \n    0.844 \n    0.234 \n    40.14 \n    0.105 \n    182.87 \n    0.190 \n    57.98 \n    669 \n    22.30 \n    352.4 \n    84.75 \n  \n  \n    b04 \n    0.416 \n    4.89 \n    0.865 \n    0.229 \n    8.97 \n    0.100 \n    228.70 \n    0.206 \n    58.30 \n    776 \n    8.97 \n    287.0 \n    58.30 \n  \n  \n    b05 \n    0.400 \n    4.93 \n    0.860 \n    0.218 \n    40.16 \n    0.095 \n    316.82 \n    0.237 \n    22.31 \n    647 \n    22.31 \n    276.7 \n    62.47 \n  \n  \n    b06 \n    0.396 \n    5.06 \n    0.858 \n    0.216 \n    4.48 \n    0.084 \n    309.28 \n    0.228 \n    35.86 \n    722 \n    4.48 \n    304.8 \n    58.27 \n  \n  \n    b07 \n    0.383 \n    5.00 \n    0.848 \n    0.216 \n    0.00 \n    0.099 \n    321.57 \n    0.236 \n    35.73 \n    732 \n    0.00 \n    281.4 \n    40.20 \n  \n  \n    b08 \n    0.311 \n    4.18 \n    0.817 \n    0.232 \n    71.01 \n    0.152 \n    91.90 \n    0.158 \n    317.46 \n    409 \n    45.95 \n    359.2 \n    79.36 \n  \n  \n    b09 \n    0.349 \n    4.61 \n    0.826 \n    0.229 \n    47.60 \n    0.124 \n    160.10 \n    0.177 \n    147.12 \n    597 \n    8.65 \n    510.6 \n    95.20 \n  \n  \n    b10 \n    0.320 \n    4.46 \n    0.832 \n    0.266 \n    169.94 \n    0.127 \n    108.93 \n    0.160 \n    217.87 \n    636 \n    95.86 \n    470.6 \n    130.72 \n  \n  \n    b11 \n    0.416 \n    5.16 \n    0.865 \n    0.220 \n    39.28 \n    0.122 \n    117.85 \n    0.205 \n    87.30 \n    803 \n    8.73 \n    344.8 \n    104.76 \n  \n  \n    b12 \n    0.377 \n    4.73 \n    0.831 \n    0.206 \n    8.71 \n    0.126 \n    183.01 \n    0.210 \n    69.72 \n    593 \n    4.36 \n    291.9 \n    52.29 \n  \n  \n    b13 \n    0.409 \n    4.78 \n    0.859 \n    0.216 \n    39.70 \n    0.117 \n    88.22 \n    0.216 \n    17.64 \n    865 \n    26.47 \n    277.9 \n    61.76 \n  \n  \n    b14 \n    0.378 \n    4.64 \n    0.849 \n    0.240 \n    48.59 \n    0.114 \n    145.76 \n    0.196 \n    83.92 \n    645 \n    35.34 \n    428.4 \n    106.01 \n  \n  \n    b15 \n    0.350 \n    4.81 \n    0.836 \n    0.228 \n    21.64 \n    0.094 \n    285.59 \n    0.212 \n    233.66 \n    710 \n    17.31 \n    294.2 \n    77.89 \n  \n  \n    b16 \n    0.362 \n    4.86 \n    0.846 \n    0.225 \n    8.85 \n    0.114 \n    309.74 \n    0.223 \n    146.02 \n    580 \n    8.85 \n    323.0 \n    44.25 \n  \n  \n    b17 \n    0.353 \n    4.59 \n    0.823 \n    0.214 \n    17.48 \n    0.117 \n    174.82 \n    0.220 \n    144.23 \n    503 \n    8.74 \n    236.0 \n    52.45 \n  \n  \n    b18 \n    0.353 \n    4.83 \n    0.809 \n    0.211 \n    9.12 \n    0.094 \n    282.59 \n    0.224 \n    164.08 \n    638 \n    9.12 \n    250.7 \n    41.02 \n  \n  \n    b19 \n    0.361 \n    4.66 \n    0.838 \n    0.235 \n    56.84 \n    0.121 \n    135.55 \n    0.198 \n    218.63 \n    621 \n    34.98 \n    362.9 \n    109.31 \n  \n  \n    b20 \n    0.389 \n    4.78 \n    0.857 \n    0.234 \n    17.41 \n    0.125 \n    221.93 \n    0.183 \n    60.92 \n    696 \n    17.41 \n    356.8 \n    100.09 \n  \n  \n    b21 \n    0.381 \n    4.83 \n    0.835 \n    0.214 \n    4.39 \n    0.110 \n    201.93 \n    0.204 \n    39.51 \n    825 \n    4.39 \n    381.9 \n    158.03 \n  \n  \n    b22 \n    0.356 \n    5.03 \n    0.830 \n    0.220 \n    17.44 \n    0.097 \n    296.43 \n    0.199 \n    65.39 \n    955 \n    17.44 \n    435.9 \n    126.42 \n  \n  \n    b23 \n    0.357 \n    4.76 \n    0.832 \n    0.220 \n    33.96 \n    0.146 \n    212.22 \n    0.160 \n    220.71 \n    688 \n    33.96 \n    462.6 \n    110.36 \n  \n  \n    b24 \n    0.409 \n    4.81 \n    0.863 \n    0.208 \n    8.53 \n    0.145 \n    76.73 \n    0.193 \n    34.10 \n    759 \n    8.53 \n    413.5 \n    183.29 \n  \n  \n    b25 \n    0.353 \n    4.89 \n    0.822 \n    0.223 \n    33.78 \n    0.147 \n    156.25 \n    0.171 \n    67.57 \n    777 \n    33.78 \n    367.4 \n    156.25 \n  \n  \n    b26 \n    0.373 \n    4.76 \n    0.842 \n    0.211 \n    38.86 \n    0.137 \n    211.57 \n    0.189 \n    207.25 \n    695 \n    25.91 \n    297.9 \n    60.45 \n  \n  \n    b27 \n    0.351 \n    4.73 \n    0.832 \n    0.238 \n    17.68 \n    0.107 \n    203.36 \n    0.195 \n    225.46 \n    469 \n    17.68 \n    344.8 \n    70.73 \n  \n  \n    c01 \n    0.409 \n    4.76 \n    0.862 \n    0.219 \n    41.41 \n    0.126 \n    136.65 \n    0.177 \n    78.67 \n    617 \n    20.70 \n    397.5 \n    103.52 \n  \n  \n    c02 \n    0.399 \n    4.95 \n    0.876 \n    0.211 \n    21.56 \n    0.118 \n    181.11 \n    0.213 \n    47.43 \n    750 \n    17.25 \n    426.9 \n    116.43 \n  \n  \n    c03 \n    0.389 \n    4.92 \n    0.849 \n    0.219 \n    4.24 \n    0.131 \n    191.00 \n    0.198 \n    59.42 \n    896 \n    4.24 \n    318.3 \n    182.51 \n  \n  \n    c04 \n    0.387 \n    4.68 \n    0.846 \n    0.230 \n    64.10 \n    0.133 \n    119.66 \n    0.167 \n    85.47 \n    765 \n    21.37 \n    350.4 \n    153.85 \n  \n  \n    c05 \n    0.401 \n    4.79 \n    0.861 \n    0.230 \n    37.53 \n    0.143 \n    141.78 \n    0.176 \n    116.76 \n    696 \n    25.02 \n    437.9 \n    175.15 \n  \n  \n    c06 \n    0.391 \n    4.77 \n    0.851 \n    0.223 \n    12.34 \n    0.114 \n    152.14 \n    0.185 \n    65.79 \n    810 \n    4.11 \n    370.1 \n    94.57 \n  \n  \n    c07 \n    0.390 \n    5.02 \n    0.860 \n    0.202 \n    20.65 \n    0.143 \n    140.44 \n    0.187 \n    45.44 \n    933 \n    0.00 \n    371.7 \n    115.66 \n  \n  \n    c08 \n    0.367 \n    4.63 \n    0.856 \n    0.233 \n    38.28 \n    0.126 \n    72.31 \n    0.184 \n    93.58 \n    736 \n    21.27 \n    391.3 \n    114.84 \n  \n  \n    c09 \n    0.385 \n    4.80 \n    0.829 \n    0.215 \n    17.07 \n    0.133 \n    132.31 \n    0.180 \n    55.48 \n    670 \n    8.54 \n    354.2 \n    102.43 \n  \n  \n    c10 \n    0.374 \n    4.69 \n    0.828 \n    0.197 \n    20.32 \n    0.160 \n    134.09 \n    0.189 \n    85.33 \n    740 \n    16.25 \n    394.1 \n    85.33 \n  \n  \n    c11 \n    0.397 \n    4.94 \n    0.851 \n    0.206 \n    4.03 \n    0.140 \n    169.35 \n    0.183 \n    32.26 \n    835 \n    0.00 \n    338.7 \n    100.81 \n  \n  \n    c12 \n    0.388 \n    5.01 \n    0.851 \n    0.207 \n    0.00 \n    0.122 \n    230.57 \n    0.230 \n    25.62 \n    670 \n    0.00 \n    286.1 \n    76.86 \n  \n  \n    c13 \n    0.389 \n    4.86 \n    0.853 \n    0.225 \n    8.76 \n    0.110 \n    127.08 \n    0.210 \n    70.11 \n    640 \n    8.76 \n    337.4 \n    122.70 \n  \n  \n    c14 \n    0.397 \n    4.91 \n    0.849 \n    0.207 \n    3.88 \n    0.129 \n    151.51 \n    0.197 \n    15.54 \n    905 \n    0.00 \n    345.8 \n    120.44 \n  \n  \n    c15 \n    0.452 \n    4.85 \n    0.898 \n    0.183 \n    0.00 \n    0.129 \n    59.93 \n    0.204 \n    19.98 \n    875 \n    0.00 \n    263.7 \n    95.89 \n  \n  \n    c16 \n    0.383 \n    4.84 \n    0.852 \n    0.223 \n    8.65 \n    0.126 \n    199.05 \n    0.174 \n    34.62 \n    723 \n    4.33 \n    419.7 \n    73.56 \n  \n  \n    c17 \n    0.389 \n    5.05 \n    0.860 \n    0.192 \n    16.88 \n    0.149 \n    147.68 \n    0.245 \n    33.76 \n    684 \n    8.44 \n    316.5 \n    118.14 \n  \n  \n    d01 \n    0.324 \n    4.58 \n    0.789 \n    0.223 \n    4.52 \n    0.093 \n    280.16 \n    0.198 \n    135.56 \n    651 \n    4.52 \n    402.2 \n    108.45 \n  \n  \n    d02 \n    0.303 \n    4.83 \n    0.780 \n    0.219 \n    4.28 \n    0.120 \n    364.18 \n    0.149 \n    179.95 \n    638 \n    4.28 \n    535.6 \n    145.67 \n  \n  \n    d03 \n    0.336 \n    5.02 \n    0.814 \n    0.220 \n    0.00 \n    0.112 \n    167.24 \n    0.172 \n    85.76 \n    896 \n    0.00 \n    467.4 \n    115.78 \n  \n  \n    d04 \n    0.254 \n    4.52 \n    0.741 \n    0.201 \n    12.94 \n    0.133 \n    319.24 \n    0.183 \n    56.08 \n    462 \n    12.94 \n    418.5 \n    86.28 \n  \n  \n    d05 \n    0.375 \n    4.87 \n    0.862 \n    0.233 \n    4.15 \n    0.122 \n    219.92 \n    0.154 \n    141.08 \n    560 \n    4.15 \n    456.4 \n    95.44 \n  \n  \n    d06 \n    0.316 \n    4.29 \n    0.811 \n    0.223 \n    8.63 \n    0.129 \n    86.32 \n    0.165 \n    237.38 \n    561 \n    0.00 \n    353.9 \n    107.90 \n  \n  \n    d07 \n    0.293 \n    4.19 \n    0.779 \n    0.230 \n    42.39 \n    0.143 \n    118.69 \n    0.173 \n    233.15 \n    382 \n    29.67 \n    343.4 \n    105.98 \n  \n  \n    d08 \n    0.307 \n    4.75 \n    0.738 \n    0.196 \n    8.58 \n    0.133 \n    141.51 \n    0.164 \n    68.61 \n    823 \n    8.58 \n    385.9 \n    94.34 \n  \n  \n    d09 \n    0.297 \n    5.06 \n    0.755 \n    0.168 \n    0.00 \n    0.111 \n    293.99 \n    0.255 \n    51.88 \n    679 \n    0.00 \n    263.7 \n    56.20 \n  \n  \n    d10 \n    0.374 \n    5.14 \n    0.872 \n    0.240 \n    0.00 \n    0.094 \n    361.93 \n    0.192 \n    125.11 \n    791 \n    0.00 \n    393.2 \n    93.83 \n  \n  \n    d11 \n    0.291 \n    4.70 \n    0.781 \n    0.206 \n    4.34 \n    0.115 \n    295.14 \n    0.169 \n    95.49 \n    825 \n    4.34 \n    473.1 \n    121.53 \n  \n  \n    d12 \n    0.346 \n    5.29 \n    0.833 \n    0.202 \n    0.00 \n    0.119 \n    372.94 \n    0.240 \n    56.38 \n    924 \n    0.00 \n    329.6 \n    78.06 \n  \n  \n    d13 \n    0.286 \n    4.22 \n    0.755 \n    0.192 \n    4.47 \n    0.099 \n    67.08 \n    0.205 \n    411.45 \n    438 \n    4.47 \n    438.3 \n    80.50 \n  \n  \n    d14 \n    0.351 \n    5.17 \n    0.804 \n    0.192 \n    17.57 \n    0.111 \n    316.34 \n    0.201 \n    83.48 \n    795 \n    17.57 \n    263.6 \n    70.30 \n  \n  \n    d15 \n    0.348 \n    4.99 \n    0.828 \n    0.197 \n    4.32 \n    0.130 \n    233.26 \n    0.199 \n    86.39 \n    946 \n    4.32 \n    293.7 \n    107.99 \n  \n  \n    d16 \n    0.220 \n    3.92 \n    0.635 \n    0.175 \n    24.48 \n    0.165 \n    89.76 \n    0.139 \n    326.40 \n    461 \n    24.48 \n    224.4 \n    28.56 \n  \n  \n    d17 \n    0.361 \n    4.57 \n    0.859 \n    0.239 \n    29.89 \n    0.140 \n    106.75 \n    0.186 \n    290.35 \n    500 \n    8.54 \n    405.6 \n    55.51 \n  \n  \n    e01 \n    0.335 \n    4.81 \n    0.831 \n    0.239 \n    137.29 \n    0.113 \n    150.58 \n    0.197 \n    155.00 \n    660 \n    39.86 \n    553.6 \n    119.58 \n  \n  \n    e02 \n    0.348 \n    4.41 \n    0.851 \n    0.227 \n    70.21 \n    0.115 \n    92.15 \n    0.194 \n    302.76 \n    763 \n    39.49 \n    373.0 \n    109.70 \n  \n  \n    e03 \n    0.297 \n    5.00 \n    0.793 \n    0.208 \n    26.58 \n    0.108 \n    190.52 \n    0.206 \n    168.37 \n    882 \n    26.58 \n    301.3 \n    199.38 \n  \n  \n    e04 \n    0.388 \n    5.11 \n    0.850 \n    0.204 \n    8.72 \n    0.114 \n    300.79 \n    0.207 \n    139.49 \n    924 \n    4.36 \n    326.9 \n    183.09 \n  \n  \n    e05 \n    0.298 \n    4.60 \n    0.779 \n    0.225 \n    35.26 \n    0.111 \n    83.74 \n    0.206 \n    114.59 \n    454 \n    17.63 \n    290.9 \n    88.14 \n  \n  \n    e06 \n    0.365 \n    4.84 \n    0.853 \n    0.208 \n    4.50 \n    0.100 \n    117.01 \n    0.246 \n    85.51 \n    693 \n    0.00 \n    396.0 \n    112.51 \n  \n  \n    e07 \n    0.257 \n    4.31 \n    0.731 \n    0.159 \n    33.76 \n    0.123 \n    71.73 \n    0.249 \n    84.39 \n    443 \n    21.10 \n    270.0 \n    46.41 \n  \n  \n    e08 \n    0.239 \n    4.79 \n    0.669 \n    0.161 \n    40.54 \n    0.093 \n    319.82 \n    0.268 \n    49.55 \n    468 \n    22.52 \n    256.8 \n    36.04 \n  \n  \n    e09 \n    0.319 \n    4.73 \n    0.719 \n    0.174 \n    0.00 \n    0.214 \n    44.43 \n    0.146 \n    36.35 \n    553 \n    0.00 \n    254.4 \n    68.66 \n  \n  \n    e10 \n    0.338 \n    4.62 \n    0.831 \n    0.207 \n    129.11 \n    0.125 \n    120.78 \n    0.227 \n    112.45 \n    816 \n    16.66 \n    354.0 \n    141.61 \n  \n  \n    e11 \n    0.380 \n    4.54 \n    0.835 \n    0.188 \n    0.00 \n    0.134 \n    30.00 \n    0.225 \n    55.72 \n    463 \n    0.00 \n    312.9 \n    60.01 \n  \n  \n    e12 \n    0.370 \n    5.02 \n    0.833 \n    0.184 \n    48.08 \n    0.120 \n    135.49 \n    0.220 \n    100.52 \n    975 \n    13.11 \n    240.4 \n    65.56 \n  \n  \n    e13 \n    0.308 \n    4.50 \n    0.750 \n    0.172 \n    4.42 \n    0.111 \n    61.92 \n    0.195 \n    172.49 \n    610 \n    0.00 \n    371.5 \n    101.72 \n  \n  \n    e14 \n    0.349 \n    4.63 \n    0.865 \n    0.166 \n    85.43 \n    0.135 \n    34.17 \n    0.278 \n    132.42 \n    577 \n    51.26 \n    316.1 \n    46.99 \n  \n  \n    e15 \n    0.255 \n    4.57 \n    0.763 \n    0.113 \n    0.00 \n    0.142 \n    132.88 \n    0.313 \n    17.14 \n    604 \n    0.00 \n    270.0 \n    51.44 \n  \n  \n    e16 \n    0.295 \n    4.41 \n    0.755 \n    0.169 \n    53.98 \n    0.099 \n    89.97 \n    0.242 \n    116.96 \n    580 \n    13.49 \n    341.9 \n    35.99 \n  \n  \n    e17 \n    0.330 \n    4.50 \n    0.818 \n    0.185 \n    26.74 \n    0.101 \n    133.69 \n    0.250 \n    120.32 \n    660 \n    22.28 \n    325.3 \n    80.21 \n  \n  \n    e18 \n    0.351 \n    4.72 \n    0.831 \n    0.184 \n    0.00 \n    0.119 \n    152.24 \n    0.204 \n    43.50 \n    452 \n    0.00 \n    282.7 \n    73.94 \n  \n  \n    e19 \n    0.376 \n    4.51 \n    0.849 \n    0.211 \n    13.29 \n    0.107 \n    88.57 \n    0.218 \n    270.15 \n    664 \n    13.29 \n    482.7 \n    110.72 \n  \n  \n    e20 \n    0.331 \n    4.64 \n    0.820 \n    0.208 \n    111.06 \n    0.111 \n    168.81 \n    0.222 \n    226.57 \n    720 \n    35.54 \n    395.4 \n    164.37 \n  \n  \n    e21 \n    0.280 \n    4.67 \n    0.758 \n    0.168 \n    0.00 \n    0.106 \n    267.98 \n    0.271 \n    62.53 \n    759 \n    0.00 \n    259.0 \n    160.79 \n  \n  \n    e22 \n    0.364 \n    5.22 \n    0.829 \n    0.174 \n    4.16 \n    0.156 \n    179.02 \n    0.204 \n    58.28 \n    1062 \n    0.00 \n    254.0 \n    49.96 \n  \n  \n    e23 \n    0.354 \n    4.72 \n    0.850 \n    0.189 \n    0.00 \n    0.130 \n    186.39 \n    0.209 \n    199.39 \n    798 \n    0.00 \n    398.8 \n    78.02 \n  \n  \n    e24 \n    0.315 \n    4.65 \n    0.828 \n    0.234 \n    13.32 \n    0.112 \n    115.40 \n    0.207 \n    124.28 \n    666 \n    8.88 \n    452.7 \n    133.16 \n  \n  \n    e25 \n    0.350 \n    5.16 \n    0.817 \n    0.164 \n    0.00 \n    0.092 \n    229.01 \n    0.269 \n    26.94 \n    817 \n    0.00 \n    282.9 \n    94.30 \n  \n  \n    e26 \n    0.362 \n    4.99 \n    0.852 \n    0.204 \n    8.92 \n    0.105 \n    276.42 \n    0.195 \n    62.42 \n    695 \n    0.00 \n    312.1 \n    62.42 \n  \n  \n    e27 \n    0.238 \n    4.95 \n    0.614 \n    0.127 \n    4.30 \n    0.140 \n    297.03 \n    0.319 \n    12.91 \n    375 \n    0.00 \n    314.2 \n    21.52 \n  \n  \n    e28 \n    0.317 \n    5.31 \n    0.813 \n    0.197 \n    21.82 \n    0.121 \n    401.57 \n    0.244 \n    170.23 \n    663 \n    13.10 \n    327.4 \n    113.49 \n  \n  \n    e29 \n    0.325 \n    5.14 \n    0.821 \n    0.195 \n    13.36 \n    0.102 \n    360.64 \n    0.249 \n    40.07 \n    779 \n    13.36 \n    365.1 \n    84.59 \n  \n  \n    e30 \n    0.345 \n    5.17 \n    0.825 \n    0.196 \n    65.39 \n    0.124 \n    261.55 \n    0.256 \n    274.63 \n    628 \n    26.16 \n    261.6 \n    61.03 \n  \n  \n    e31 \n    0.367 \n    5.17 \n    0.854 \n    0.205 \n    35.60 \n    0.109 \n    378.28 \n    0.264 \n    155.76 \n    694 \n    4.45 \n    289.3 \n    89.01 \n  \n  \n    e32 \n    0.324 \n    5.36 \n    0.824 \n    0.195 \n    4.28 \n    0.127 \n    380.50 \n    0.246 \n    119.71 \n    795 \n    4.28 \n    359.1 \n    158.19 \n  \n  \n    e33 \n    0.356 \n    4.66 \n    0.854 \n    0.219 \n    49.26 \n    0.101 \n    206.00 \n    0.246 \n    80.61 \n    699 \n    22.39 \n    322.4 \n    62.70 \n  \n  \n    e34 \n    0.341 \n    5.19 \n    0.838 \n    0.157 \n    8.70 \n    0.126 \n    134.84 \n    0.276 \n    30.45 \n    861 \n    4.35 \n    334.9 \n    65.25 \n  \n  \n    e35 \n    0.314 \n    4.59 \n    0.818 \n    0.218 \n    48.48 \n    0.115 \n    145.44 \n    0.239 \n    154.25 \n    511 \n    44.07 \n    374.6 \n    114.59 \n  \n  \n    e36 \n    0.308 \n    4.56 \n    0.792 \n    0.199 \n    67.20 \n    0.095 \n    112.01 \n    0.200 \n    232.97 \n    775 \n    26.88 \n    371.9 \n    174.73 \n  \n  \n    f01 \n    0.347 \n    5.02 \n    0.831 \n    0.212 \n    16.89 \n    0.149 \n    168.85 \n    0.203 \n    101.31 \n    688 \n    8.44 \n    472.8 \n    113.97 \n  \n  \n    f02 \n    0.357 \n    4.57 \n    0.833 \n    0.210 \n    26.50 \n    0.113 \n    70.67 \n    0.182 \n    110.42 \n    530 \n    8.83 \n    366.6 \n    75.09 \n  \n  \n    f03 \n    0.345 \n    4.98 \n    0.828 \n    0.211 \n    26.60 \n    0.112 \n    265.96 \n    0.199 \n    128.55 \n    878 \n    22.16 \n    430.0 \n    88.65 \n  \n  \n    f04 \n    0.367 \n    4.72 \n    0.848 \n    0.194 \n    22.14 \n    0.112 \n    150.58 \n    0.256 \n    97.43 \n    647 \n    17.71 \n    327.7 \n    70.86 \n  \n  \n    f05 \n    0.395 \n    4.70 \n    0.861 \n    0.181 \n    48.89 \n    0.105 \n    97.78 \n    0.231 \n    53.33 \n    653 \n    26.67 \n    368.9 \n    31.11 \n  \n  \n    f06 \n    0.309 \n    4.28 \n    0.815 \n    0.229 \n    111.39 \n    0.155 \n    185.64 \n    0.175 \n    391.91 \n    532 \n    33.00 \n    552.8 \n    57.76 \n  \n  \n    f07 \n    0.357 \n    4.92 \n    0.843 \n    0.196 \n    4.54 \n    0.087 \n    308.53 \n    0.216 \n    40.84 \n    1025 \n    4.54 \n    390.2 \n    145.19 \n  \n  \n    f08 \n    0.334 \n    4.86 \n    0.815 \n    0.207 \n    38.20 \n    0.143 \n    135.82 \n    0.204 \n    38.20 \n    688 \n    25.47 \n    539.0 \n    144.31 \n  \n  \n    f09 \n    0.336 \n    4.46 \n    0.807 \n    0.209 \n    39.05 \n    0.130 \n    108.46 \n    0.173 \n    138.83 \n    620 \n    17.35 \n    355.7 \n    78.09 \n  \n  \n    f10 \n    0.347 \n    4.76 \n    0.833 \n    0.207 \n    34.32 \n    0.139 \n    124.41 \n    0.218 \n    51.48 \n    639 \n    17.16 \n    330.3 \n    68.64 \n  \n  \n    f11 \n    0.330 \n    4.75 \n    0.829 \n    0.215 \n    17.32 \n    0.119 \n    186.15 \n    0.222 \n    43.29 \n    667 \n    0.00 \n    419.9 \n    73.59 \n  \n  \n    f12 \n    0.306 \n    4.66 \n    0.820 \n    0.233 \n    16.94 \n    0.150 \n    135.54 \n    0.173 \n    72.00 \n    601 \n    8.47 \n    495.6 \n    139.77 \n  \n  \n    f13 \n    0.279 \n    4.45 \n    0.769 \n    0.171 \n    51.81 \n    0.104 \n    189.98 \n    0.230 \n    228.84 \n    656 \n    34.54 \n    280.7 \n    94.99 \n  \n  \n    f14 \n    0.361 \n    4.94 \n    0.827 \n    0.218 \n    8.75 \n    0.112 \n    223.19 \n    0.202 \n    17.50 \n    551 \n    4.38 \n    267.0 \n    70.02 \n  \n  \n    f15 \n    0.352 \n    5.26 \n    0.826 \n    0.183 \n    4.47 \n    0.104 \n    406.98 \n    0.229 \n    35.78 \n    1042 \n    4.47 \n    313.1 \n    62.61 \n  \n  \n    f16 \n    0.346 \n    4.62 \n    0.821 \n    0.194 \n    4.29 \n    0.141 \n    55.79 \n    0.171 \n    25.75 \n    644 \n    4.29 \n    403.4 \n    64.38 \n  \n  \n    f17 \n    0.342 \n    4.68 \n    0.809 \n    0.183 \n    0.00 \n    0.095 \n    107.38 \n    0.206 \n    35.79 \n    501 \n    0.00 \n    187.9 \n    53.69 \n  \n  \n    f18 \n    0.368 \n    4.45 \n    0.827 \n    0.216 \n    17.18 \n    0.131 \n    94.50 \n    0.178 \n    330.76 \n    520 \n    17.18 \n    292.1 \n    137.46 \n  \n  \n    f19 \n    0.356 \n    5.04 \n    0.847 \n    0.210 \n    25.85 \n    0.109 \n    193.88 \n    0.211 \n    64.63 \n    844 \n    25.85 \n    323.1 \n    77.55 \n  \n  \n    f20 \n    0.408 \n    4.88 \n    0.833 \n    0.191 \n    8.78 \n    0.119 \n    188.76 \n    0.210 \n    48.29 \n    571 \n    4.39 \n    338.0 \n    57.07 \n  \n  \n    f21 \n    0.361 \n    4.70 \n    0.829 \n    0.209 \n    4.52 \n    0.092 \n    153.71 \n    0.221 \n    135.62 \n    687 \n    4.52 \n    316.5 \n    126.58 \n  \n  \n    f22 \n    0.313 \n    4.50 \n    0.778 \n    0.190 \n    32.91 \n    0.111 \n    98.72 \n    0.195 \n    41.13 \n    555 \n    20.57 \n    316.7 \n    49.36 \n  \n  \n    f23 \n    0.334 \n    4.90 \n    0.812 \n    0.228 \n    8.70 \n    0.122 \n    204.44 \n    0.174 \n    226.19 \n    805 \n    4.35 \n    456.7 \n    104.39 \n  \n  \n    f24 \n    0.373 \n    4.54 \n    0.837 \n    0.204 \n    33.56 \n    0.149 \n    67.11 \n    0.193 \n    247.48 \n    529 \n    8.39 \n    364.9 \n    130.03 \n  \n  \n    f25 \n    0.334 \n    5.02 \n    0.788 \n    0.208 \n    77.82 \n    0.129 \n    142.67 \n    0.201 \n    172.94 \n    986 \n    30.26 \n    354.5 \n    168.61 \n  \n  \n    f26 \n    0.327 \n    4.33 \n    0.818 \n    0.172 \n    0.00 \n    0.109 \n    75.29 \n    0.250 \n    44.29 \n    483 \n    0.00 \n    354.3 \n    48.72 \n  \n  \n    f27 \n    0.321 \n    4.51 \n    0.804 \n    0.210 \n    4.37 \n    0.118 \n    118.01 \n    0.183 \n    26.22 \n    734 \n    0.00 \n    511.4 \n    161.71 \n  \n  \n    f28 \n    0.345 \n    4.82 \n    0.829 \n    0.216 \n    0.00 \n    0.109 \n    247.24 \n    0.178 \n    79.47 \n    503 \n    0.00 \n    331.1 \n    66.22 \n  \n  \n    f29 \n    0.412 \n    5.04 \n    0.841 \n    0.194 \n    8.69 \n    0.126 \n    147.76 \n    0.207 \n    30.42 \n    665 \n    4.35 \n    260.8 \n    95.61 \n  \n  \n    f30 \n    0.388 \n    4.86 \n    0.853 \n    0.217 \n    4.43 \n    0.114 \n    119.58 \n    0.206 \n    17.71 \n    669 \n    0.00 \n    460.6 \n    88.57 \n  \n  \n    f31 \n    0.296 \n    4.52 \n    0.777 \n    0.225 \n    9.01 \n    0.094 \n    90.09 \n    0.159 \n    67.57 \n    324 \n    9.01 \n    436.9 \n    58.56 \n  \n  \n    f32 \n    0.340 \n    4.62 \n    0.806 \n    0.189 \n    0.00 \n    0.138 \n    137.81 \n    0.186 \n    64.60 \n    461 \n    0.00 \n    335.9 \n    51.68 \n  \n  \n    f33 \n    0.297 \n    4.80 \n    0.790 \n    0.214 \n    22.53 \n    0.091 \n    234.34 \n    0.224 \n    112.66 \n    888 \n    13.52 \n    342.5 \n    103.65 \n  \n  \n    f34 \n    0.325 \n    4.86 \n    0.789 \n    0.154 \n    0.00 \n    0.130 \n    65.22 \n    0.310 \n    30.43 \n    626 \n    0.00 \n    252.2 \n    82.61 \n  \n  \n    f35 \n    0.293 \n    4.42 \n    0.741 \n    0.170 \n    45.10 \n    0.166 \n    49.20 \n    0.218 \n    53.30 \n    439 \n    28.70 \n    352.6 \n    24.60 \n  \n  \n    f36 \n    0.300 \n    4.20 \n    0.786 \n    0.190 \n    93.14 \n    0.143 \n    67.74 \n    0.147 \n    93.14 \n    356 \n    50.80 \n    563.1 \n    88.91 \n  \n  \n    f37 \n    0.321 \n    4.78 \n    0.779 \n    0.215 \n    0.00 \n    0.089 \n    346.54 \n    0.191 \n    58.51 \n    878 \n    0.00 \n    333.0 \n    130.51 \n  \n  \n    f38 \n    0.357 \n    4.51 \n    0.842 \n    0.207 \n    8.32 \n    0.105 \n    83.16 \n    0.204 \n    37.42 \n    532 \n    0.00 \n    523.9 \n    70.69 \n  \n  \n    f39 \n    0.305 \n    5.53 \n    0.792 \n    0.177 \n    0.00 \n    0.117 \n    340.28 \n    0.250 \n    63.80 \n    710 \n    0.00 \n    285.0 \n    97.83 \n  \n  \n    f40 \n    0.368 \n    5.19 \n    0.809 \n    0.182 \n    0.00 \n    0.109 \n    302.22 \n    0.234 \n    35.56 \n    747 \n    0.00 \n    231.1 \n    88.89 \n  \n  \n    f41 \n    0.321 \n    4.59 \n    0.743 \n    0.168 \n    4.42 \n    0.100 \n    44.25 \n    0.165 \n    159.29 \n    659 \n    0.00 \n    336.3 \n    84.07 \n  \n  \n    f42 \n    0.327 \n    4.66 \n    0.797 \n    0.246 \n    25.49 \n    0.147 \n    165.68 \n    0.140 \n    191.16 \n    573 \n    25.49 \n    454.5 \n    161.43 \n  \n  \n    f43 \n    0.328 \n    4.64 \n    0.794 \n    0.203 \n    4.37 \n    0.118 \n    218.53 \n    0.231 \n    43.71 \n    625 \n    4.37 \n    402.1 \n    78.67 \n  \n  \n    f44 \n    0.284 \n    5.48 \n    0.733 \n    0.151 \n    0.00 \n    0.103 \n    547.88 \n    0.252 \n    53.45 \n    1131 \n    0.00 \n    267.3 \n    66.81 \n  \n  \n    f45 \n    0.292 \n    4.84 \n    0.748 \n    0.181 \n    4.42 \n    0.105 \n    101.72 \n    0.187 \n    57.50 \n    610 \n    4.42 \n    247.7 \n    48.65 \n  \n  \n    f46 \n    0.407 \n    5.13 \n    0.868 \n    0.223 \n    13.23 \n    0.110 \n    224.97 \n    0.214 \n    61.76 \n    609 \n    4.41 \n    330.8 \n    70.58 \n  \n  \n    f47 \n    0.318 \n    4.38 \n    0.830 \n    0.220 \n    13.19 \n    0.101 \n    123.08 \n    0.177 \n    351.65 \n    690 \n    13.19 \n    470.3 \n    79.12 \n  \n  \n    f48 \n    0.308 \n    4.93 \n    0.815 \n    0.208 \n    0.00 \n    0.092 \n    319.53 \n    0.201 \n    22.50 \n    869 \n    0.00 \n    535.6 \n    99.01 \n  \n  \n    g01 \n    0.340 \n    4.88 \n    0.829 \n    0.244 \n    13.11 \n    0.100 \n    218.44 \n    0.169 \n    183.49 \n    878 \n    13.11 \n    393.2 \n    152.91 \n  \n  \n    g02 \n    0.337 \n    5.30 \n    0.805 \n    0.199 \n    0.00 \n    0.121 \n    381.12 \n    0.219 \n    12.99 \n    996 \n    0.00 \n    398.4 \n    103.94 \n  \n  \n    g03 \n    0.353 \n    4.87 \n    0.833 \n    0.200 \n    29.14 \n    0.144 \n    99.92 \n    0.217 \n    124.90 \n    666 \n    16.65 \n    283.1 \n    87.43 \n  \n  \n    g04 \n    0.330 \n    4.32 \n    0.799 \n    0.211 \n    25.45 \n    0.122 \n    55.13 \n    0.180 \n    237.49 \n    611 \n    8.48 \n    373.2 \n    122.99 \n  \n  \n    g05 \n    0.334 \n    4.48 \n    0.810 \n    0.201 \n    12.54 \n    0.136 \n    45.97 \n    0.211 \n    79.40 \n    631 \n    12.54 \n    326.0 \n    125.37 \n  \n  \n    g06 \n    0.360 \n    4.81 \n    0.837 \n    0.215 \n    8.34 \n    0.132 \n    120.98 \n    0.165 \n    162.70 \n    505 \n    4.17 \n    438.0 \n    54.23 \n  \n  \n    g07 \n    0.363 \n    5.19 \n    0.821 \n    0.195 \n    0.00 \n    0.129 \n    350.95 \n    0.183 \n    99.65 \n    789 \n    0.00 \n    234.0 \n    103.99 \n  \n  \n    g08 \n    0.369 \n    5.05 \n    0.838 \n    0.217 \n    4.31 \n    0.128 \n    193.80 \n    0.167 \n    21.53 \n    991 \n    4.31 \n    491.0 \n    129.20 \n  \n  \n    g09 \n    0.343 \n    4.89 \n    0.818 \n    0.178 \n    8.78 \n    0.117 \n    276.56 \n    0.221 \n    43.90 \n    571 \n    4.39 \n    298.5 \n    61.46 \n  \n  \n    g10 \n    0.279 \n    4.87 \n    0.747 \n    0.203 \n    0.00 \n    0.110 \n    243.58 \n    0.192 \n    134.84 \n    592 \n    0.00 \n    343.6 \n    86.99 \n  \n  \n    g11 \n    0.347 \n    4.94 \n    0.837 \n    0.212 \n    0.00 \n    0.099 \n    227.98 \n    0.206 \n    205.63 \n    787 \n    0.00 \n    473.8 \n    165.40 \n  \n  \n    g12 \n    0.303 \n    4.31 \n    0.804 \n    0.244 \n    25.73 \n    0.131 \n    180.10 \n    0.152 \n    231.56 \n    540 \n    21.44 \n    416.0 \n    94.34 \n  \n  \n    g13 \n    0.350 \n    4.85 \n    0.804 \n    0.176 \n    4.33 \n    0.132 \n    263.95 \n    0.214 \n    147.12 \n    749 \n    0.00 \n    268.3 \n    64.91 \n  \n  \n    g14 \n    0.371 \n    4.51 \n    0.844 \n    0.194 \n    12.93 \n    0.125 \n    73.28 \n    0.219 \n    185.34 \n    634 \n    12.93 \n    344.8 \n    103.45 \n  \n  \n    g15 \n    0.326 \n    4.80 \n    0.786 \n    0.176 \n    7.96 \n    0.172 \n    202.86 \n    0.181 \n    43.76 \n    656 \n    7.96 \n    342.1 \n    182.97 \n  \n  \n    g16 \n    0.332 \n    5.10 \n    0.816 \n    0.190 \n    8.98 \n    0.090 \n    431.27 \n    0.229 \n    76.37 \n    988 \n    8.98 \n    242.6 \n    89.85 \n  \n  \n    g17 \n    0.337 \n    4.70 \n    0.810 \n    0.240 \n    72.90 \n    0.162 \n    206.56 \n    0.157 \n    182.26 \n    701 \n    36.45 \n    360.5 \n    125.56 \n  \n  \n    g18 \n    0.368 \n    4.61 \n    0.824 \n    0.202 \n    0.00 \n    0.126 \n    104.85 \n    0.180 \n    61.16 \n    773 \n    0.00 \n    340.8 \n    104.85 \n  \n  \n    g19 \n    0.336 \n    4.86 \n    0.775 \n    0.192 \n    4.43 \n    0.113 \n    150.51 \n    0.191 \n    39.84 \n    912 \n    4.43 \n    301.0 \n    110.67 \n  \n  \n    g20 \n    0.325 \n    4.86 \n    0.816 \n    0.198 \n    0.00 \n    0.102 \n    266.84 \n    0.230 \n    74.37 \n    626 \n    0.00 \n    306.2 \n    96.24 \n  \n  \n    g21 \n    0.339 \n    4.87 \n    0.801 \n    0.202 \n    17.82 \n    0.110 \n    258.35 \n    0.185 \n    187.08 \n    846 \n    4.45 \n    320.7 \n    66.81 \n  \n  \n    g22 \n    0.366 \n    5.18 \n    0.861 \n    0.204 \n    43.67 \n    0.117 \n    248.91 \n    0.210 \n    187.77 \n    860 \n    43.67 \n    458.5 \n    109.17 \n  \n  \n    g23 \n    0.375 \n    4.87 \n    0.852 \n    0.223 \n    4.32 \n    0.134 \n    189.98 \n    0.186 \n    155.44 \n    734 \n    4.32 \n    323.8 \n    64.77 \n  \n  \n    g24 \n    0.336 \n    4.89 \n    0.810 \n    0.216 \n    4.52 \n    0.081 \n    330.17 \n    0.201 \n    67.84 \n    660 \n    4.52 \n    339.2 \n    94.98 \n  \n  \n    g25 \n    0.282 \n    5.04 \n    0.749 \n    0.181 \n    17.74 \n    0.110 \n    492.24 \n    0.222 \n    57.65 \n    785 \n    17.74 \n    354.8 \n    53.22 \n  \n  \n    g26 \n    0.325 \n    4.58 \n    0.795 \n    0.206 \n    18.22 \n    0.074 \n    127.56 \n    0.213 \n    132.12 \n    697 \n    13.67 \n    309.8 \n    27.34 \n  \n  \n    g27 \n    0.363 \n    4.70 \n    0.834 \n    0.192 \n    4.24 \n    0.147 \n    114.45 \n    0.178 \n    118.69 \n    644 \n    4.24 \n    288.3 \n    110.22 \n  \n  \n    g28 \n    0.340 \n    4.93 \n    0.806 \n    0.225 \n    0.00 \n    0.093 \n    212.57 \n    0.162 \n    85.93 \n    760 \n    0.00 \n    348.3 \n    135.69 \n  \n  \n    g29 \n    0.348 \n    4.80 \n    0.818 \n    0.212 \n    0.00 \n    0.105 \n    290.18 \n    0.189 \n    218.75 \n    754 \n    0.00 \n    281.2 \n    84.82 \n  \n  \n    g30 \n    0.338 \n    5.40 \n    0.831 \n    0.216 \n    12.89 \n    0.138 \n    249.14 \n    0.182 \n    42.95 \n    979 \n    12.89 \n    451.0 \n    98.80 \n  \n  \n    g31 \n    0.342 \n    4.76 \n    0.820 \n    0.218 \n    4.35 \n    0.128 \n    152.31 \n    0.154 \n    34.81 \n    631 \n    0.00 \n    352.5 \n    95.73 \n  \n  \n    g32 \n    0.348 \n    4.56 \n    0.838 \n    0.222 \n    47.54 \n    0.130 \n    129.65 \n    0.158 \n    159.90 \n    449 \n    17.29 \n    328.4 \n    64.82 \n  \n  \n    g33 \n    0.335 \n    4.37 \n    0.825 \n    0.234 \n    8.56 \n    0.121 \n    68.52 \n    0.143 \n    428.27 \n    505 \n    4.28 \n    565.3 \n    205.57 \n  \n  \n    g34 \n    0.354 \n    4.73 \n    0.835 \n    0.211 \n    8.51 \n    0.149 \n    200.00 \n    0.173 \n    114.89 \n    451 \n    8.51 \n    366.0 \n    72.34 \n  \n  \n    g35 \n    0.357 \n    5.08 \n    0.844 \n    0.189 \n    4.38 \n    0.100 \n    328.23 \n    0.218 \n    205.69 \n    932 \n    4.38 \n    393.9 \n    135.67 \n  \n  \n    g36 \n    0.300 \n    4.71 \n    0.774 \n    0.221 \n    96.56 \n    0.159 \n    96.56 \n    0.144 \n    235.10 \n    449 \n    54.58 \n    302.3 \n    29.39 \n  \n  \n    g37 \n    0.368 \n    4.62 \n    0.837 \n    0.222 \n    4.40 \n    0.117 \n    83.52 \n    0.159 \n    175.82 \n    593 \n    4.40 \n    439.6 \n    158.24 \n  \n  \n    g38 \n    0.329 \n    4.74 \n    0.782 \n    0.193 \n    0.00 \n    0.132 \n    211.48 \n    0.178 \n    56.11 \n    505 \n    0.00 \n    336.6 \n    77.69 \n  \n  \n    g39 \n    0.350 \n    4.66 \n    0.826 \n    0.203 \n    61.58 \n    0.156 \n    110.84 \n    0.172 \n    102.63 \n    452 \n    8.21 \n    361.2 \n    90.31 \n  \n  \n    g40 \n    0.362 \n    4.51 \n    0.825 \n    0.209 \n    34.51 \n    0.135 \n    103.54 \n    0.180 \n    163.93 \n    483 \n    21.57 \n    362.4 \n    94.91 \n  \n  \n    g41 \n    0.293 \n    4.23 \n    0.756 \n    0.199 \n    4.39 \n    0.108 \n    65.91 \n    0.173 \n    57.12 \n    435 \n    4.39 \n    329.5 \n    79.09 \n  \n  \n    g42 \n    0.366 \n    5.25 \n    0.839 \n    0.197 \n    9.01 \n    0.086 \n    342.19 \n    0.222 \n    139.58 \n    900 \n    9.01 \n    283.7 \n    85.55 \n  \n  \n    g43 \n    0.323 \n    4.82 \n    0.796 \n    0.188 \n    0.00 \n    0.126 \n    366.33 \n    0.220 \n    91.58 \n    667 \n    0.00 \n    322.7 \n    87.22 \n  \n  \n    g44 \n    0.330 \n    5.04 \n    0.823 \n    0.196 \n    4.47 \n    0.104 \n    268.10 \n    0.200 \n    134.05 \n    795 \n    4.47 \n    415.6 \n    89.37 \n  \n  \n    g45 \n    0.356 \n    4.63 \n    0.824 \n    0.220 \n    33.98 \n    0.131 \n    101.95 \n    0.162 \n    140.19 \n    523 \n    16.99 \n    276.1 \n    63.72 \n  \n  \n    g46 \n    0.358 \n    5.09 \n    0.826 \n    0.188 \n    4.52 \n    0.094 \n    266.49 \n    0.205 \n    0.00 \n    944 \n    0.00 \n    307.1 \n    94.85 \n  \n  \n    g47 \n    0.307 \n    4.44 \n    0.761 \n    0.203 \n    69.87 \n    0.178 \n    98.64 \n    0.148 \n    456.23 \n    399 \n    32.88 \n    394.6 \n    65.76 \n  \n  \n    g48 \n    0.373 \n    4.83 \n    0.852 \n    0.201 \n    0.00 \n    0.089 \n    131.58 \n    0.224 \n    27.22 \n    526 \n    0.00 \n    308.5 \n    58.98 \n  \n  \n    g49 \n    0.330 \n    4.52 \n    0.841 \n    0.236 \n    16.56 \n    0.131 \n    169.77 \n    0.150 \n    178.05 \n    518 \n    16.56 \n    405.8 \n    111.80 \n  \n  \n    g50 \n    0.412 \n    4.77 \n    0.878 \n    0.212 \n    8.65 \n    0.122 \n    69.20 \n    0.196 \n    12.98 \n    714 \n    4.33 \n    320.1 \n    47.58 \n  \n  \n    g51 \n    0.392 \n    4.61 \n    0.851 \n    0.227 \n    35.12 \n    0.122 \n    127.31 \n    0.189 \n    175.59 \n    707 \n    13.17 \n    346.8 \n    74.63 \n  \n  \n    g52 \n    0.341 \n    4.69 \n    0.808 \n    0.197 \n    0.00 \n    0.108 \n    127.47 \n    0.166 \n    61.54 \n    659 \n    0.00 \n    246.2 \n    109.89 \n  \n  \n    g53 \n    0.354 \n    4.76 \n    0.834 \n    0.196 \n    0.00 \n    0.131 \n    134.37 \n    0.159 \n    73.69 \n    312 \n    0.00 \n    316.4 \n    47.68 \n  \n  \n    g54 \n    0.301 \n    4.33 \n    0.788 \n    0.229 \n    103.09 \n    0.131 \n    94.50 \n    0.158 \n    158.94 \n    361 \n    51.55 \n    347.9 \n    94.50 \n  \n  \n    g55 \n    0.326 \n    4.51 \n    0.802 \n    0.222 \n    8.83 \n    0.113 \n    225.07 \n    0.190 \n    229.48 \n    781 \n    8.83 \n    238.3 \n    57.37 \n  \n  \n    g56 \n    0.322 \n    4.64 \n    0.800 \n    0.207 \n    0.00 \n    0.116 \n    184.05 \n    0.191 \n    131.46 \n    609 \n    0.00 \n    337.4 \n    100.79 \n  \n  \n    g57 \n    0.349 \n    5.35 \n    0.829 \n    0.169 \n    0.00 \n    0.127 \n    391.66 \n    0.226 \n    114.94 \n    1060 \n    0.00 \n    302.3 \n    123.46 \n  \n  \n    g58 \n    0.344 \n    4.80 \n    0.818 \n    0.214 \n    8.75 \n    0.118 \n    139.98 \n    0.163 \n    56.87 \n    442 \n    4.37 \n    345.6 \n    135.61 \n  \n  \n    g59 \n    0.362 \n    4.65 \n    0.832 \n    0.209 \n    8.65 \n    0.091 \n    151.38 \n    0.165 \n    69.20 \n    718 \n    0.00 \n    354.7 \n    142.73 \n  \n  \n    g60 \n    0.283 \n    4.13 \n    0.783 \n    0.203 \n    79.96 \n    0.108 \n    71.08 \n    0.150 \n    177.70 \n    404 \n    75.52 \n    351.0 \n    111.06 \n  \n  \n    g61 \n    0.339 \n    5.11 \n    0.789 \n    0.209 \n    17.64 \n    0.102 \n    255.84 \n    0.185 \n    70.58 \n    838 \n    4.41 \n    401.4 \n    92.63 \n  \n  \n    g62 \n    0.388 \n    4.96 \n    0.824 \n    0.176 \n    13.13 \n    0.111 \n    227.57 \n    0.235 \n    17.50 \n    731 \n    4.38 \n    188.2 \n    48.14 \n  \n  \n    g63 \n    0.378 \n    4.89 \n    0.821 \n    0.177 \n    4.53 \n    0.091 \n    217.29 \n    0.211 \n    99.59 \n    996 \n    4.53 \n    190.1 \n    54.32 \n  \n  \n    g64 \n    0.352 \n    4.46 \n    0.845 \n    0.201 \n    0.00 \n    0.126 \n    76.69 \n    0.154 \n    127.82 \n    213 \n    0.00 \n    238.6 \n    12.78 \n  \n  \n    g65 \n    0.367 \n    5.11 \n    0.841 \n    0.208 \n    0.00 \n    0.091 \n    290.83 \n    0.204 \n    44.74 \n    743 \n    0.00 \n    340.0 \n    67.11 \n  \n  \n    g66 \n    0.341 \n    4.36 \n    0.803 \n    0.220 \n    0.00 \n    0.118 \n    43.44 \n    0.141 \n    238.92 \n    504 \n    0.00 \n    412.7 \n    52.13 \n  \n  \n    g67 \n    0.323 \n    4.47 \n    0.794 \n    0.217 \n    8.37 \n    0.127 \n    108.83 \n    0.134 \n    121.39 \n    339 \n    8.37 \n    401.8 \n    71.16 \n  \n  \n    g68 \n    0.349 \n    4.76 \n    0.821 \n    0.227 \n    4.26 \n    0.116 \n    132.03 \n    0.173 \n    55.37 \n    549 \n    0.00 \n    408.9 \n    80.92 \n  \n  \n    g69 \n    0.373 \n    5.15 \n    0.819 \n    0.195 \n    8.31 \n    0.138 \n    253.43 \n    0.217 \n    49.85 \n    694 \n    0.00 \n    398.8 \n    128.79 \n  \n  \n    g70 \n    0.322 \n    4.48 \n    0.821 \n    0.235 \n    17.68 \n    0.113 \n    159.15 \n    0.146 \n    353.67 \n    676 \n    17.68 \n    610.1 \n    172.41 \n  \n  \n    g71 \n    0.348 \n    4.75 \n    0.818 \n    0.221 \n    13.07 \n    0.125 \n    196.08 \n    0.184 \n    56.65 \n    645 \n    0.00 \n    418.3 \n    100.22 \n  \n  \n    g72 \n    0.321 \n    5.00 \n    0.782 \n    0.186 \n    13.35 \n    0.102 \n    507.34 \n    0.202 \n    57.85 \n    1015 \n    13.35 \n    298.2 \n    89.01 \n  \n  \n    g73 \n    0.328 \n    4.81 \n    0.787 \n    0.203 \n    21.43 \n    0.128 \n    145.74 \n    0.199 \n    38.58 \n    784 \n    21.43 \n    325.8 \n    107.16 \n  \n  \n    g74 \n    0.338 \n    4.98 \n    0.837 \n    0.231 \n    41.53 \n    0.165 \n    207.64 \n    0.166 \n    149.50 \n    826 \n    16.61 \n    361.3 \n    91.36 \n  \n  \n    g75 \n    0.324 \n    4.30 \n    0.801 \n    0.226 \n    43.71 \n    0.125 \n    113.64 \n    0.154 \n    174.82 \n    616 \n    43.71 \n    371.5 \n    83.04 \n  \n  \n    h01 \n    0.274 \n    5.62 \n    0.711 \n    0.144 \n    0.00 \n    0.144 \n    716.06 \n    0.292 \n    4.14 \n    836 \n    0.00 \n    190.4 \n    16.56 \n  \n  \n    h02 \n    0.289 \n    5.24 \n    0.778 \n    0.164 \n    8.47 \n    0.097 \n    347.46 \n    0.221 \n    177.97 \n    856 \n    8.47 \n    266.9 \n    156.78 \n  \n  \n    h03 \n    0.305 \n    4.58 \n    0.777 \n    0.216 \n    0.00 \n    0.084 \n    240.15 \n    0.174 \n    172.18 \n    766 \n    0.00 \n    312.6 \n    149.52 \n  \n  \n    h04 \n    0.290 \n    5.33 \n    0.753 \n    0.163 \n    0.00 \n    0.106 \n    380.13 \n    0.312 \n    12.96 \n    583 \n    0.00 \n    311.0 \n    95.03 \n  \n  \n    h05 \n    0.279 \n    4.95 \n    0.751 \n    0.196 \n    0.00 \n    0.091 \n    318.67 \n    0.232 \n    44.88 \n    794 \n    0.00 \n    291.7 \n    58.35 \n  \n  \n    h06 \n    0.339 \n    5.55 \n    0.788 \n    0.145 \n    0.00 \n    0.117 \n    456.24 \n    0.282 \n    16.59 \n    883 \n    0.00 \n    141.0 \n    24.89 \n  \n  \n    h07 \n    0.274 \n    5.12 \n    0.716 \n    0.166 \n    0.00 \n    0.126 \n    357.14 \n    0.220 \n    12.31 \n    878 \n    0.00 \n    266.8 \n    69.79 \n  \n  \n    h08 \n    0.264 \n    4.76 \n    0.623 \n    0.175 \n    0.00 \n    0.116 \n    198.45 \n    0.217 \n    146.68 \n    393 \n    0.00 \n    271.8 \n    17.26 \n  \n  \n    h09 \n    0.254 \n    5.06 \n    0.640 \n    0.112 \n    0.00 \n    0.140 \n    281.52 \n    0.244 \n    8.16 \n    722 \n    0.00 \n    167.3 \n    16.32 \n  \n  \n    h10 \n    0.317 \n    5.67 \n    0.740 \n    0.123 \n    0.00 \n    0.122 \n    306.96 \n    0.284 \n    4.32 \n    1064 \n    0.00 \n    86.5 \n    12.97 \n  \n  \n    h11 \n    0.346 \n    5.47 \n    0.827 \n    0.142 \n    0.00 \n    0.092 \n    301.69 \n    0.310 \n    8.87 \n    763 \n    0.00 \n    217.4 \n    53.24 \n  \n  \n    h12 \n    0.173 \n    4.67 \n    0.509 \n    0.120 \n    0.00 \n    0.117 \n    468.49 \n    0.233 \n    4.15 \n    448 \n    0.00 \n    186.6 \n    4.15 \n  \n  \n    h13 \n    0.337 \n    4.85 \n    0.799 \n    0.199 \n    8.32 \n    0.110 \n    187.11 \n    0.206 \n    87.32 \n    765 \n    8.32 \n    303.5 \n    41.58 \n  \n  \n    h14 \n    0.198 \n    5.00 \n    0.562 \n    0.133 \n    0.00 \n    0.143 \n    388.43 \n    0.238 \n    8.26 \n    545 \n    0.00 \n    289.3 \n    4.13 \n  \n  \n    h15 \n    0.288 \n    4.91 \n    0.750 \n    0.152 \n    0.00 \n    0.109 \n    367.65 \n    0.264 \n    47.58 \n    670 \n    0.00 \n    298.4 \n    90.83 \n  \n  \n    h16 \n    0.285 \n    5.00 \n    0.717 \n    0.171 \n    0.00 \n    0.105 \n    294.37 \n    0.253 \n    8.53 \n    478 \n    0.00 \n    221.8 \n    17.07 \n  \n  \n    h17 \n    0.260 \n    4.84 \n    0.650 \n    0.167 \n    7.88 \n    0.118 \n    370.22 \n    0.223 \n    47.26 \n    441 \n    3.94 \n    303.3 \n    47.26 \n  \n  \n    h18 \n    0.307 \n    4.77 \n    0.780 \n    0.202 \n    26.67 \n    0.090 \n    373.33 \n    0.197 \n    186.67 \n    729 \n    26.67 \n    346.7 \n    97.78 \n  \n  \n    h19 \n    0.306 \n    4.98 \n    0.777 \n    0.191 \n    0.00 \n    0.082 \n    346.40 \n    0.244 \n    68.37 \n    747 \n    0.00 \n    355.5 \n    68.37 \n  \n  \n    h20 \n    0.279 \n    5.30 \n    0.728 \n    0.132 \n    0.00 \n    0.098 \n    432.25 \n    0.312 \n    8.31 \n    698 \n    0.00 \n    228.6 \n    58.19 \n  \n  \n    h21 \n    0.323 \n    5.30 \n    0.794 \n    0.169 \n    4.36 \n    0.092 \n    348.43 \n    0.255 \n    65.33 \n    849 \n    4.36 \n    322.3 \n    74.04 \n  \n  \n    h22 \n    0.207 \n    5.04 \n    0.558 \n    0.118 \n    0.00 \n    0.109 \n    593.98 \n    0.247 \n    7.82 \n    492 \n    0.00 \n    132.9 \n    7.82 \n  \n  \n    h23 \n    0.237 \n    5.28 \n    0.713 \n    0.163 \n    0.00 \n    0.111 \n    280.46 \n    0.265 \n    21.91 \n    771 \n    0.00 \n    298.0 \n    65.73 \n  \n  \n    h24 \n    0.203 \n    4.49 \n    0.619 \n    0.144 \n    0.00 \n    0.108 \n    190.51 \n    0.222 \n    287.80 \n    559 \n    0.00 \n    214.8 \n    4.05 \n  \n  \n    h25 \n    0.325 \n    4.92 \n    0.791 \n    0.206 \n    8.68 \n    0.128 \n    264.64 \n    0.220 \n    143.17 \n    725 \n    4.34 \n    277.7 \n    52.06 \n  \n  \n    h26 \n    0.320 \n    5.03 \n    0.790 \n    0.185 \n    0.00 \n    0.089 \n    313.64 \n    0.233 \n    18.18 \n    632 \n    0.00 \n    322.7 \n    50.00 \n  \n  \n    h27 \n    0.343 \n    5.31 \n    0.824 \n    0.177 \n    0.00 \n    0.088 \n    467.08 \n    0.270 \n    88.97 \n    801 \n    0.00 \n    231.3 \n    80.07 \n  \n  \n    h28 \n    0.305 \n    5.48 \n    0.745 \n    0.138 \n    0.00 \n    0.123 \n    332.33 \n    0.295 \n    12.95 \n    678 \n    0.00 \n    82.0 \n    4.32 \n  \n  \n    h29 \n    0.323 \n    5.12 \n    0.775 \n    0.179 \n    0.00 \n    0.084 \n    346.07 \n    0.250 \n    62.92 \n    849 \n    0.00 \n    323.6 \n    67.42 \n  \n  \n    h30 \n    0.366 \n    5.41 \n    0.853 \n    0.190 \n    9.03 \n    0.097 \n    438.12 \n    0.255 \n    90.33 \n    727 \n    9.03 \n    329.7 \n    72.27 \n  \n  \n    j01 \n    0.266 \n    5.06 \n    0.690 \n    0.159 \n    4.57 \n    0.089 \n    429.42 \n    0.250 \n    4.57 \n    767 \n    4.57 \n    278.7 \n    118.78 \n  \n  \n    j02 \n    0.273 \n    4.84 \n    0.686 \n    0.127 \n    0.00 \n    0.076 \n    247.48 \n    0.315 \n    9.17 \n    577 \n    0.00 \n    256.6 \n    41.25 \n  \n  \n    j03 \n    0.263 \n    4.68 \n    0.707 \n    0.166 \n    0.00 \n    0.083 \n    267.57 \n    0.240 \n    36.28 \n    812 \n    0.00 \n    185.9 \n    27.21 \n  \n  \n    j04 \n    0.302 \n    4.99 \n    0.763 \n    0.162 \n    0.00 \n    0.098 \n    282.78 \n    0.282 \n    34.28 \n    805 \n    0.00 \n    329.9 \n    68.55 \n  \n  \n    j05 \n    0.319 \n    5.40 \n    0.806 \n    0.169 \n    4.39 \n    0.104 \n    232.87 \n    0.244 \n    13.18 \n    909 \n    4.39 \n    487.7 \n    105.45 \n  \n  \n    j06 \n    0.288 \n    5.17 \n    0.708 \n    0.137 \n    0.00 \n    0.058 \n    484.40 \n    0.312 \n    13.97 \n    587 \n    0.00 \n    265.5 \n    69.86 \n  \n  \n    j07 \n    0.274 \n    5.04 \n    0.721 \n    0.146 \n    4.22 \n    0.100 \n    333.33 \n    0.267 \n    8.44 \n    696 \n    4.22 \n    219.4 \n    92.83 \n  \n  \n    j08 \n    0.334 \n    5.23 \n    0.807 \n    0.174 \n    0.00 \n    0.092 \n    272.11 \n    0.244 \n    22.68 \n    1025 \n    0.00 \n    285.7 \n    95.24 \n  \n  \n    j09 \n    0.258 \n    5.12 \n    0.705 \n    0.123 \n    0.00 \n    0.116 \n    295.39 \n    0.281 \n    4.34 \n    626 \n    0.00 \n    212.9 \n    21.72 \n  \n  \n    j10 \n    0.331 \n    4.50 \n    0.821 \n    0.197 \n    0.00 \n    0.115 \n    79.58 \n    0.215 \n    114.94 \n    645 \n    0.00 \n    477.5 \n    106.10 \n  \n  \n    j11 \n    0.315 \n    4.80 \n    0.804 \n    0.207 \n    8.78 \n    0.115 \n    193.24 \n    0.207 \n    21.96 \n    716 \n    8.78 \n    360.1 \n    83.44 \n  \n  \n    j12 \n    0.262 \n    5.08 \n    0.690 \n    0.161 \n    0.00 \n    0.136 \n    146.05 \n    0.200 \n    17.18 \n    1117 \n    0.00 \n    425.3 \n    77.32 \n  \n  \n    j13 \n    0.225 \n    4.75 \n    0.662 \n    0.148 \n    0.00 \n    0.119 \n    383.61 \n    0.267 \n    17.44 \n    602 \n    0.00 \n    283.3 \n    74.11 \n  \n  \n    j14 \n    0.308 \n    5.38 \n    0.767 \n    0.161 \n    4.35 \n    0.121 \n    308.96 \n    0.235 \n    17.41 \n    592 \n    4.35 \n    295.9 \n    87.03 \n  \n  \n    j15 \n    0.340 \n    5.42 \n    0.764 \n    0.128 \n    0.00 \n    0.131 \n    233.06 \n    0.243 \n    0.00 \n    1148 \n    0.00 \n    263.3 \n    60.42 \n  \n  \n    j16 \n    0.239 \n    5.18 \n    0.638 \n    0.141 \n    0.00 \n    0.101 \n    155.00 \n    0.273 \n    8.86 \n    757 \n    0.00 \n    336.6 \n    53.14 \n  \n  \n    j17 \n    0.293 \n    5.84 \n    0.766 \n    0.138 \n    0.00 \n    0.093 \n    375.38 \n    0.234 \n    43.65 \n    1174 \n    0.00 \n    357.9 \n    34.92 \n  \n  \n    j18 \n    0.158 \n    4.20 \n    0.527 \n    0.150 \n    4.37 \n    0.121 \n    144.10 \n    0.248 \n    152.84 \n    734 \n    4.37 \n    515.3 \n    39.30 \n  \n  \n    j19 \n    0.224 \n    4.68 \n    0.694 \n    0.168 \n    0.00 \n    0.150 \n    257.18 \n    0.235 \n    128.59 \n    532 \n    0.00 \n    330.0 \n    47.15 \n  \n  \n    j20 \n    0.177 \n    4.28 \n    0.611 \n    0.158 \n    4.52 \n    0.095 \n    244.23 \n    0.257 \n    131.16 \n    642 \n    4.52 \n    307.6 \n    4.52 \n  \n  \n    j21 \n    0.136 \n    4.20 \n    0.520 \n    0.138 \n    0.00 \n    0.093 \n    112.16 \n    0.285 \n    49.35 \n    502 \n    0.00 \n    273.7 \n    13.46 \n  \n  \n    j22 \n    0.349 \n    5.42 \n    0.842 \n    0.174 \n    4.58 \n    0.079 \n    302.06 \n    0.224 \n    118.99 \n    1071 \n    4.58 \n    389.0 \n    82.38 \n  \n  \n    j23 \n    0.332 \n    5.31 \n    0.798 \n    0.176 \n    8.56 \n    0.136 \n    282.41 \n    0.218 \n    4.28 \n    1121 \n    8.56 \n    380.8 \n    111.25 \n  \n  \n    j24 \n    0.375 \n    5.42 \n    0.847 \n    0.183 \n    0.00 \n    0.096 \n    368.87 \n    0.252 \n    53.98 \n    1044 \n    0.00 \n    436.3 \n    80.97 \n  \n  \n    j25 \n    0.278 \n    5.32 \n    0.725 \n    0.160 \n    4.36 \n    0.110 \n    514.83 \n    0.267 \n    8.73 \n    833 \n    4.36 \n    231.2 \n    61.08 \n  \n  \n    j26 \n    0.330 \n    5.45 \n    0.800 \n    0.147 \n    43.50 \n    0.131 \n    413.22 \n    0.246 \n    17.40 \n    900 \n    4.35 \n    278.4 \n    56.55 \n  \n  \n    j27 \n    0.305 \n    4.86 \n    0.815 \n    0.224 \n    0.00 \n    0.109 \n    186.09 \n    0.191 \n    119.63 \n    625 \n    0.00 \n    514.0 \n    124.06 \n  \n  \n    j28 \n    0.269 \n    5.28 \n    0.744 \n    0.218 \n    35.67 \n    0.099 \n    468.12 \n    0.189 \n    222.92 \n    633 \n    13.38 \n    396.8 \n    89.17 \n  \n  \n    j29 \n    0.315 \n    5.42 \n    0.777 \n    0.180 \n    8.97 \n    0.088 \n    470.85 \n    0.270 \n    62.78 \n    704 \n    4.48 \n    251.1 \n    130.04 \n  \n  \n    j30 \n    0.309 \n    4.86 \n    0.798 \n    0.204 \n    13.23 \n    0.110 \n    180.86 \n    0.206 \n    141.16 \n    494 \n    8.82 \n    242.6 \n    105.87 \n  \n  \n    j31 \n    0.308 \n    4.78 \n    0.794 \n    0.220 \n    17.13 \n    0.138 \n    141.33 \n    0.156 \n    141.33 \n    698 \n    8.56 \n    479.7 \n    197.00 \n  \n  \n    j32 \n    0.207 \n    4.79 \n    0.600 \n    0.139 \n    0.00 \n    0.083 \n    410.33 \n    0.297 \n    41.49 \n    281 \n    0.00 \n    272.0 \n    36.88 \n  \n  \n    j33 \n    0.237 \n    4.63 \n    0.701 \n    0.251 \n    84.78 \n    0.098 \n    182.95 \n    0.187 \n    156.18 \n    736 \n    8.93 \n    562.2 \n    66.93 \n  \n  \n    j34 \n    0.306 \n    5.23 \n    0.813 \n    0.197 \n    13.59 \n    0.094 \n    271.74 \n    0.202 \n    67.94 \n    987 \n    13.59 \n    484.6 \n    244.56 \n  \n  \n    j35 \n    0.332 \n    4.92 \n    0.823 \n    0.203 \n    33.33 \n    0.150 \n    195.83 \n    0.193 \n    108.33 \n    775 \n    8.33 \n    366.7 \n    195.83 \n  \n  \n    j36 \n    0.335 \n    4.87 \n    0.821 \n    0.214 \n    0.00 \n    0.105 \n    181.34 \n    0.152 \n    79.61 \n    721 \n    0.00 \n    362.7 \n    92.88 \n  \n  \n    j37 \n    0.341 \n    5.43 \n    0.796 \n    0.186 \n    27.46 \n    0.084 \n    389.02 \n    0.225 \n    27.46 \n    897 \n    4.58 \n    384.4 \n    137.30 \n  \n  \n    j38 \n    0.294 \n    5.31 \n    0.791 \n    0.182 \n    0.00 \n    0.072 \n    321.99 \n    0.263 \n    13.80 \n    1150 \n    0.00 \n    299.0 \n    73.60 \n  \n  \n    j39 \n    0.336 \n    4.88 \n    0.796 \n    0.202 \n    12.83 \n    0.129 \n    192.39 \n    0.221 \n    102.61 \n    646 \n    8.55 \n    513.0 \n    132.53 \n  \n  \n    j40 \n    0.300 \n    4.96 \n    0.743 \n    0.174 \n    0.00 \n    0.079 \n    327.04 \n    0.254 \n    64.49 \n    871 \n    0.00 \n    345.5 \n    152.00 \n  \n  \n    j41 \n    0.250 \n    4.89 \n    0.691 \n    0.160 \n    0.00 \n    0.100 \n    253.45 \n    0.270 \n    93.38 \n    627 \n    0.00 \n    337.9 \n    57.80 \n  \n  \n    j42 \n    0.308 \n    5.24 \n    0.791 \n    0.198 \n    0.00 \n    0.118 \n    338.54 \n    0.210 \n    21.70 \n    968 \n    0.00 \n    390.6 \n    99.83 \n  \n  \n    j43 \n    0.329 \n    5.22 \n    0.808 \n    0.184 \n    8.89 \n    0.113 \n    453.33 \n    0.243 \n    26.67 \n    893 \n    8.89 \n    284.4 \n    44.44 \n  \n  \n    j44 \n    0.262 \n    5.18 \n    0.703 \n    0.176 \n    4.47 \n    0.095 \n    665.77 \n    0.261 \n    0.00 \n    518 \n    4.47 \n    245.8 \n    31.28 \n  \n  \n    j45 \n    0.265 \n    5.14 \n    0.700 \n    0.159 \n    0.00 \n    0.099 \n    171.63 \n    0.257 \n    27.10 \n    425 \n    0.00 \n    234.9 \n    45.17 \n  \n  \n    j46 \n    0.288 \n    4.96 \n    0.749 \n    0.184 \n    13.24 \n    0.109 \n    295.68 \n    0.229 \n    136.81 \n    534 \n    13.24 \n    348.6 \n    48.54 \n  \n  \n    j47 \n    0.323 \n    5.17 \n    0.796 \n    0.188 \n    13.27 \n    0.110 \n    265.49 \n    0.238 \n    70.80 \n    721 \n    13.27 \n    296.5 \n    84.07 \n  \n  \n    j48 \n    0.270 \n    5.11 \n    0.757 \n    0.189 \n    17.38 \n    0.129 \n    504.13 \n    0.251 \n    169.49 \n    574 \n    13.04 \n    378.1 \n    26.08 \n  \n  \n    j49 \n    0.294 \n    5.31 \n    0.781 \n    0.193 \n    4.57 \n    0.077 \n    351.76 \n    0.250 \n    73.09 \n    1055 \n    4.57 \n    292.4 \n    109.64 \n  \n  \n    j50 \n    0.257 \n    5.08 \n    0.730 \n    0.153 \n    0.00 \n    0.116 \n    357.91 \n    0.255 \n    38.81 \n    759 \n    0.00 \n    418.3 \n    116.43 \n  \n  \n    j51 \n    0.345 \n    5.32 \n    0.819 \n    0.179 \n    4.43 \n    0.110 \n    301.55 \n    0.228 \n    31.04 \n    1078 \n    0.00 \n    252.8 \n    84.26 \n  \n  \n    j52 \n    0.233 \n    4.29 \n    0.723 \n    0.241 \n    4.43 \n    0.107 \n    181.82 \n    0.135 \n    390.24 \n    514 \n    4.43 \n    465.6 \n    119.73 \n  \n  \n    j53 \n    0.289 \n    4.80 \n    0.750 \n    0.200 \n    8.60 \n    0.136 \n    206.27 \n    0.179 \n    64.46 \n    700 \n    8.60 \n    571.6 \n    103.14 \n  \n  \n    j54 \n    0.335 \n    4.97 \n    0.802 \n    0.199 \n    8.87 \n    0.110 \n    168.59 \n    0.191 \n    84.30 \n    1038 \n    8.87 \n    496.9 \n    124.22 \n  \n  \n    j55 \n    0.362 \n    4.61 \n    0.835 \n    0.196 \n    4.50 \n    0.099 \n    63.01 \n    0.181 \n    90.01 \n    324 \n    4.50 \n    441.0 \n    40.50 \n  \n  \n    j56 \n    0.340 \n    4.93 \n    0.805 \n    0.180 \n    0.00 \n    0.108 \n    172.26 \n    0.233 \n    4.42 \n    468 \n    0.00 \n    207.6 \n    30.92 \n  \n  \n    j57 \n    0.324 \n    4.75 \n    0.792 \n    0.204 \n    4.47 \n    0.100 \n    187.58 \n    0.179 \n    102.72 \n    965 \n    4.47 \n    375.2 \n    129.52 \n  \n  \n    j58 \n    0.320 \n    4.92 \n    0.762 \n    0.193 \n    0.00 \n    0.106 \n    202.91 \n    0.195 \n    52.93 \n    521 \n    0.00 \n    176.4 \n    48.52 \n  \n  \n    j59 \n    0.285 \n    4.85 \n    0.776 \n    0.215 \n    0.00 \n    0.114 \n    220.26 \n    0.178 \n    57.27 \n    604 \n    0.00 \n    519.8 \n    127.75 \n  \n  \n    j60 \n    0.379 \n    5.02 \n    0.852 \n    0.224 \n    17.57 \n    0.109 \n    232.87 \n    0.232 \n    74.69 \n    712 \n    13.18 \n    470.1 \n    123.02 \n  \n  \n    j61 \n    0.354 \n    4.67 \n    0.824 \n    0.208 \n    0.00 \n    0.128 \n    150.15 \n    0.190 \n    90.09 \n    674 \n    0.00 \n    390.4 \n    81.51 \n  \n  \n    j62 \n    0.355 \n    4.73 \n    0.835 \n    0.224 \n    18.14 \n    0.083 \n    195.01 \n    0.180 \n    317.46 \n    712 \n    18.14 \n    417.2 \n    204.08 \n  \n  \n    j63 \n    0.334 \n    4.99 \n    0.835 \n    0.224 \n    4.35 \n    0.118 \n    295.78 \n    0.192 \n    78.30 \n    818 \n    4.35 \n    374.1 \n    117.44 \n  \n  \n    j64 \n    0.366 \n    4.80 \n    0.842 \n    0.180 \n    4.37 \n    0.123 \n    209.61 \n    0.193 \n    78.60 \n    751 \n    4.37 \n    406.1 \n    96.07 \n  \n  \n    j65 \n    0.297 \n    4.49 \n    0.762 \n    0.210 \n    4.19 \n    0.153 \n    167.65 \n    0.176 \n    67.06 \n    666 \n    0.00 \n    331.1 \n    104.78 \n  \n  \n    j66 \n    0.351 \n    4.88 \n    0.821 \n    0.216 \n    8.65 \n    0.134 \n    194.55 \n    0.176 \n    95.11 \n    800 \n    4.32 \n    358.8 \n    159.97 \n  \n  \n    j67 \n    0.339 \n    4.93 \n    0.810 \n    0.210 \n    13.57 \n    0.087 \n    199.09 \n    0.176 \n    76.92 \n    995 \n    13.57 \n    375.6 \n    95.02 \n  \n  \n    j68 \n    0.377 \n    4.90 \n    0.858 \n    0.184 \n    8.69 \n    0.133 \n    225.89 \n    0.205 \n    47.78 \n    573 \n    8.69 \n    382.3 \n    52.13 \n  \n  \n    j69 \n    0.207 \n    4.94 \n    0.612 \n    0.138 \n    4.39 \n    0.111 \n    337.72 \n    0.281 \n    13.16 \n    596 \n    4.39 \n    263.2 \n    13.16 \n  \n  \n    j70 \n    0.287 \n    4.98 \n    0.711 \n    0.136 \n    0.00 \n    0.070 \n    352.24 \n    0.303 \n    9.15 \n    704 \n    0.00 \n    210.4 \n    45.75 \n  \n  \n    j71 \n    0.308 \n    5.49 \n    0.805 \n    0.180 \n    4.36 \n    0.115 \n    287.46 \n    0.247 \n    17.42 \n    775 \n    4.36 \n    365.9 \n    69.69 \n  \n  \n    j72 \n    0.373 \n    5.64 \n    0.839 \n    0.144 \n    0.00 \n    0.124 \n    438.45 \n    0.274 \n    4.22 \n    839 \n    0.00 \n    198.1 \n    80.10 \n  \n  \n    j73 \n    0.295 \n    5.39 \n    0.718 \n    0.136 \n    8.04 \n    0.153 \n    454.55 \n    0.264 \n    4.02 \n    660 \n    4.02 \n    164.9 \n    16.09 \n  \n  \n    j74 \n    0.313 \n    5.57 \n    0.807 \n    0.147 \n    4.34 \n    0.116 \n    637.47 \n    0.298 \n    4.34 \n    746 \n    4.34 \n    303.6 \n    78.06 \n  \n  \n    j75 \n    0.238 \n    4.82 \n    0.648 \n    0.138 \n    0.00 \n    0.093 \n    272.20 \n    0.274 \n    0.00 \n    473 \n    0.00 \n    397.1 \n    89.25 \n  \n  \n    j76 \n    0.354 \n    5.21 \n    0.832 \n    0.171 \n    0.00 \n    0.116 \n    230.67 \n    0.280 \n    17.09 \n    756 \n    0.00 \n    401.5 \n    46.99 \n  \n  \n    j77 \n    0.263 \n    4.82 \n    0.701 \n    0.122 \n    0.00 \n    0.108 \n    118.81 \n    0.267 \n    0.00 \n    523 \n    0.00 \n    225.7 \n    31.68 \n  \n  \n    j78 \n    0.284 \n    5.35 \n    0.739 \n    0.125 \n    4.22 \n    0.135 \n    362.87 \n    0.292 \n    29.54 \n    895 \n    4.22 \n    282.7 \n    71.73 \n  \n  \n    j79 \n    0.205 \n    4.48 \n    0.608 \n    0.155 \n    4.45 \n    0.093 \n    262.69 \n    0.250 \n    102.40 \n    543 \n    4.45 \n    347.3 \n    71.24 \n  \n  \n    j80 \n    0.233 \n    5.09 \n    0.652 \n    0.118 \n    0.00 \n    0.093 \n    257.32 \n    0.279 \n    0.00 \n    697 \n    0.00 \n    310.6 \n    66.55 \n  \n  \n    k01 \n    0.314 \n    4.44 \n    0.768 \n    0.199 \n    102.46 \n    0.177 \n    90.16 \n    0.140 \n    151.64 \n    512 \n    20.49 \n    495.9 \n    135.25 \n  \n  \n    k02 \n    0.351 \n    4.46 \n    0.828 \n    0.210 \n    47.31 \n    0.138 \n    137.63 \n    0.178 \n    163.44 \n    482 \n    25.81 \n    421.5 \n    38.71 \n  \n  \n    k03 \n    0.342 \n    4.21 \n    0.816 \n    0.236 \n    117.60 \n    0.125 \n    47.91 \n    0.161 \n    209.06 \n    549 \n    56.62 \n    322.3 \n    47.91 \n  \n  \n    k04 \n    0.307 \n    4.49 \n    0.796 \n    0.217 \n    47.62 \n    0.128 \n    129.87 \n    0.167 \n    108.22 \n    667 \n    25.97 \n    480.5 \n    73.59 \n  \n  \n    k05 \n    0.297 \n    4.18 \n    0.757 \n    0.202 \n    21.32 \n    0.144 \n    55.44 \n    0.137 \n    72.50 \n    358 \n    12.79 \n    528.8 \n    102.34 \n  \n  \n    k06 \n    0.351 \n    4.51 \n    0.798 \n    0.160 \n    47.49 \n    0.186 \n    43.53 \n    0.165 \n    67.27 \n    542 \n    19.79 \n    407.6 \n    35.62 \n  \n  \n    k07 \n    0.285 \n    4.15 \n    0.763 \n    0.228 \n    217.03 \n    0.204 \n    20.48 \n    0.111 \n    274.37 \n    438 \n    65.52 \n    622.4 \n    77.81 \n  \n  \n    k08 \n    0.296 \n    4.41 \n    0.772 \n    0.232 \n    75.79 \n    0.153 \n    96.84 \n    0.142 \n    160.00 \n    425 \n    54.74 \n    585.3 \n    88.42 \n  \n  \n    k09 \n    0.299 \n    4.11 \n    0.770 \n    0.208 \n    61.38 \n    0.118 \n    92.06 \n    0.159 \n    385.80 \n    311 \n    35.07 \n    403.3 \n    87.68 \n  \n  \n    k10 \n    0.347 \n    4.46 \n    0.812 \n    0.196 \n    4.17 \n    0.160 \n    54.26 \n    0.159 \n    125.21 \n    417 \n    4.17 \n    400.7 \n    37.56 \n  \n  \n    k11 \n    0.289 \n    4.18 \n    0.738 \n    0.167 \n    16.68 \n    0.166 \n    54.21 \n    0.176 \n    70.89 \n    384 \n    0.00 \n    379.5 \n    29.19 \n  \n  \n    k12 \n    0.375 \n    4.68 \n    0.836 \n    0.204 \n    65.98 \n    0.167 \n    57.73 \n    0.172 \n    181.44 \n    532 \n    20.62 \n    367.0 \n    70.10 \n  \n  \n    k13 \n    0.313 \n    4.36 \n    0.775 \n    0.211 \n    50.76 \n    0.154 \n    93.06 \n    0.165 \n    219.97 \n    427 \n    25.38 \n    355.3 \n    63.45 \n  \n  \n    k14 \n    0.347 \n    4.54 \n    0.842 \n    0.207 \n    25.64 \n    0.137 \n    89.74 \n    0.173 \n    106.84 \n    462 \n    8.55 \n    367.5 \n    42.73 \n  \n  \n    k15 \n    0.337 \n    4.46 \n    0.809 \n    0.208 \n    47.23 \n    0.138 \n    98.75 \n    0.165 \n    176.04 \n    404 \n    4.29 \n    386.4 \n    25.76 \n  \n  \n    k16 \n    0.390 \n    4.46 \n    0.850 \n    0.191 \n    12.94 \n    0.126 \n    47.43 \n    0.201 \n    60.37 \n    712 \n    4.31 \n    336.4 \n    34.50 \n  \n  \n    k17 \n    0.333 \n    4.41 \n    0.811 \n    0.215 \n    58.82 \n    0.160 \n    92.44 \n    0.161 \n    331.93 \n    441 \n    33.61 \n    357.1 \n    33.61 \n  \n  \n    k18 \n    0.322 \n    4.16 \n    0.813 \n    0.259 \n    234.44 \n    0.100 \n    72.14 \n    0.140 \n    455.37 \n    568 \n    112.71 \n    491.4 \n    112.71 \n  \n  \n    k19 \n    0.306 \n    4.33 \n    0.755 \n    0.204 \n    29.85 \n    0.121 \n    119.40 \n    0.148 \n    106.61 \n    452 \n    8.53 \n    477.6 \n    115.14 \n  \n  \n    k20 \n    0.359 \n    4.46 \n    0.839 \n    0.228 \n    50.46 \n    0.156 \n    130.36 \n    0.156 \n    155.59 \n    500 \n    29.44 \n    416.3 \n    67.28 \n  \n  \n    k21 \n    0.337 \n    4.39 \n    0.793 \n    0.191 \n    46.30 \n    0.155 \n    63.13 \n    0.164 \n    75.76 \n    522 \n    25.25 \n    446.1 \n    42.09 \n  \n  \n    k22 \n    0.320 \n    4.32 \n    0.780 \n    0.215 \n    99.92 \n    0.161 \n    83.26 \n    0.157 \n    212.32 \n    329 \n    29.14 \n    395.5 \n    41.63 \n  \n  \n    k23 \n    0.354 \n    4.50 \n    0.820 \n    0.204 \n    38.81 \n    0.128 \n    120.74 \n    0.173 \n    146.62 \n    634 \n    12.94 \n    452.8 \n    25.87 \n  \n  \n    k24 \n    0.229 \n    3.76 \n    0.699 \n    0.229 \n    392.72 \n    0.178 \n    0.00 \n    0.112 \n    376.19 \n    339 \n    186.03 \n    483.7 \n    74.41 \n  \n  \n    k25 \n    0.353 \n    4.39 \n    0.831 \n    0.210 \n    4.48 \n    0.100 \n    103.14 \n    0.170 \n    17.94 \n    520 \n    4.48 \n    583.0 \n    76.23 \n  \n  \n    k26 \n    0.277 \n    4.11 \n    0.752 \n    0.228 \n    108.15 \n    0.165 \n    20.80 \n    0.124 \n    232.94 \n    391 \n    49.92 \n    495.0 \n    74.88 \n  \n  \n    k27 \n    0.348 \n    4.55 \n    0.806 \n    0.205 \n    47.43 \n    0.122 \n    103.49 \n    0.201 \n    129.37 \n    410 \n    17.25 \n    396.7 \n    90.56 \n  \n  \n    k28 \n    0.276 \n    4.08 \n    0.742 \n    0.195 \n    119.19 \n    0.201 \n    19.86 \n    0.137 \n    214.54 \n    381 \n    47.68 \n    349.6 \n    47.68 \n  \n  \n    k29 \n    0.375 \n    4.46 \n    0.845 \n    0.220 \n    13.25 \n    0.115 \n    66.25 \n    0.177 \n    384.28 \n    610 \n    8.83 \n    379.9 \n    70.67 \n  \n  \n    l01 \n    0.294 \n    4.12 \n    0.763 \n    0.232 \n    244.31 \n    0.158 \n    80.03 \n    0.130 \n    623.42 \n    451 \n    80.03 \n    459.1 \n    84.25 \n  \n  \n    l02 \n    0.270 \n    4.04 \n    0.726 \n    0.218 \n    278.48 \n    0.147 \n    71.73 \n    0.152 \n    514.77 \n    392 \n    84.39 \n    413.5 \n    80.17 \n  \n  \n    l03 \n    0.286 \n    4.09 \n    0.765 \n    0.213 \n    181.22 \n    0.172 \n    32.95 \n    0.138 \n    234.76 \n    367 \n    111.20 \n    510.7 \n    57.66 \n  \n  \n    l04 \n    0.302 \n    4.25 \n    0.745 \n    0.193 \n    43.76 \n    0.123 \n    74.40 \n    0.164 \n    148.80 \n    350 \n    17.50 \n    551.4 \n    61.27 \n  \n  \n    l05 \n    0.295 \n    4.28 \n    0.755 \n    0.216 \n    181.74 \n    0.151 \n    54.95 \n    0.145 \n    283.18 \n    334 \n    59.17 \n    541.0 \n    42.27 \n  \n  \n    l06 \n    0.285 \n    4.12 \n    0.747 \n    0.181 \n    39.34 \n    0.120 \n    39.34 \n    0.152 \n    74.30 \n    411 \n    26.22 \n    550.7 \n    34.97 \n  \n  \n    l07 \n    0.328 \n    4.36 \n    0.809 \n    0.225 \n    158.73 \n    0.134 \n    85.80 \n    0.162 \n    175.89 \n    450 \n    51.48 \n    523.4 \n    51.48 \n  \n  \n    l08 \n    0.267 \n    4.12 \n    0.784 \n    0.246 \n    309.36 \n    0.186 \n    36.16 \n    0.106 \n    353.56 \n    438 \n    140.62 \n    707.1 \n    140.62 \n  \n  \n    l09 \n    0.282 \n    4.30 \n    0.774 \n    0.206 \n    167.15 \n    0.153 \n    50.15 \n    0.131 \n    112.83 \n    393 \n    108.65 \n    501.5 \n    62.68 \n  \n  \n    l10 \n    0.272 \n    4.40 \n    0.758 \n    0.228 \n    150.56 \n    0.162 \n    50.19 \n    0.113 \n    108.74 \n    318 \n    108.74 \n    598.1 \n    62.73 \n  \n  \n    l11 \n    0.324 \n    4.38 \n    0.807 \n    0.247 \n    123.88 \n    0.146 \n    68.35 \n    0.152 \n    290.47 \n    517 \n    34.17 \n    482.7 \n    119.61 \n  \n  \n    l12 \n    0.346 \n    4.49 \n    0.818 \n    0.225 \n    197.04 \n    0.180 \n    73.89 \n    0.132 \n    258.62 \n    406 \n    61.58 \n    484.4 \n    41.05 \n  \n  \n    l13 \n    0.292 \n    4.14 \n    0.784 \n    0.227 \n    245.32 \n    0.165 \n    66.53 \n    0.136 \n    274.43 \n    387 \n    108.11 \n    461.5 \n    58.21 \n  \n  \n    l14 \n    0.293 \n    4.43 \n    0.787 \n    0.220 \n    44.28 \n    0.194 \n    92.59 \n    0.144 \n    80.52 \n    459 \n    32.21 \n    467.0 \n    112.72 \n  \n  \n    l15 \n    0.337 \n    4.60 \n    0.811 \n    0.219 \n    110.31 \n    0.150 \n    97.58 \n    0.157 \n    67.88 \n    475 \n    46.67 \n    352.1 \n    118.80 \n  \n  \n    l16 \n    0.291 \n    4.32 \n    0.763 \n    0.215 \n    140.55 \n    0.168 \n    78.55 \n    0.146 \n    152.96 \n    318 \n    49.61 \n    413.4 \n    16.54 \n  \n  \n    l17 \n    0.346 \n    4.54 \n    0.813 \n    0.215 \n    166.88 \n    0.145 \n    145.49 \n    0.184 \n    158.32 \n    377 \n    77.02 \n    397.9 \n    51.35 \n  \n  \n    l18 \n    0.287 \n    4.14 \n    0.765 \n    0.230 \n    98.97 \n    0.139 \n    55.94 \n    0.123 \n    185.03 \n    288 \n    38.73 \n    537.9 \n    77.45 \n  \n  \n    l19 \n    0.327 \n    4.43 \n    0.791 \n    0.182 \n    47.21 \n    0.142 \n    90.13 \n    0.182 \n    68.67 \n    322 \n    17.17 \n    523.6 \n    72.96 \n  \n  \n    l20 \n    0.297 \n    4.00 \n    0.791 \n    0.224 \n    220.05 \n    0.187 \n    52.98 \n    0.153 \n    387.12 \n    444 \n    105.95 \n    444.2 \n    36.67 \n  \n  \n    l21 \n    0.329 \n    4.38 \n    0.772 \n    0.187 \n    78.74 \n    0.169 \n    58.02 \n    0.182 \n    107.75 \n    518 \n    24.86 \n    364.7 \n    58.02 \n  \n  \n    l22 \n    0.315 \n    4.21 \n    0.820 \n    0.252 \n    152.67 \n    0.152 \n    25.45 \n    0.145 \n    402.88 \n    509 \n    29.69 \n    581.0 \n    55.13 \n  \n  \n    l23 \n    0.303 \n    4.23 \n    0.776 \n    0.207 \n    141.03 \n    0.143 \n    68.38 \n    0.145 \n    188.03 \n    376 \n    98.29 \n    645.3 \n    59.83 \n  \n  \n    l24 \n    0.275 \n    4.01 \n    0.782 \n    0.240 \n    314.80 \n    0.162 \n    36.80 \n    0.117 \n    621.42 \n    270 \n    53.15 \n    613.2 \n    89.94 \n  \n  \n    m01 \n    0.332 \n    4.47 \n    0.813 \n    0.233 \n    140.79 \n    0.192 \n    96.54 \n    0.130 \n    277.55 \n    547 \n    44.25 \n    426.4 \n    96.54 \n  \n  \n    m02 \n    0.313 \n    4.59 \n    0.787 \n    0.216 \n    12.44 \n    0.168 \n    111.94 \n    0.141 \n    194.86 \n    531 \n    4.15 \n    427.0 \n    49.75 \n  \n  \n    m03 \n    0.298 \n    4.41 \n    0.797 \n    0.231 \n    63.48 \n    0.152 \n    93.10 \n    0.153 \n    207.36 \n    461 \n    33.85 \n    516.3 \n    105.80 \n  \n  \n    m04 \n    0.310 \n    4.39 \n    0.783 \n    0.228 \n    186.92 \n    0.187 \n    69.08 \n    0.146 \n    304.75 \n    508 \n    105.65 \n    418.5 \n    73.14 \n  \n  \n    m05 \n    0.381 \n    5.01 \n    0.835 \n    0.198 \n    12.66 \n    0.154 \n    194.18 \n    0.202 \n    118.19 \n    772 \n    8.44 \n    325.0 \n    101.31 \n  \n  \n    m06 \n    0.293 \n    4.33 \n    0.772 \n    0.212 \n    54.67 \n    0.152 \n    71.49 \n    0.156 \n    214.47 \n    475 \n    16.82 \n    454.2 \n    46.26 \n  \n  \n    n01 \n    0.258 \n    3.98 \n    0.717 \n    0.208 \n    285.95 \n    0.182 \n    40.85 \n    0.125 \n    326.80 \n    294 \n    93.95 \n    416.7 \n    93.95 \n  \n  \n    n02 \n    0.284 \n    4.06 \n    0.774 \n    0.184 \n    146.87 \n    0.155 \n    29.38 \n    0.144 \n    310.53 \n    298 \n    58.75 \n    524.5 \n    41.96 \n  \n  \n    n03 \n    0.296 \n    4.34 \n    0.751 \n    0.187 \n    92.44 \n    0.163 \n    46.22 \n    0.168 \n    159.66 \n    244 \n    12.61 \n    315.1 \n    37.81 \n  \n  \n    n04 \n    0.321 \n    4.26 \n    0.803 \n    0.209 \n    132.03 \n    0.149 \n    46.85 \n    0.169 \n    289.61 \n    515 \n    42.59 \n    370.5 \n    21.30 \n  \n  \n    n05 \n    0.270 \n    4.06 \n    0.733 \n    0.203 \n    238.29 \n    0.172 \n    41.09 \n    0.130 \n    312.24 \n    304 \n    102.71 \n    439.6 \n    98.60 \n  \n  \n    n06 \n    0.319 \n    4.31 \n    0.777 \n    0.191 \n    13.16 \n    0.115 \n    114.08 \n    0.170 \n    421.24 \n    448 \n    4.39 \n    552.9 \n    48.27 \n  \n  \n    n07 \n    0.334 \n    4.38 \n    0.799 \n    0.203 \n    174.85 \n    0.162 \n    54.12 \n    0.167 \n    291.42 \n    375 \n    58.28 \n    349.7 \n    16.65 \n  \n  \n    n08 \n    0.360 \n    4.58 \n    0.816 \n    0.180 \n    0.00 \n    0.119 \n    144.17 \n    0.171 \n    21.84 \n    572 \n    0.00 \n    480.6 \n    74.27 \n  \n  \n    n09 \n    0.310 \n    4.29 \n    0.795 \n    0.214 \n    185.26 \n    0.191 \n    56.38 \n    0.123 \n    257.75 \n    366 \n    60.41 \n    656.5 \n    92.63 \n  \n  \n    n10 \n    0.289 \n    4.28 \n    0.764 \n    0.188 \n    186.31 \n    0.188 \n    20.25 \n    0.140 \n    283.52 \n    288 \n    76.95 \n    449.6 \n    28.35 \n  \n  \n    n11 \n    0.344 \n    4.49 \n    0.830 \n    0.217 \n    117.03 \n    0.127 \n    65.02 \n    0.180 \n    86.69 \n    464 \n    39.01 \n    468.1 \n    56.35 \n  \n  \n    n12 \n    0.291 \n    4.24 \n    0.751 \n    0.200 \n    214.61 \n    0.173 \n    53.65 \n    0.136 \n    202.23 \n    256 \n    103.18 \n    478.7 \n    86.67 \n  \n  \n    n13 \n    0.343 \n    4.37 \n    0.816 \n    0.220 \n    179.54 \n    0.160 \n    33.40 \n    0.140 \n    238.00 \n    438 \n    70.98 \n    538.6 \n    54.28 \n  \n  \n    n14 \n    0.353 \n    4.44 \n    0.827 \n    0.211 \n    100.80 \n    0.155 \n    58.80 \n    0.143 \n    138.60 \n    391 \n    21.00 \n    604.8 \n    71.40 \n  \n  \n    n15 \n    0.312 \n    4.35 \n    0.787 \n    0.187 \n    90.59 \n    0.131 \n    77.65 \n    0.176 \n    120.79 \n    410 \n    25.88 \n    366.7 \n    30.20 \n  \n  \n    n16 \n    0.351 \n    4.53 \n    0.819 \n    0.194 \n    87.32 \n    0.156 \n    49.90 \n    0.181 \n    345.11 \n    565 \n    45.74 \n    573.8 \n    87.32 \n  \n  \n    n17 \n    0.280 \n    4.12 \n    0.767 \n    0.223 \n    268.88 \n    0.208 \n    59.31 \n    0.124 \n    498.22 \n    340 \n    102.81 \n    510.1 \n    67.22 \n  \n  \n    n18 \n    0.300 \n    4.03 \n    0.801 \n    0.236 \n    48.35 \n    0.122 \n    79.12 \n    0.142 \n    593.41 \n    396 \n    26.37 \n    501.1 \n    57.14 \n  \n  \n    n19 \n    0.321 \n    4.30 \n    0.823 \n    0.209 \n    40.78 \n    0.166 \n    28.55 \n    0.159 \n    118.27 \n    445 \n    20.39 \n    444.5 \n    69.33 \n  \n  \n    n20 \n    0.342 \n    4.29 \n    0.814 \n    0.177 \n    101.87 \n    0.145 \n    59.42 \n    0.171 \n    191.00 \n    509 \n    46.69 \n    445.7 \n    50.93 \n  \n  \n    n21 \n    0.363 \n    4.37 \n    0.840 \n    0.213 \n    68.41 \n    0.138 \n    59.85 \n    0.177 \n    359.13 \n    513 \n    25.65 \n    410.4 \n    21.38 \n  \n  \n    n22 \n    0.302 \n    4.25 \n    0.781 \n    0.195 \n    99.44 \n    0.133 \n    90.79 \n    0.171 \n    121.06 \n    432 \n    47.56 \n    575.0 \n    60.53 \n  \n  \n    n23 \n    0.330 \n    4.29 \n    0.804 \n    0.219 \n    110.73 \n    0.142 \n    42.59 \n    0.161 \n    166.10 \n    439 \n    72.40 \n    438.7 \n    76.66 \n  \n  \n    n24 \n    0.278 \n    4.17 \n    0.733 \n    0.199 \n    126.83 \n    0.205 \n    63.42 \n    0.115 \n    332.94 \n    392 \n    43.60 \n    535.1 \n    67.38 \n  \n  \n    n25 \n    0.328 \n    4.43 \n    0.779 \n    0.168 \n    20.68 \n    0.146 \n    74.44 \n    0.198 \n    45.49 \n    430 \n    12.41 \n    351.5 \n    37.22 \n  \n  \n    n26 \n    0.342 \n    4.47 \n    0.801 \n    0.205 \n    128.15 \n    0.132 \n    64.08 \n    0.180 \n    149.51 \n    449 \n    46.99 \n    422.9 \n    51.26 \n  \n  \n    n27 \n    0.272 \n    4.23 \n    0.748 \n    0.212 \n    138.83 \n    0.173 \n    20.42 \n    0.132 \n    212.33 \n    372 \n    40.83 \n    641.1 \n    53.08 \n  \n  \n    n28 \n    0.394 \n    4.65 \n    0.852 \n    0.192 \n    26.30 \n    0.117 \n    83.30 \n    0.195 \n    87.68 \n    741 \n    21.92 \n    377.0 \n    74.53 \n  \n  \n    n29 \n    0.296 \n    4.26 \n    0.756 \n    0.188 \n    214.84 \n    0.216 \n    27.34 \n    0.145 \n    214.84 \n    441 \n    74.22 \n    488.3 \n    39.06 \n  \n  \n    p01 \n    0.333 \n    4.35 \n    0.804 \n    0.208 \n    60.03 \n    0.128 \n    77.19 \n    0.177 \n    64.32 \n    506 \n    38.59 \n    398.8 \n    17.15 \n  \n  \n    p02 \n    0.315 \n    4.31 \n    0.778 \n    0.236 \n    159.48 \n    0.126 \n    51.72 \n    0.159 \n    237.07 \n    474 \n    99.14 \n    344.8 \n    94.83 \n  \n  \n    p03 \n    0.324 \n    4.24 \n    0.826 \n    0.242 \n    214.88 \n    0.174 \n    53.72 \n    0.130 \n    173.55 \n    521 \n    90.91 \n    508.3 \n    111.57 \n  \n  \n    p04 \n    0.333 \n    4.44 \n    0.816 \n    0.203 \n    106.05 \n    0.159 \n    62.84 \n    0.149 \n    98.19 \n    625 \n    23.57 \n    518.5 \n    70.70 \n  \n  \n    p05 \n    0.337 \n    4.48 \n    0.809 \n    0.223 \n    55.41 \n    0.145 \n    34.10 \n    0.154 \n    102.30 \n    627 \n    38.36 \n    481.7 \n    123.61 \n  \n  \n    p06 \n    0.326 \n    4.40 \n    0.773 \n    0.214 \n    232.47 \n    0.191 \n    55.16 \n    0.132 \n    370.37 \n    469 \n    90.62 \n    358.6 \n    59.10 \n  \n  \n    p07 \n    0.308 \n    4.53 \n    0.793 \n    0.231 \n    12.58 \n    0.158 \n    113.21 \n    0.143 \n    205.45 \n    499 \n    8.39 \n    389.9 \n    75.47 \n  \n  \n    p08 \n    0.287 \n    4.14 \n    0.756 \n    0.169 \n    25.90 \n    0.136 \n    43.16 \n    0.133 \n    293.48 \n    557 \n    17.26 \n    517.9 \n    34.53 \n  \n  \n    p09 \n    0.306 \n    4.03 \n    0.778 \n    0.238 \n    109.74 \n    0.090 \n    9.14 \n    0.136 \n    393.23 \n    370 \n    86.88 \n    516.7 \n    45.73 \n  \n  \n    p10 \n    0.328 \n    4.31 \n    0.806 \n    0.225 \n    163.45 \n    0.153 \n    75.44 \n    0.162 \n    364.63 \n    503 \n    92.20 \n    331.1 \n    62.87 \n  \n  \n    p11 \n    0.309 \n    4.14 \n    0.793 \n    0.237 \n    98.04 \n    0.141 \n    63.94 \n    0.125 \n    502.98 \n    426 \n    42.63 \n    477.4 \n    72.46 \n  \n  \n    p12 \n    0.269 \n    4.10 \n    0.731 \n    0.215 \n    133.98 \n    0.184 \n    73.08 \n    0.114 \n    272.03 \n    447 \n    48.72 \n    471.0 \n    64.96 \n  \n  \n    p13 \n    0.322 \n    4.33 \n    0.816 \n    0.178 \n    0.00 \n    0.135 \n    76.99 \n    0.166 \n    192.47 \n    535 \n    0.00 \n    410.6 \n    34.22 \n  \n  \n    p14 \n    0.314 \n    4.22 \n    0.777 \n    0.216 \n    248.10 \n    0.203 \n    52.02 \n    0.116 \n    336.13 \n    372 \n    92.04 \n    572.2 \n    104.04 \n  \n  \n    p15 \n    0.290 \n    4.34 \n    0.758 \n    0.208 \n    146.59 \n    0.206 \n    55.47 \n    0.125 \n    150.56 \n    527 \n    43.58 \n    427.9 \n    47.54 \n  \n  \n    p16 \n    0.320 \n    4.24 \n    0.805 \n    0.218 \n    237.72 \n    0.208 \n    7.92 \n    0.154 \n    245.64 \n    400 \n    95.09 \n    423.9 \n    39.62 \n  \n  \n    p17 \n    0.301 \n    4.41 \n    0.804 \n    0.255 \n    135.49 \n    0.122 \n    96.15 \n    0.130 \n    104.89 \n    625 \n    109.27 \n    472.0 \n    109.27 \n  \n  \n    p18 \n    0.263 \n    4.01 \n    0.762 \n    0.228 \n    307.82 \n    0.186 \n    60.75 \n    0.098 \n    607.53 \n    360 \n    125.56 \n    441.5 \n    89.11 \n  \n  \n    p19 \n    0.263 \n    4.00 \n    0.764 \n    0.241 \n    379.75 \n    0.185 \n    53.08 \n    0.109 \n    567.58 \n    392 \n    151.08 \n    551.2 \n    114.33 \n  \n  \n    p20 \n    0.293 \n    3.96 \n    0.754 \n    0.207 \n    136.03 \n    0.175 \n    28.85 \n    0.126 \n    618.30 \n    602 \n    103.05 \n    486.4 \n    94.81 \n  \n  \n    p21 \n    0.325 \n    4.48 \n    0.828 \n    0.226 \n    51.63 \n    0.145 \n    55.60 \n    0.145 \n    146.94 \n    524 \n    31.77 \n    595.7 \n    87.37 \n  \n  \n    p22 \n    0.274 \n    4.07 \n    0.765 \n    0.235 \n    270.38 \n    0.169 \n    69.64 \n    0.116 \n    553.05 \n    389 \n    147.48 \n    585.8 \n    102.42 \n  \n  \n    p23 \n    0.297 \n    4.40 \n    0.769 \n    0.201 \n    168.89 \n    0.198 \n    113.90 \n    0.163 \n    200.31 \n    683 \n    62.84 \n    361.4 \n    74.63 \n  \n  \n    p24 \n    0.224 \n    4.20 \n    0.671 \n    0.199 \n    261.59 \n    0.207 \n    11.89 \n    0.123 \n    265.56 \n    218 \n    83.23 \n    443.9 \n    35.67 \n  \n  \n    p25 \n    0.305 \n    4.33 \n    0.770 \n    0.208 \n    82.25 \n    0.125 \n    121.21 \n    0.135 \n    90.91 \n    537 \n    43.29 \n    376.6 \n    73.59 \n  \n  \n    p26 \n    0.245 \n    4.06 \n    0.679 \n    0.171 \n    59.42 \n    0.144 \n    67.91 \n    0.140 \n    496.60 \n    611 \n    33.96 \n    534.8 \n    67.91 \n  \n  \n    p27 \n    0.297 \n    4.28 \n    0.771 \n    0.219 \n    191.13 \n    0.183 \n    85.40 \n    0.166 \n    187.07 \n    350 \n    113.87 \n    422.9 \n    69.13 \n  \n  \n    p28 \n    0.332 \n    4.38 \n    0.814 \n    0.211 \n    59.85 \n    0.134 \n    123.98 \n    0.144 \n    85.51 \n    462 \n    21.38 \n    495.9 \n    128.26 \n  \n  \n    p29 \n    0.378 \n    4.85 \n    0.844 \n    0.227 \n    235.83 \n    0.175 \n    161.36 \n    0.163 \n    343.40 \n    554 \n    82.75 \n    306.2 \n    66.20 \n  \n  \n    r01 \n    0.364 \n    4.52 \n    0.845 \n    0.211 \n    64.38 \n    0.138 \n    158.80 \n    0.185 \n    115.88 \n    369 \n    30.04 \n    394.9 \n    60.09 \n  \n  \n    r02 \n    0.343 \n    4.61 \n    0.831 \n    0.217 \n    75.28 \n    0.155 \n    62.73 \n    0.162 \n    409.87 \n    506 \n    41.82 \n    368.0 \n    71.10 \n  \n  \n    r03 \n    0.381 \n    4.64 \n    0.843 \n    0.200 \n    19.34 \n    0.168 \n    81.24 \n    0.161 \n    243.71 \n    727 \n    11.61 \n    421.7 \n    96.71 \n  \n  \n    r04 \n    0.322 \n    4.23 \n    0.810 \n    0.241 \n    168.44 \n    0.112 \n    75.36 \n    0.153 \n    208.33 \n    403 \n    115.25 \n    478.7 \n    88.65 \n  \n  \n    r05 \n    0.336 \n    4.60 \n    0.804 \n    0.221 \n    121.75 \n    0.179 \n    121.75 \n    0.183 \n    271.92 \n    503 \n    56.82 \n    357.1 \n    101.46 \n  \n  \n    r06 \n    0.315 \n    4.36 \n    0.754 \n    0.209 \n    47.88 \n    0.196 \n    75.82 \n    0.154 \n    307.26 \n    499 \n    31.92 \n    403.0 \n    55.87 \n  \n  \n    r07 \n    0.277 \n    4.34 \n    0.743 \n    0.224 \n    97.72 \n    0.174 \n    97.72 \n    0.131 \n    276.87 \n    505 \n    40.72 \n    557.8 \n    134.37 \n  \n  \n    r08 \n    0.437 \n    5.03 \n    0.866 \n    0.191 \n    8.44 \n    0.147 \n    164.49 \n    0.178 \n    63.26 \n    848 \n    0.00 \n    362.7 \n    75.92 \n  \n  \n    r09 \n    0.443 \n    4.63 \n    0.877 \n    0.217 \n    120.07 \n    0.141 \n    98.63 \n    0.184 \n    325.90 \n    502 \n    42.88 \n    394.5 \n    107.20 \n  \n\n\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/factor-analysis.html#output-of-factanal",
    "href": "slides/factor-analysis.html#output-of-factanal",
    "title": "Factor analysis",
    "section": "Output of factanal()",
    "text": "Output of factanal()\n\n\nCode\nfa\n\n\n\nCall:\nfactanal(x = ., factors = 4, scores = \"regression\")\n\nUniquenesses:\n     ttr word_len   p_bigr   p_pobi     p_mw      p_c  p_nomin   p_noun \n   0.014    0.057    0.096    0.241    0.005    0.544    0.245    0.250 \n  p_ppss    p_adj    p_neg    p_adv   p_qual \n   0.422    0.383    0.132    0.414    0.510 \n\nLoadings:\n         Factor1 Factor2 Factor3 Factor4\nttr              -0.132   0.983         \nword_len  0.879  -0.354   0.159  -0.144 \np_bigr                    0.920   0.237 \np_pobi   -0.347   0.281   0.424   0.616 \np_mw     -0.289   0.949                 \np_c      -0.400   0.543                 \np_nomin   0.792  -0.281  -0.198         \np_noun    0.609  -0.387  -0.107  -0.466 \np_ppss   -0.440   0.528           0.312 \np_adj     0.684  -0.303   0.148   0.190 \np_neg    -0.296   0.869           0.143 \np_adv    -0.376   0.327           0.577 \np_qual    0.165           0.138   0.665 \n\n               Factor1 Factor2 Factor3 Factor4\nSS loadings      3.056   2.880   2.132   1.620\nProportion Var   0.235   0.222   0.164   0.125\nCumulative Var   0.235   0.457   0.621   0.745\n\nTest of the hypothesis that 4 factors are sufficient.\nThe chi square statistic is 371 on 32 degrees of freedom.\nThe p-value is 2.33e-59 \n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/factor-analysis.html#correlations-between-variables",
    "href": "slides/factor-analysis.html#correlations-between-variables",
    "title": "Factor analysis",
    "section": "Correlations between variables",
    "text": "Correlations between variables\n\n\nCode\ndataset %>%\n  select(word_len, p_nomin, p_ppss, p_pobi, ttr) %>% ggpairs()\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/factor-analysis.html#factors-1-and-2",
    "href": "slides/factor-analysis.html#factors-1-and-2",
    "title": "Factor analysis",
    "section": "Factors 1 and 2",
    "text": "Factors 1 and 2\n\n\nCode\nfa_loads <- unclass(loadings(fa)) %>% \n  as_tibble(rownames = \"Variable\") %>% \n  mutate(across(where(is.numeric), ~ if_else(abs(.x) < 0.3, NA_real_, .x))) %>% \n  filter(!is.na(Factor1) | !is.na(Factor2))\nkbl(fa_loads[,1:3]) %>% kable_paper(font_size = 22)\nscores <- as_tibble(fa$scores, rownames = \"File\")\nbrown_mapping <- read_tsv(here::here(\"resources\", \"brown_files.tsv\"), show_col_types = FALSE) %>%\n  select(Component, Register) %>% deframe()\nregister_centroids <- scores %>%\n  mutate(Component = str_extract(File, \"[a-z]\"), Register = brown_mapping[Component]) %>%\n  group_by(Register) %>%\n  summarize(Factor1 = mean(Factor1), Factor2 = mean(Factor2))\ntop_1 <- register_centroids %>% arrange(Factor1) %>%\n  tail(2) %>% summarize(across(where(is.numeric), mean))\ntop_2 <- register_centroids %>% arrange(Factor2) %>%\n  tail(2) %>% summarize(across(where(is.numeric), mean))\n\nggplot(scores, aes(x = Factor1, y = Factor2)) +\n  geom_text(aes(label = File), color = \"gray\", size = 6, alpha = 0.8) +\n  geom_point(data = register_centroids, size = 6) +\n  annotate(\"segment\", x = 1.5, y = 1, xend = top_1$Factor1, yend = top_1$Factor2) +\n  annotate(\"label\", x = 1.5, y = 1, label = \"Miscellaneous & Learned\", size = 8) +\n  annotate(\"segment\", x = -0.8, y = 3, xend = top_2$Factor1, yend = top_2$Factor2) +\n  annotate(\"label\", x = -0.8, y = 3, label = \"Romance and Mystery\", size = 8) +\n  theme_minimal(base_size = 23) +\n  geom_hline(yintercept = 0, color = \"gray\", linetype = 3) +\n  geom_vline(xintercept = 0, color = \"gray\", linetype = 3)\n\n\n\n\n\n\n \n  \n    Variable \n    Factor1 \n    Factor2 \n  \n \n\n  \n    word_len \n    0.879 \n    -0.354 \n  \n  \n    p_pobi \n    -0.347 \n     \n  \n  \n    p_mw \n     \n    0.949 \n  \n  \n    p_c \n    -0.400 \n    0.543 \n  \n  \n    p_nomin \n    0.792 \n     \n  \n  \n    p_noun \n    0.609 \n    -0.387 \n  \n  \n    p_ppss \n    -0.440 \n    0.528 \n  \n  \n    p_adj \n    0.684 \n    -0.303 \n  \n  \n    p_neg \n     \n    0.869 \n  \n  \n    p_adv \n    -0.376 \n    0.327 \n  \n\n\n\n\n\n\n\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/factor-analysis.html#representative-files-for-factor-1",
    "href": "slides/factor-analysis.html#representative-files-for-factor-1",
    "title": "Factor analysis",
    "section": "Representative files for Factor 1",
    "text": "Representative files for Factor 1\n\n\nCode\nget_random_s <- function(filepath) {\n  read_xml(filepath) %>% \n    xml_find_all(\"//d1:s\") %>% \n    sample(1) %>% \n    xml_children() %>% \n    xml_text() %>% \n    paste(collapse = \" \")\n}\nset.seed(5)\nfilepath <- function(file) here::here(\"studies\", \"_corpora\", \"brown_tei\", paste0(file, \".xml\"))\nright_1 <- filter(scores, Factor1 == max(Factor1))$File\nleft_1 <- filter(scores, Factor1 == min(Factor1))$File\n\n\n\n\n\n\n\n\n\n\n\nPole\nPreferred\nDispreferred\nRandom example\n\n\n\n\nRight\nlong words, nominalizations, nouns adjectives\npunctuation, POS bigrams, pronouns, adverbs\nHe showed convincingly that anxiety is a learned ( conditioned ) reaction and is the basis of experimental and clinical neuroses and assumed , therefore , that the neuronal changes which underlie the neuroses are functional and reversible .\n\n\nLeft\nshort words, punctuation POS bigrams, pronouns adverbs\nnominalizations, nouns, adjectives\nThrough which he has granted us the very great and precious promises , so that through them you may become partaker of the divine nature .\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/factor-analysis.html#representative-files-for-factor-2",
    "href": "slides/factor-analysis.html#representative-files-for-factor-2",
    "title": "Factor analysis",
    "section": "Representative files for Factor 2",
    "text": "Representative files for Factor 2\n\n\n\n\n\n\n\n\n\nPole\nPreferred\nDispreferred\nRandom example\n\n\n\n\nRight\nshort words, composite expressions, negations, punctuation, personal pronouns, adverbs\nnouns and adjectives\nYou ought to know that ’’ .\n\n\nLeft\nlong words, nouns and adjectives\ncomposite expressions, negations, punctuation, personal pronouns, adverbs\nMost of us would say that in this delight at the killing of others or the causing of suffering there is something very unfitting .\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "slides/factor-analysis.html#loadings",
    "href": "slides/factor-analysis.html#loadings",
    "title": "Factor analysis",
    "section": "Loadings",
    "text": "Loadings\n\n\n\noptions(knitr.kable.NA = \"\")\n\nfa_loads <- loadings(fa) %>% \n  unclass() %>% \n  as_tibble(rownames = \"Variable\") %>% \n  mutate(across(\n    where(is.numeric),\n    ~ if_else(\n      abs(.x) < 0.3,\n      NA_real_,\n      .x))\n    )\n\nkbl(fa_loads) %>%\n  kable_paper(font_size = 22)\n\n\n\n\n\n \n  \n    Variable \n    Factor1 \n    Factor2 \n    Factor3 \n    Factor4 \n  \n \n\n  \n    ttr \n     \n     \n    0.983 \n     \n  \n  \n    word_len \n    0.879 \n    -0.354 \n     \n     \n  \n  \n    p_bigr \n     \n     \n    0.920 \n     \n  \n  \n    p_pobi \n    -0.347 \n     \n    0.424 \n    0.616 \n  \n  \n    p_mw \n     \n    0.949 \n     \n     \n  \n  \n    p_c \n    -0.400 \n    0.543 \n     \n     \n  \n  \n    p_nomin \n    0.792 \n     \n     \n     \n  \n  \n    p_noun \n    0.609 \n    -0.387 \n     \n    -0.466 \n  \n  \n    p_ppss \n    -0.440 \n    0.528 \n     \n    0.312 \n  \n  \n    p_adj \n    0.684 \n    -0.303 \n     \n     \n  \n  \n    p_neg \n     \n    0.869 \n     \n     \n  \n  \n    p_adv \n    -0.376 \n    0.327 \n     \n    0.577 \n  \n  \n    p_qual \n     \n     \n     \n    0.665 \n  \n\n\n\n\n\n\n\n\nData extraction"
  },
  {
    "objectID": "slides/factor-analysis.html#uniqueness",
    "href": "slides/factor-analysis.html#uniqueness",
    "title": "Factor analysis",
    "section": "Uniqueness",
    "text": "Uniqueness\n\n\n\nuniqueness <- fa$uniquenesses %>% \n  enframe(\n    name = \"Variable\",\n    value = \"Uniqueness\"\n    ) %>% \n  arrange(desc(Uniqueness))\n\nuniqueness %>% kbl() %>% \n  kable_paper(font_size = 22)\n\n\n\n\n\n\n \n  \n    Variable \n    Uniqueness \n  \n \n\n  \n    p_c \n    0.544 \n  \n  \n    p_qual \n    0.510 \n  \n  \n    p_ppss \n    0.422 \n  \n  \n    p_adv \n    0.414 \n  \n  \n    p_adj \n    0.383 \n  \n  \n    p_noun \n    0.250 \n  \n  \n    p_nomin \n    0.245 \n  \n  \n    p_pobi \n    0.241 \n  \n  \n    p_neg \n    0.132 \n  \n  \n    p_bigr \n    0.096 \n  \n  \n    word_len \n    0.057 \n  \n  \n    ttr \n    0.014 \n  \n  \n    p_mw \n    0.005 \n  \n\n\n\n\n\n\n\n\nData extraction"
  },
  {
    "objectID": "slides/factor-analysis.html#variance",
    "href": "slides/factor-analysis.html#variance",
    "title": "Factor analysis",
    "section": "Variance",
    "text": "Variance\n\n\n\nfactanal_variances <- function(loadings) {\n  vars <- colSums(loadings^2)/nrow(loadings)\n  as_tibble_row(vars) %>%\n    pivot_longer(\n      everything(),\n      names_to = \"Factor\",\n      values_to = \"Variance\",\n      names_transform = ~str_remove(.x, \"Factor\"))\n  }\nfa7 <- dataset %>%\n  data.frame(row.names = \"filename\") %>%\n  as.matrix() %>% factanal(7)\nloadings(fa7) %>% factanal_variances() %>% \n  ggplot(aes(x = Factor, y = Variance)) +\n  geom_point(size = 7) +\n  scale_y_continuous(labels = scales::label_percent()) +\n  theme_minimal(base_size = 35)\n\n\n\n\n\n\n\n\n\nData extraction"
  },
  {
    "objectID": "slides/factor-analysis.html#scores",
    "href": "slides/factor-analysis.html#scores",
    "title": "Factor analysis",
    "section": "Scores",
    "text": "Scores\n\nscores <- as_tibble(fa$scores, rownames = \"File\")\nhead(scores, 7) %>% kbl() %>% kable_paper(font_size = 22) %>% column_spec(1, bold = T)\ncentroids <- scores %>% \n  mutate(Register = str_extract(File, \"[a-z]\")) %>% \n  group_by(Register) %>% \n  summarize(across(where(is.numeric), mean))\nhead(centroids, 7) %>% kbl() %>% kable_paper(font_size = 22) %>% column_spec(1, bold = T, border_left = T)\n\n\n\n\n\n \n  \n    File \n    Factor1 \n    Factor2 \n    Factor3 \n    Factor4 \n  \n \n\n  \n    a01 \n    0.540 \n    -0.177 \n    0.583 \n    -0.935 \n  \n  \n    a02 \n    0.122 \n    -0.130 \n    1.116 \n    -1.797 \n  \n  \n    a03 \n    0.209 \n    -0.207 \n    0.250 \n    -1.275 \n  \n  \n    a04 \n    1.217 \n    -0.211 \n    1.067 \n    0.305 \n  \n  \n    a05 \n    -0.148 \n    -0.686 \n    -0.276 \n    -0.716 \n  \n  \n    a06 \n    0.435 \n    0.417 \n    0.521 \n    -1.223 \n  \n  \n    a07 \n    0.825 \n    -0.005 \n    0.877 \n    -1.020 \n  \n\n\n\n\n\n\n\n \n  \n    Register \n    Factor1 \n    Factor2 \n    Factor3 \n    Factor4 \n  \n \n\n  \n    a \n    -0.024 \n    -0.138 \n    0.919 \n    -0.874 \n  \n  \n    b \n    0.111 \n    -0.130 \n    0.954 \n    0.253 \n  \n  \n    c \n    0.079 \n    -0.296 \n    1.341 \n    0.165 \n  \n  \n    d \n    -0.106 \n    -0.641 \n    -0.330 \n    0.683 \n  \n  \n    e \n    0.240 \n    -0.091 \n    -0.024 \n    -0.125 \n  \n  \n    f \n    -0.050 \n    -0.395 \n    0.171 \n    0.158 \n  \n  \n    g \n    -0.025 \n    -0.451 \n    0.248 \n    0.385 \n  \n\n\n\n\n\n\n\n\nData extraction"
  },
  {
    "objectID": "slides/factor-analysis.html#references",
    "href": "slides/factor-analysis.html#references",
    "title": "Factor analysis",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBiber, Douglas. 1988. Variation Across Speech and Writing. First. Cambridge University Press. https://doi.org/10.1017/CBO9780511621024.\n\n\nLevshina, Natalia. 2015. How to Do Linguistics with R: Data Exploration and Statistical Analysis. Amsterdam; Philadelphia: John Benjamins Publishing Company."
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Slides",
    "section": "",
    "text": "These are the slides of the course, Fall of 2022. If you would like to download slides as PDF, please follow the instructions in the Quarto documentation.\n\n1 Introduction\n\n\n\n\n\n2 Read corpora\nDownload the R script.\n\n\n\n\n\n3 Contingency tables\nDownload the R script.\n\n\n\n\n\n\n4 Association measures\nDownload the R script.\n\n\n\n\n\n5 Linear regression\nDownload the R script.\n\n\n\n\n\n6 Logistic regression\nDownload the R script.\n\n\n\n\n\n7 Correspondence Analysis\nDownload the R script.\n\n\n\n\n\n8 Factor Analysis\nDownload the R script."
  },
  {
    "objectID": "slides/init.html#outline",
    "href": "slides/init.html#outline",
    "title": "Reading and exploring a corpus",
    "section": "Outline",
    "text": "Outline\n\nInitialize project\nAdd corpus\nExplore corpus\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#steps",
    "href": "slides/init.html#steps",
    "title": "Reading and exploring a corpus",
    "section": "Steps",
    "text": "Steps\n\nCreate an R project, check on the “git repository” checkbox and commit changes\n\n\nAlternatively, run usethis::use_git()\n\n\nAdd remote on the terminal following the instructions on GitHub (see next slide).\n\n\n\n\n\n\n\nIn the future\n\n\nFor future projects, the workflow may be different; check Happy Git With R for a guide.\n\n\n\n\nInitialize project"
  },
  {
    "objectID": "slides/init.html#connect-repository-to-github",
    "href": "slides/init.html#connect-repository-to-github",
    "title": "Reading and exploring a corpus",
    "section": "Connect repository to GitHub",
    "text": "Connect repository to GitHub\nThis only needs to be done once at the beginning.\ngit remote add origin <url>\ngit branch -M main\ngit push -u origin main\n\n\n\n\n\n\nNote\n\n\n<url> is the url of your repository.\n\n\n\n\nInitialize project"
  },
  {
    "objectID": "slides/init.html#notes",
    "href": "slides/init.html#notes",
    "title": "Reading and exploring a corpus",
    "section": "Notes",
    "text": "Notes\n\nIf you make changes on the remote, use git pull before making changes in the local repo.\nAvoid .RData with Tools > Global Options > General > Workspace/History\nYou can work with Git(Hub) on the Git tab of RStudio or on the Git Bash Terminal\n\n\nInitialize project"
  },
  {
    "objectID": "slides/init.html#git-branch",
    "href": "slides/init.html#git-branch",
    "title": "Reading and exploring a corpus",
    "section": "Git branch",
    "text": "Git branch\nOptionally, you can start “new work” on a new branch and then merge it to main.\ngit branch explore-corpus\ngit checkout explore-corpus\n\n\n\n\n\n\nTip\n\n\nI will only look at what you push to the main branch.\n\n\n\n\nAdd corpus"
  },
  {
    "objectID": "slides/init.html#add-a-folder-with-a-corpus",
    "href": "slides/init.html#add-a-folder-with-a-corpus",
    "title": "Reading and exploring a corpus",
    "section": "Add a folder with a corpus",
    "text": "Add a folder with a corpus\nDownload the corpora from Toledo (mcl.zip file with various corpora) and copy/move the brown folder to your project. There are different options.\n\nTop levelCorpus/corpora folderData folder\n\n\nTo be accessed with here::here(\"brown\").\nproject\n|_brown\n|_project.Rproj\n|_.gitignore\n\n\nTo be accessed with here::here(\"corpus\", \"brown\").\nproject\n|_corpus\n| \\_brown\n|_project.Rproj\n|_.gitignore\n\n\nTo be accessed with here::here(\"data\", \"corpus\", \"brown\").\nproject\n|_data\n| \\_corpus\n|   \\_brown\n|_project.Rproj\n|_.gitignore\n\n\n\n\nAdd corpus"
  },
  {
    "objectID": "slides/init.html#gitignore",
    "href": "slides/init.html#gitignore",
    "title": "Reading and exploring a corpus",
    "section": ".gitignore",
    "text": ".gitignore\nWe don’t want to track the corpus on git (because of size and licenses).\n\nOpen .gitignore.\nAdd a line for the folder to ignore, e.g. /brown/.\n\n(Check git status!)\n\nAdd corpus"
  },
  {
    "objectID": "slides/init.html#create-new-r-script",
    "href": "slides/init.html#create-new-r-script",
    "title": "Reading and exploring a corpus",
    "section": "Create new R script",
    "text": "Create new R script\n\nEasier to run the code again and to share it.\nLoad packages first.\nDo not change your working directory; use here() or reliable relative paths.\n\n\n\n\n\n\n\nTip\n\n\nYou can add comments to explain what you did and even hierarchical sections!\n\n\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#start-of-the-script",
    "href": "slides/init.html#start-of-the-script",
    "title": "Reading and exploring a corpus",
    "section": "Start of the script",
    "text": "Start of the script\n\n# Load packages ----\nlibrary(here)\nlibrary(tidyverse)\nlibrary(mclm)\n\n# Load data ----\npath_to_corpus <- here(\"studies\", \"_corpora\", \"brown\") # adapt\n\n## List filenames ----\nbrown_fnames <- get_fnames(path_to_corpus)\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#inspect-objects",
    "href": "slides/init.html#inspect-objects",
    "title": "Reading and exploring a corpus",
    "section": "Inspect objects",
    "text": "Inspect objects\nIn the console:\n\npath_to_corpus = print(path_to_corpus)\nprint(brown_fnames, hide_path = path_to_corpus)\nexplore(brown_fnames)\ndrop_re(brown_fnames, \"/c[a-z]\")\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#create-a-frequency-list",
    "href": "slides/init.html#create-a-frequency-list",
    "title": "Reading and exploring a corpus",
    "section": "Create a frequency list",
    "text": "Create a frequency list\nCreate it on the script, inspect in the console (or from the script).\n\nbrown_fnames <- brown_fnames %>% \n  keep_re(\"/c[a-z]\")\nflist <- freqlist(brown_fnames, re_token_splitter = re(\"\\\\s+\"))\n\n\n\n\n\n\n\nTip\n\n\nCheck out the “freqlists” tutorial of {mclmtutorials} (learnr::run_tutorial(\"freqlists\", \"mclmtutorials\")) to learn why we need the re_token_splitter argument.\n\n\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#explore-the-frequency-list",
    "href": "slides/init.html#explore-the-frequency-list",
    "title": "Reading and exploring a corpus",
    "section": "Explore the frequency list",
    "text": "Explore the frequency list\n\nprint(flist, n = 3)\n\nFrequency list (types in list: 63517, tokens in list: 1162192)\nrank   type abs_freq nrm_freq\n---- ------ -------- --------\n   1 the/at    69013  593.818\n   2    ,/,    58153  500.373\n   3    ./.    48812  419.999\n...\n\nn_tokens(flist)\n\n[1] 1162192\n\nn_types(flist)\n\n[1] 63517\n\n\n\n\n\n\n\n\nTip\n\n\nCheck out the documentation\n\n\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#plot-frequencies-i",
    "href": "slides/init.html#plot-frequencies-i",
    "title": "Reading and exploring a corpus",
    "section": "Plot frequencies I",
    "text": "Plot frequencies I\n\nas_tibble(flist) %>% \n  ggplot(aes(x = rank, y = abs_freq)) +\n  geom_point(alpha = 0.3) +\n  theme_minimal()\n\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#plot-frequencies-ii",
    "href": "slides/init.html#plot-frequencies-ii",
    "title": "Reading and exploring a corpus",
    "section": "Plot frequencies II",
    "text": "Plot frequencies II\n\nas_tibble(flist) %>% \n  ggplot(aes(x = rank, y = abs_freq)) +\n  geom_point(alpha = 0.3) +\n  ggrepel::geom_text_repel(data = as_tibble(keep_bool(flist, flist > 10000)),\n                  aes(label = type), xlim = c(0, NA)) +\n  theme_minimal()\n\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#plot-frequencies-iii",
    "href": "slides/init.html#plot-frequencies-iii",
    "title": "Reading and exploring a corpus",
    "section": "Plot frequencies III",
    "text": "Plot frequencies III\n\nas_tibble(flist) %>%\n  mutate(freq_range = case_when(\n    abs_freq == 1 ~ \"1\",\n    abs_freq <= 5 ~ \"2-5\",\n    abs_freq <= 100 ~ \"6-100\",\n    abs_freq <= 1000 ~ \"101-1000\",\n    TRUE ~ \"> 1000\"\n  ) %>% fct_reorder(abs_freq)) %>% \n  ggplot(aes(x = freq_range)) +\n  geom_bar() +\n  geom_label(stat = \"count\", aes(label = ..count..))\n\n\n\nExplore corpus"
  },
  {
    "objectID": "slides/init.html#plot-frequencies-iii-output",
    "href": "slides/init.html#plot-frequencies-iii-output",
    "title": "Reading and exploring a corpus",
    "section": "Plot frequencies III",
    "text": "Plot frequencies III"
  },
  {
    "objectID": "slides/init.html#read-the-documentation",
    "href": "slides/init.html#read-the-documentation",
    "title": "Reading and exploring a corpus",
    "section": "Read the documentation",
    "text": "Read the documentation\nIt might be in a README file, online, as a paper…\n\nWhat time period(s) is/are covered?\nWhat genre(s)? Language varieties?\nWritten? Transcripts of oral texts?\nIs it a monitor corpus?\n\n\n\n\n\n\n\nLicenses\n\n\nCheck also the permissions you have as user of the corpus.\n\n\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#quarto-document",
    "href": "slides/init.html#quarto-document",
    "title": "Reading and exploring a corpus",
    "section": "Quarto document",
    "text": "Quarto document\n\nCreate basic Quarto document\nSet meta data on the YAML choosing output\nOptional: render to check it’s working\nRemove current text and write your own\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#code-in-a-quarto-document",
    "href": "slides/init.html#code-in-a-quarto-document",
    "title": "Reading and exploring a corpus",
    "section": "Code in a Quarto document",
    "text": "Code in a Quarto document\n\nInline code surrounded by backticks and starting with “r”.\nCode chunks: to run arbitrary code, create tables and plots, print glosses with {glossr}.\nRead external scripts with source() or with the code or file chunk options.\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#inline-code",
    "href": "slides/init.html#inline-code",
    "title": "Reading and exploring a corpus",
    "section": "Inline code",
    "text": "Inline code\nMarkdown\nThe Brown corpus used in this project has `r prettyNum(n_tokens(flist))` tokens\nand `r n_types(flist)` types,\ngiving us a type-token ratio of `r round(n_types(flist)/n_tokens(flist), 2)`.\n\nOutput\nThe Brown corpus used in this project has 1162192 tokens and 63517 types, giving us a type-token ratio of 0.05.\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#code-chunks",
    "href": "slides/init.html#code-chunks",
    "title": "Reading and exploring a corpus",
    "section": "Code chunks",
    "text": "Code chunks\n\ntbl <- (flist) %>% head(5)\ntbl\n\nFrequency list (types in list: 5, tokens in list: 239548)\n<total number of tokens: 1162192>\nrank orig_rank   type abs_freq nrm_freq\n---- --------- ------ -------- --------\n   1         1 the/at    69013  593.818\n   2         2    ,/,    58153  500.373\n   3         3    ./.    48812  419.999\n   4         4  of/in    35028  301.396\n   5         5 and/cc    28542  245.588\n\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#tables-with-knitrkable",
    "href": "slides/init.html#tables-with-knitrkable",
    "title": "Reading and exploring a corpus",
    "section": "Tables with knitr::kable()",
    "text": "Tables with knitr::kable()\n\nlibrary(knitr)\ntbl <- (flist) %>% head(5)\ntbl %>% \n  kable()\n\n\n\nTable 1: Top 5 types and their frequencies.\n\n\nrank\norig_rank\ntype\nabs_freq\nnrm_freq\n\n\n\n\n1\n1\nthe/at\n69013\n593.8175\n\n\n2\n2\n,/,\n58153\n500.3734\n\n\n3\n3\n./.\n48812\n419.9994\n\n\n4\n4\nof/in\n35028\n301.3960\n\n\n5\n5\nand/cc\n28542\n245.5876\n\n\n\n\n\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#tables-with-kableextrakbl",
    "href": "slides/init.html#tables-with-kableextrakbl",
    "title": "Reading and exploring a corpus",
    "section": "Tables with kableExtra::kbl()",
    "text": "Tables with kableExtra::kbl()\n\nlibrary(kableExtra)\ntbl <- (flist) %>% head(5)\ntbl %>% \n  kbl() %>% \n  kable_paper()\n\n\n\nTable 2:  Top 5 types and their frequencies. \n \n  \n    rank \n    orig_rank \n    type \n    abs_freq \n    nrm_freq \n  \n \n\n  \n    1 \n    1 \n    the/at \n    69013 \n    593.8175 \n  \n  \n    2 \n    2 \n    ,/, \n    58153 \n    500.3734 \n  \n  \n    3 \n    3 \n    ./. \n    48812 \n    419.9994 \n  \n  \n    4 \n    4 \n    of/in \n    35028 \n    301.3960 \n  \n  \n    5 \n    5 \n    and/cc \n    28542 \n    245.5876 \n  \n\n\n\n\n\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#import-code",
    "href": "slides/init.html#import-code",
    "title": "Reading and exploring a corpus",
    "section": "Import code",
    "text": "Import code\nIn both cases, you might want to use the include: false chunk option to avoid printing neither the code itself or its output.\n\nThe codesource()filecode=readLines\n\n\n# script.R    \nx <- \"New variable called x\"    \nprint(x)    \n\n\n\n```{r}\n#| label: setup-chunk\nsource(here::here(\"R\", \"script.R\"), local = knitr::knit_global())\n```\n\n[1] \"New variable called x\"\n\n\n\n\n\n```{r}\n#| label: file\n#| file: !expr here::here(\"R\", \"script.R\")\nx <- \"New variable called x\"\nprint(x)\n```\n\n[1] \"New variable called x\"\n\n\n\n\n\n```{r}\n#| label: code\n#| code: !expr readLines(here::here(\"R\", \"script.R\"))\nx <- \"New variable called x\"\nprint(x)\n```\n\n[1] \"New variable called x\"\n\n\n\n\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/init.html#stage-commit-push",
    "href": "slides/init.html#stage-commit-push",
    "title": "Reading and exploring a corpus",
    "section": "Stage, commit, push",
    "text": "Stage, commit, push\n\nCommit whenever you reach a “stage”.\nPush at most once a day.\n\ngit status\ngit add .\ngit commit -m \"my first quarto document\"\ngit push\n\n\n\n\n\n\nBranches and remotes\n\n\nThe first time you try to push from a local branch you may get an error! Just follow the instructions, don’t panic :)"
  },
  {
    "objectID": "slides/init.html#from-a-branch",
    "href": "slides/init.html#from-a-branch",
    "title": "Reading and exploring a corpus",
    "section": "From a branch",
    "text": "From a branch\nIf you were in your explore-corpus branch and want to bring changes to main\n\ngit mergegit checkout\n\n\nmain is now totally up-to-speed.\ngit checkout main\ngit merge explore-corpus\n\n\nGet only selected folders/files.\ngit checkout main\ngit checkout explore-corpus script.R\n\n\n\n\nDescribe corpus"
  },
  {
    "objectID": "slides/intro.html#outline",
    "href": "slides/intro.html#outline",
    "title": "Methods of Corpus Linguistics",
    "section": "Outline",
    "text": "Outline\n\nTopics\nSoftware to install\nFormat of the classes\nExam and submission procedure"
  },
  {
    "objectID": "slides/intro.html#corpus-linguistics",
    "href": "slides/intro.html#corpus-linguistics",
    "title": "Methods of Corpus Linguistics",
    "section": "Corpus linguistics",
    "text": "Corpus linguistics\n\nWhat is a corpus?\nWhat is corpus linguistics?\nCollocations and keywords analysis\nComparison of variants and of varieties\n\n\nTopics"
  },
  {
    "objectID": "slides/intro.html#statistical-techniques",
    "href": "slides/intro.html#statistical-techniques",
    "title": "Methods of Corpus Linguistics",
    "section": "Statistical techniques",
    "text": "Statistical techniques\n\nAssociation measures\n\nComparison of variants\n\nLogistic regression\nConditional inference trees\n\nComparison of varieties\n\nCorrespondence Analysis\nFactor Analysis\n\n\nTopics"
  },
  {
    "objectID": "slides/intro.html#programs",
    "href": "slides/intro.html#programs",
    "title": "Methods of Corpus Linguistics",
    "section": "Programs",
    "text": "Programs\n\nR – the programming language\nRStudio – IDE for R\nQuarto – publishing system\nTinytex (I will show you how)\n(Optionally) Git\n\n\n\n\n\n\n\nCosts?\n\n\nEverything open source and free!\n\n\n\n\nSoftware to install"
  },
  {
    "objectID": "slides/intro.html#r-packages-from-cran",
    "href": "slides/intro.html#r-packages-from-cran",
    "title": "Methods of Corpus Linguistics",
    "section": "R packages from CRAN",
    "text": "R packages from CRAN\nAfter installing R, you should install the following R packages.\n\ntidyverse (a group of data manipulation packages)\nca (for Correspondence Analysis)\nhere (for file paths)\nxml2 (to work with XML files)\neasystats (for reporting stats)1\nggeffects (to plot regression effects)\n\n\nSoftware to install\n\nYou might also want to run easystats::install_suggested() to install additional useful packages"
  },
  {
    "objectID": "slides/intro.html#r-development-packages",
    "href": "slides/intro.html#r-development-packages",
    "title": "Methods of Corpus Linguistics",
    "section": "R development packages",
    "text": "R development packages\n\nmclm (“masterclm/mclm”)\nmclmtutorials (“masterclm/mclmtutorials”)\nlearnr (“rstudio/learnr”)\ngradethis (“rstudio/gradethis”)\n(Optionally) glossr (“montesmariana/glossr”) if you want to write interlinear glosses\n\n\nSoftware to install"
  },
  {
    "objectID": "slides/intro.html#how-to-install-r-packages",
    "href": "slides/intro.html#how-to-install-r-packages",
    "title": "Methods of Corpus Linguistics",
    "section": "How to install R packages",
    "text": "How to install R packages\n\nFrom CRAN: with install.packages(\"package\")\n\n\ninstall.packages(c(\"tidyverse\", \"ca\", \"here\", \"xml2\", \"easystats\", \"ggeffects\"))\n\n\nDevelopment packages: with remotes::install_github(\"user/repo\")\n\n\nlibrary(remotes)\ninstall_github(\"masterclm/mclm\")\ninstall_github(\"masterclm/mclmtutorials\")\ninstall_github(\"rstudio/learnr\")\ninstall_github(\"rstudio/gradethis\")\n\n\n\nSoftware to install\n\n\nRun this code in the R console."
  },
  {
    "objectID": "slides/intro.html#technical-setup",
    "href": "slides/intro.html#technical-setup",
    "title": "Methods of Corpus Linguistics",
    "section": "Technical setup",
    "text": "Technical setup\n\nBasics of R, RStudio, R projects\nBasics of Quarto (to write your paper!)\nBasics of Git (option to submit the paper)\n\n\nFormat of the classes"
  },
  {
    "objectID": "slides/intro.html#theoretical-classes",
    "href": "slides/intro.html#theoretical-classes",
    "title": "Methods of Corpus Linguistics",
    "section": "Theoretical classes",
    "text": "Theoretical classes\nLectures on the different topics, with slides\n\nCorpus linguistics\nAssociation measures\nLogistic regression and conditional trees\nCorrespondence Analysis\nFactor Analysis\n\n\nFormat of the classes"
  },
  {
    "objectID": "slides/intro.html#case-studies",
    "href": "slides/intro.html#case-studies",
    "title": "Methods of Corpus Linguistics",
    "section": "Case studies",
    "text": "Case studies\nGoing through analyses, with notebooks and code you can copy-paste\n\nCollocation and keyword analysis\nAlternation studies: analysis of variants\nLectometry: analysis of varieties\nRegister analysis: analysis of varieties\n\n\nFormat of the classes"
  },
  {
    "objectID": "slides/intro.html#exam-format",
    "href": "slides/intro.html#exam-format",
    "title": "Methods of Corpus Linguistics",
    "section": "Exam format",
    "text": "Exam format\n\nPaper with analysis\n\nChoose a corpus and at least one technique\nDefine a research question that the technique can address\nSmall literature review\nPerform analysis\n\nFull project to be submitted: R code, paper written in Quarto, bibliography\n\n\nExam and submission procedure"
  },
  {
    "objectID": "slides/intro.html#submission-procedure",
    "href": "slides/intro.html#submission-procedure",
    "title": "Methods of Corpus Linguistics",
    "section": "Submission procedure",
    "text": "Submission procedure\nOption 1: Toledo\n\nTurn your project folder into a zip file (excluding corpus) and submit via Toledo Assignments\n\nOption 2: Git & GitHub\n\nFollow the instructions to set up the Git repository and push your project\nOptional intermediate assignments to get used to the tasks, Git and GitHub\n\n\nExam and submission procedure"
  },
  {
    "objectID": "slides/intro.html#git-submission---setup",
    "href": "slides/intro.html#git-submission---setup",
    "title": "Methods of Corpus Linguistics",
    "section": "Git submission - setup",
    "text": "Git submission - setup\n\nCreate an R project with a git repository\nStage and commit your changes, either in the Git tab or by typing git add . followed by git commit -m \"some message\" in the Git Bash Terminal\nAdd a remote to the repository by typing git remote add origin <url> in the Git Bash Terminal\nSet the main branch to “main” with git branch -M main if it isn’t already.\nUpload your changes with git push -u origin main\n\n\nExam and submission procedure"
  },
  {
    "objectID": "slides/intro.html#git-submission---later",
    "href": "slides/intro.html#git-submission---later",
    "title": "Methods of Corpus Linguistics",
    "section": "Git submission - later",
    "text": "Git submission - later\n\nStage your changes with git add . (. to stage everything)\nTake a snapshot of your work with git commit -m \"some message\"\nWhen you want to submit, run git push\n\n\nExam and submission procedure"
  },
  {
    "objectID": "slides/linear-regression.html#outline",
    "href": "slides/linear-regression.html#outline",
    "title": "Linear regression analysis",
    "section": "Outline",
    "text": "Outline\n\nSimple linear regression\nMultiple linear regression\nCategorical predictors\nEasystats"
  },
  {
    "objectID": "slides/linear-regression.html#set-up-code",
    "href": "slides/linear-regression.html#set-up-code",
    "title": "Linear regression analysis",
    "section": "Set up code",
    "text": "Set up code\nIn case you want to run this by yourself…\n\nlibrary(tidyverse)\nlibrary(kableExtra)\nset.seed(2022)\n\nutt_lengths <- tibble(age = 3:17) %>% \n  mutate(utterance_length = 3 + 0.5*age + rnorm(n(), sd = 0.3))\nm1 <- lm(utterance_length ~ age, data = utt_lengths)\nutt_lengths$fit <- m1$fitted.values\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#scatterplot",
    "href": "slides/linear-regression.html#scatterplot",
    "title": "Linear regression analysis",
    "section": "Scatterplot",
    "text": "Scatterplot\nWhat is the relationship between the age of a child and the mean length of their utterances (in a corpus)?\n\n\ng <- ggplot(utt_lengths, aes(\n  x = age,\n  y = utterance_length\n  )) +\n  geom_point(size = 3) +\n  labs(x = \"Age\",\n       y = \"Utterance length\") +\n  xlim(0,18) + ylim(0,15) +\n  theme_minimal(base_size = 20) +\n  theme(aspect.ratio = 1)\ng\n\n\n\n\n\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#intercept",
    "href": "slides/linear-regression.html#intercept",
    "title": "Linear regression analysis",
    "section": "Intercept",
    "text": "Intercept\nThe average utterance length is 7.885:\n\n\ng_int <- g +\n  geom_hline(\n    aes(\n      yintercept = mean(utterance_length)\n      ),\n    linetype = 2,\n    color = \"coral\",\n    size = 1\n    )\ng_int\n\n\n\n\n\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#fitted-model",
    "href": "slides/linear-regression.html#fitted-model",
    "title": "Linear regression analysis",
    "section": "Fitted model",
    "text": "Fitted model\nBut we try to find a line that fits the trend the best:\n\n\ng_fit <- g +\n  geom_abline(\n    intercept = m1$coefficients[[1]],\n    slope = m1$coefficients[[2]],\n    linetype = 2,\n    color = \"darkcyan\",\n    size = 1\n    )\ng_fit\n\n\n\n\n\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#error",
    "href": "slides/linear-regression.html#error",
    "title": "Linear regression analysis",
    "section": "Error",
    "text": "Error\n\n\nSum of squares total\n\n\nCode\ng_int +\n  geom_segment(aes(xend = age, yend = mean(utterance_length)), color = \"coral3\", size = 2)\n\n\n\n\n\n\nSum of squares error\n\n\nCode\ng_fit +\n  geom_segment(aes(xend = age, yend = fit), color = \"cyan4\", size = 2)\n\n\n\n\n\n\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#data",
    "href": "slides/linear-regression.html#data",
    "title": "Linear regression analysis",
    "section": "Data",
    "text": "Data\n\n\nCode\nutt_lengths %>% mutate(intercept = mean(utterance_length)) %>%\n  slice_sample(n = 7) %>% kb_style()\n\n\n\n\n \n  \n    age \n    utterance_length \n    fit \n    intercept \n  \n \n\n  \n    17 \n    11.48 \n    11.52 \n    7.88 \n  \n  \n    14 \n    9.94 \n    9.96 \n    7.88 \n  \n  \n    16 \n    11.03 \n    11.00 \n    7.88 \n  \n  \n    8 \n    6.13 \n    6.85 \n    7.88 \n  \n  \n    9 \n    7.18 \n    7.37 \n    7.88 \n  \n  \n    15 \n    10.21 \n    10.48 \n    7.88 \n  \n  \n    7 \n    6.40 \n    6.33 \n    7.88 \n  \n\n\n\n\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#lm-output",
    "href": "slides/linear-regression.html#lm-output",
    "title": "Linear regression analysis",
    "section": "lm() output",
    "text": "lm() output\n\n\nCode\nsummary(m1)\n\n\n\nCall:\nlm(formula = utterance_length ~ age, data = utt_lengths)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-0.717 -0.154 -0.016  0.174  0.518 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.6958     0.2036    13.2  6.3e-09 ***\nage           0.5189     0.0187    27.8  5.8e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.313 on 13 degrees of freedom\nMultiple R-squared:  0.983, Adjusted R-squared:  0.982 \nF-statistic:  771 on 1 and 13 DF,  p-value: 5.84e-13\n\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#simple-linear-regression-1",
    "href": "slides/linear-regression.html#simple-linear-regression-1",
    "title": "Linear regression analysis",
    "section": "Simple linear regression",
    "text": "Simple linear regression\n\nSimple\n\none predictor variable\n\nLinear\n\nlinear relation between estimated parameters and response variable\n\nNotation\n\ny ~ x\n\nEstimation: OSL\n\nOrdinary least squares, minimizing sum of squares of residuals\n\n\n\nSimple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#add-a-predictor",
    "href": "slides/linear-regression.html#add-a-predictor",
    "title": "Linear regression analysis",
    "section": "Add a predictor",
    "text": "Add a predictor\n\nset.seed(7)\nutt_lengths <- utt_lengths %>% \n  mutate(pages_read = n()/2 + rnorm(n(), mean = 6, sd = 3))\n\nm2 <- lm(utterance_length ~ age + pages_read, data = utt_lengths)\nutt_lengths$fit2 <- m2$fitted.values\n\nutt_lengths %>% slice_tail(n = 7) %>% kb_style()\n\n\n\n \n  \n    age \n    utterance_length \n    fit \n    pages_read \n    fit2 \n  \n \n\n  \n    11 \n    8.72 \n    8.40 \n    14.0 \n    8.35 \n  \n  \n    12 \n    9.07 \n    8.92 \n    20.1 \n    9.04 \n  \n  \n    13 \n    9.80 \n    9.44 \n    14.6 \n    9.38 \n  \n  \n    14 \n    9.94 \n    9.96 \n    21.6 \n    10.10 \n  \n  \n    15 \n    10.21 \n    10.48 \n    20.3 \n    10.56 \n  \n  \n    16 \n    11.03 \n    11.00 \n    14.5 \n    10.89 \n  \n  \n    17 \n    11.48 \n    11.52 \n    19.2 \n    11.54 \n  \n\n\n\n\n\n\nMultiple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#scatterplot-and-fitted-model",
    "href": "slides/linear-regression.html#scatterplot-and-fitted-model",
    "title": "Linear regression analysis",
    "section": "Scatterplot and fitted model",
    "text": "Scatterplot and fitted model\n\n\nCode\nplt = require(\"https://cdn.plot.ly/plotly-latest.min.js\")\npoints = [{x: data.age, y: data.pages_read, z: data.utterance_length,\ntype: \"scatter3d\", mode: \"markers\", hovertemplate: '<b>Age</b>: %{x}' + '<br><b>UT</b>: %{z:.2f}<br>' +  '<b>PR</b>: %{y:.2f}'}]\nsurface = [{x: data.age, y: data.pages_read, z: surface_fit,\ntype: \"surface\", showscale: false}]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nviewof traces = Inputs.checkbox(\n  [\n    {value: points[0], label: \"Observed values\"},\n    {value: surface[0], label: \"Fitted plane\"}\n  ],\n  {\n    value: [points[0]],\n    label: \"Show:\",\n    format: (d) => d.label,\n    valueof: (d) => d.value\n  }\n)\n\n\n\n\n\n\n\n\n\nCode\nplt.newPlot(DOM.element(\"div\"), traces, layout)\n\n\n\n\n\n\n\n\nMultiple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#model",
    "href": "slides/linear-regression.html#model",
    "title": "Linear regression analysis",
    "section": "Model",
    "text": "Model\n\n\nCode\nsummary(m2)\n\n\n\nCall:\nlm(formula = utterance_length ~ age + pages_read, data = utt_lengths)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.6100 -0.1804 -0.0159  0.2139  0.4207 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.3841     0.3157    7.55  6.7e-06 ***\nage           0.5052     0.0212   23.82  1.8e-11 ***\npages_read    0.0295     0.0232    1.27     0.23    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.306 on 12 degrees of freedom\nMultiple R-squared:  0.985, Adjusted R-squared:  0.983 \nF-statistic:  405 on 2 and 12 DF,  p-value: 9.72e-12\n\n\n\nMultiple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#multiple-linear-regression-1",
    "href": "slides/linear-regression.html#multiple-linear-regression-1",
    "title": "Linear regression analysis",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nMultiple\n\nmore than one predictor\n\nNotation\n\ny ~ x1 + x2 (fitting on a plane)\n\n\ny ~ x1 + x2 + ... + xn (fitting on a hyperplane)\n\nEstimation: OSL\n\nOrdinary least squares, minimizing sum of squares of residuals\n\n\n\nMultiple linear regression"
  },
  {
    "objectID": "slides/linear-regression.html#two-levels",
    "href": "slides/linear-regression.html#two-levels",
    "title": "Linear regression analysis",
    "section": "Two levels",
    "text": "Two levels\n\nE.g. “monolingual family”: yes/no\nTranslated into 1 dummy predictor with two levels\n\n0: reference level, e.g. no\n1: other level, e.g. yes\n\nLine fitted between 0 and 1, slope is the difference in y when yes (compared to no)\n\n\nCategorical predictors"
  },
  {
    "objectID": "slides/linear-regression.html#two-levels-example",
    "href": "slides/linear-regression.html#two-levels-example",
    "title": "Linear regression analysis",
    "section": "Two levels: example",
    "text": "Two levels: example\n\nutt_lengths_cat <- utt_lengths %>% mutate(mono = factor(c(rep(\"no\", 5), \"yes\", \"no\", \"no\", rep(\"yes\", 7))))\n\n\n\n\n\nCode\nggplot(utt_lengths_cat, aes(x = as.numeric(mono)-1, y = utterance_length)) +\n  geom_point(size = 3) + geom_smooth(method = \"lm\", se = FALSE, color = \"darkolivegreen4\") +\n  labs(x = \"Is the family monolingual?\", y = \"Utterance length\") +\n  ylim(c(0,15)) + scale_x_continuous(breaks = c(0, 1), labels = c(\"no\", \"yes\")) +\n  theme_minimal(base_size = 20) + theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nCode\nsummary(lm(utterance_length ~ mono, data = utt_lengths_cat))\n\n\n\nCall:\nlm(formula = utterance_length ~ mono, data = utt_lengths_cat)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-3.419 -0.788  0.253  0.928  2.100 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    5.983      0.566    10.6  9.5e-08 ***\nmonoyes        3.566      0.776     4.6    5e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.5 on 13 degrees of freedom\nMultiple R-squared:  0.619, Adjusted R-squared:  0.59 \nF-statistic: 21.1 on 1 and 13 DF,  p-value: 5e-04\n\n\n\n\n\nCategorical predictors"
  },
  {
    "objectID": "slides/linear-regression.html#more-than-two-levels",
    "href": "slides/linear-regression.html#more-than-two-levels",
    "title": "Linear regression analysis",
    "section": "More than two levels",
    "text": "More than two levels\n\nE.g. “L1”: EN, ES, FR…\nTranslated into n-1 dummy predictors with two levels\n\n1: one of the levels, not the reference level, e.g. Es\n0: the rest of the levels, including the reference level (e.g. EN)\n\n(Hyper)plane fitted between combinations of 0 and 1, slope is the difference in y when compared to the reference level.\n\n\nCategorical predictors"
  },
  {
    "objectID": "slides/linear-regression.html#data-categorical-predictors",
    "href": "slides/linear-regression.html#data-categorical-predictors",
    "title": "Linear regression analysis",
    "section": "Data: categorical predictors",
    "text": "Data: categorical predictors\n\nmean_utts <- c(EN = 11, ES = 7.4, ZH = 7.8)\nset.seed(2022)\nby_lang <- tibble(L1 = factor(rep(c(\"EN\", \"ES\", \"ZH\"), 15))) %>% \n  mutate(utterance_length = rnorm(n(), mean = mean_utts[L1], sd = 0.8))\nkb_style(filter(by_lang, L1 == \"EN\"), \"float_left\")\nkb_style(filter(by_lang, L1 == \"ES\"), \"float_left\")\nkb_style(filter(by_lang, L1 == \"ZH\"), \"left\")\n\n\n\n\n\n \n  \n    L1 \n    utterance_length \n  \n \n\n  \n    EN \n    11.72 \n  \n  \n    EN \n    9.84 \n  \n  \n    EN \n    10.15 \n  \n  \n    EN \n    11.19 \n  \n  \n    EN \n    10.21 \n  \n  \n    EN \n    10.94 \n  \n  \n    EN \n    11.82 \n  \n  \n    EN \n    11.31 \n  \n  \n    EN \n    10.72 \n  \n  \n    EN \n    11.26 \n  \n\n\n\n\n\n\n\n \n  \n    L1 \n    utterance_length \n  \n \n\n  \n    ES \n    6.46 \n  \n  \n    ES \n    7.13 \n  \n  \n    ES \n    7.62 \n  \n  \n    ES \n    8.21 \n  \n  \n    ES \n    7.47 \n  \n  \n    ES \n    6.88 \n  \n  \n    ES \n    8.09 \n  \n  \n    ES \n    8.29 \n  \n  \n    ES \n    6.71 \n  \n  \n    ES \n    6.99 \n  \n\n\n\n\n\n\n\n \n  \n    L1 \n    utterance_length \n  \n \n\n  \n    ZH \n    7.08 \n  \n  \n    ZH \n    5.48 \n  \n  \n    ZH \n    8.40 \n  \n  \n    ZH \n    7.65 \n  \n  \n    ZH \n    7.76 \n  \n  \n    ZH \n    7.04 \n  \n  \n    ZH \n    8.09 \n  \n  \n    ZH \n    8.77 \n  \n  \n    ZH \n    8.32 \n  \n  \n    ZH \n    7.61 \n  \n\n\n\n\n\n\n\n\nCategorical predictors"
  },
  {
    "objectID": "slides/linear-regression.html#model-with-categorical-predictors",
    "href": "slides/linear-regression.html#model-with-categorical-predictors",
    "title": "Linear regression analysis",
    "section": "Model with categorical predictors",
    "text": "Model with categorical predictors\n\n\n\n\nCode\nm_lang1 <- lm(utterance_length ~ L1, data = by_lang)\nsummary(m_lang1)\n\n\n\nCall:\nlm(formula = utterance_length ~ L1, data = by_lang)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2294 -0.5615  0.0488  0.6111  1.0603 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   10.928      0.189    57.9  < 2e-16 ***\nL1ES          -3.620      0.267   -13.6  < 2e-16 ***\nL1ZH          -3.219      0.267   -12.1  3.1e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.731 on 42 degrees of freedom\nMultiple R-squared:  0.84,  Adjusted R-squared:  0.833 \nF-statistic:  111 on 2 and 42 DF,  p-value: <2e-16\n\n\n\n\n\nCode\nes_first <- mutate(by_lang, L1 = fct_relevel(L1, \"ES\"))\nm_lang2 <- lm(utterance_length ~ L1, data = es_first)\nsummary(m_lang2)\n\n\n\nCall:\nlm(formula = utterance_length ~ L1, data = es_first)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2294 -0.5615  0.0488  0.6111  1.0603 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    7.308      0.189    38.7   <2e-16 ***\nL1EN           3.620      0.267    13.6   <2e-16 ***\nL1ZH           0.401      0.267     1.5     0.14    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.731 on 42 degrees of freedom\nMultiple R-squared:  0.84,  Adjusted R-squared:  0.833 \nF-statistic:  111 on 2 and 42 DF,  p-value: <2e-16\n\n\n\n\n\nCategorical predictors"
  },
  {
    "objectID": "slides/linear-regression.html#categorical-predictors-1",
    "href": "slides/linear-regression.html#categorical-predictors-1",
    "title": "Linear regression analysis",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nComparison is done between each level and the reference level, not in all combinations\nt-test: does the slope of an individual dummy predictor differ from 0?\nF-test of nested models: do the dummy predictors jointly reduce unexplained variation?\n\n\nCategorical predictors"
  },
  {
    "objectID": "slides/linear-regression.html#report-package",
    "href": "slides/linear-regression.html#report-package",
    "title": "Linear regression analysis",
    "section": "{report} package",
    "text": "{report} package\n\nSimpleMultiple\n\n\n\n\nCode\nreport(m1)\n\n\nWe fitted a linear model (estimated using OLS) to predict utterance_length with\nage (formula: utterance_length ~ age). The model explains a statistically\nsignificant and substantial proportion of variance (R2 = 0.98, F(1, 13) =\n771.19, p < .001, adj. R2 = 0.98). The model's intercept, corresponding to age\n= 0, is at 2.70 (95% CI [2.26, 3.14], t(13) = 13.24, p < .001). Within this\nmodel:\n\n  - The effect of age is statistically significant and positive (beta = 0.52, 95%\nCI [0.48, 0.56], t(13) = 27.77, p < .001; Std. beta = 0.99, 95% CI [0.91,\n1.07])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\n\n\n\n\nCode\nreport(m2)\n\n\nWe fitted a linear model (estimated using OLS) to predict utterance_length with\nage (formula: utterance_length ~ age + pages_read). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.99,\nF(2, 12) = 404.69, p < .001, adj. R2 = 0.98). The model's intercept,\ncorresponding to age = 0, is at 2.38 (95% CI [1.70, 3.07], t(12) = 7.55, p <\n.001). Within this model:\n\n  - The effect of age is statistically significant and positive (beta = 0.51, 95%\nCI [0.46, 0.55], t(12) = 23.82, p < .001; Std. beta = 0.97, 95% CI [0.88,\n1.05])\n  - The effect of pages read is statistically non-significant and positive (beta\n= 0.03, 95% CI [-0.02, 0.08], t(12) = 1.27, p = 0.228; Std. beta = 0.05, 95% CI\n[-0.04, 0.14])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation. and We fitted a linear\nmodel (estimated using OLS) to predict utterance_length with pages_read\n(formula: utterance_length ~ age + pages_read). The model explains a\nstatistically significant and substantial proportion of variance (R2 = 0.99,\nF(2, 12) = 404.69, p < .001, adj. R2 = 0.98). The model's intercept,\ncorresponding to pages_read = 0, is at 2.38 (95% CI [1.70, 3.07], t(12) = 7.55,\np < .001). Within this model:\n\n  - The effect of age is statistically significant and positive (beta = 0.51, 95%\nCI [0.46, 0.55], t(12) = 23.82, p < .001; Std. beta = 0.97, 95% CI [0.88,\n1.05])\n  - The effect of pages read is statistically non-significant and positive (beta\n= 0.03, 95% CI [-0.02, 0.08], t(12) = 1.27, p = 0.228; Std. beta = 0.05, 95% CI\n[-0.04, 0.14])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n\n\n\n\n\n\nEasystats"
  },
  {
    "objectID": "slides/linear-regression.html#parameters-package",
    "href": "slides/linear-regression.html#parameters-package",
    "title": "Linear regression analysis",
    "section": "{parameters} package",
    "text": "{parameters} package\n\nSimpleMultipleComparison\n\n\n\n\nCode\nprint_md(model_parameters(m1))\nplot(model_parameters(m1))\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(13)\np\n\n\n\n\n(Intercept)\n2.70\n0.20\n(2.26, 3.14)\n13.24\n< .001\n\n\nage\n0.52\n0.02\n(0.48, 0.56)\n27.77\n< .001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint_md(model_parameters(m2))\nplot(model_parameters(m2))\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(12)\np\n\n\n\n\n(Intercept)\n2.38\n0.32\n(1.70, 3.07)\n7.55\n< .001\n\n\nage\n0.51\n0.02\n(0.46, 0.55)\n23.82\n< .001\n\n\npages read\n0.03\n0.02\n(-0.02, 0.08)\n1.27\n0.228\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nprint_md(compare_parameters(m1, m2))\nplot(compare_parameters(m1, m2))\n\n\n\n\n\n\n\nParameter\nm1\nm2\n\n\n\n\n(Intercept)\n2.70 (2.26, 3.14)\n2.38 (1.70, 3.07)\n\n\nage\n0.52 (0.48, 0.56)\n0.51 (0.46, 0.55)\n\n\npages read\n\n0.03 (-0.02, 0.08)\n\n\n\n\n\n\n\nObservations\n15\n15\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEasystats"
  },
  {
    "objectID": "slides/linear-regression.html#performance-package-i",
    "href": "slides/linear-regression.html#performance-package-i",
    "title": "Linear regression analysis",
    "section": "{performance} package I",
    "text": "{performance} package I\n\nSimpleMultipleComparison\n\n\n\n\nCode\nprint_md(model_performance(m1))\n\n\n\nIndices of model performance\n\n\nAIC\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n11.54\n13.67\n0.98\n0.98\n0.29\n0.31\n\n\n\n\n\n\n\n\n\nCode\nprint_md(model_performance(m2))\n\n\n\nIndices of model performance\n\n\nAIC\nBIC\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\n11.65\n14.48\n0.99\n0.98\n0.27\n0.31\n\n\n\n\n\n\n\n\n\nCode\nm_perf <- compare_performance(m1, m2)\nprint_md(m_perf)\nplot(m_perf)\n\n\n\n\n\nComparison of Model Performance Indices\n\n\n\n\n\n\n\n\n\n\n\n\nName\nModel\nAIC (weights)\nBIC (weights)\nR2\nR2 (adj.)\nRMSE\nSigma\n\n\n\n\nm1\nlm\n11.5 (0.51)\n13.7 (0.60)\n0.98\n0.98\n0.29\n0.31\n\n\nm2\nlm\n11.6 (0.49)\n14.5 (0.40)\n0.99\n0.98\n0.27\n0.31\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEasystats"
  },
  {
    "objectID": "slides/linear-regression.html#performance-package-ii",
    "href": "slides/linear-regression.html#performance-package-ii",
    "title": "Linear regression analysis",
    "section": "{performance} package II",
    "text": "{performance} package II\n\nSimpleMultiple\n\n\n\n\nCode\ncheck_heteroscedasticity(m1)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.237).\n\n\nCode\ncheck_model(m1, check = c(\"qq\", \"ncv\"))\n\n\n\n\n\n\n\n\n\nCode\ncheck_heteroscedasticity(m2)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.947).\n\n\nCode\ncheck_model(m2, check = c(\"qq\", \"ncv\"))\n\n\n\n\n\n\n\n\n\nEasystats"
  },
  {
    "objectID": "slides/logistic-regression.html#outline",
    "href": "slides/logistic-regression.html#outline",
    "title": "Logistic regression analysis",
    "section": "Outline",
    "text": "Outline\n\nIntroduction\nProbabilities, odds and logit\nEvaluation\nInteractions\nRandom effects"
  },
  {
    "objectID": "slides/logistic-regression.html#set-up-code",
    "href": "slides/logistic-regression.html#set-up-code",
    "title": "Logistic regression analysis",
    "section": "Set up code",
    "text": "Set up code\n\nset.seed(2022)\ngonna <- tibble(variant = rep(\"gonna\", 50), comp_length = rnorm(50, 6, 0.8),\n                register = sample(c(\"formal\", \"informal\"), 50, replace = TRUE, prob = c(0.1, 0.9)))\ngoing_to <- tibble(variant = rep(\"going_to\", 50), comp_length = rnorm(50, 3, 1.2),\n                   register = sample(c(\"formal\", \"informal\"), 50, replace = TRUE, prob = c(0.7, 0.3)))\ngt <- bind_rows(gonna, going_to) %>%\n  mutate(\n    comp_length = if_else(comp_length > 1, comp_length, 1),\n    variant = fct_relevel(variant, \"gonna\"),\n    variant_num = as.numeric(variant)-1,\n    register = fct_relevel(register, \"informal\"),\n    source = factor(sample(letters, n(), replace = TRUE))\n  )\nm1 <- glm(variant ~ comp_length, data = gt, family = binomial(logit))\ngt$fit1 <- m1$fitted.values\nm2 <- glm(variant ~ comp_length + register, data = gt, family = binomial(logit))\ngt$fit2 <- m2$fitted.values\n\ngonna_plot <- ggplot(gt, aes(x = comp_length, y = variant_num)) +\n  geom_point(size = 3) + ylim(0, 1) +\n  labs(x = \"Length of complement\", y = \"Choice of construction\") +\n  scale_y_continuous(breaks = c(0, 1), labels = c(\"gonna\", \"going to\")) +\n  theme_minimal(base_size = 20) + theme(aspect.ratio = 1)\n\n\nIntroduction"
  },
  {
    "objectID": "slides/logistic-regression.html#gonna-dataset",
    "href": "slides/logistic-regression.html#gonna-dataset",
    "title": "Logistic regression analysis",
    "section": "gonna dataset",
    "text": "gonna dataset\n\n\nCode\nkb_style(filter(gt, variant == \"gonna\"), \"float_left\")\nkb_style(filter(gt, variant == \"going_to\"), \"left\")\n\n\n\n\n\n\n \n  \n    variant \n    comp_length \n    register \n    variant_num \n    source \n    fit1 \n    fit2 \n  \n \n\n  \n    gonna \n    6.72 \n    formal \n    0 \n    q \n    0.008 \n    0.050 \n  \n  \n    gonna \n    5.06 \n    informal \n    0 \n    x \n    0.348 \n    0.137 \n  \n  \n    gonna \n    5.28 \n    informal \n    0 \n    z \n    0.233 \n    0.081 \n  \n  \n    gonna \n    4.84 \n    informal \n    0 \n    i \n    0.480 \n    0.221 \n  \n  \n    gonna \n    5.74 \n    informal \n    0 \n    m \n    0.088 \n    0.026 \n  \n  \n    gonna \n    3.68 \n    informal \n    0 \n    f \n    0.946 \n    0.861 \n  \n  \n    gonna \n    5.15 \n    informal \n    0 \n    j \n    0.297 \n    0.111 \n  \n  \n    gonna \n    6.22 \n    informal \n    0 \n    p \n    0.027 \n    0.007 \n  \n  \n    gonna \n    6.60 \n    informal \n    0 \n    p \n    0.011 \n    0.003 \n  \n  \n    gonna \n    6.19 \n    formal \n    0 \n    c \n    0.029 \n    0.174 \n  \n\n\n\n\n\n\n\n \n  \n    variant \n    comp_length \n    register \n    variant_num \n    source \n    fit1 \n    fit2 \n  \n \n\n  \n    going_to \n    2.75 \n    formal \n    1 \n    g \n    0.995 \n    0.999 \n  \n  \n    going_to \n    4.22 \n    formal \n    1 \n    r \n    0.817 \n    0.975 \n  \n  \n    going_to \n    4.64 \n    formal \n    1 \n    x \n    0.608 \n    0.929 \n  \n  \n    going_to \n    4.77 \n    formal \n    1 \n    q \n    0.526 \n    0.901 \n  \n  \n    going_to \n    4.07 \n    informal \n    1 \n    f \n    0.869 \n    0.691 \n  \n  \n    going_to \n    1.78 \n    formal \n    1 \n    y \n    1.000 \n    1.000 \n  \n  \n    going_to \n    5.25 \n    formal \n    1 \n    p \n    0.251 \n    0.723 \n  \n  \n    going_to \n    4.29 \n    informal \n    1 \n    g \n    0.789 \n    0.551 \n  \n  \n    going_to \n    1.71 \n    informal \n    1 \n    h \n    1.000 \n    0.999 \n  \n  \n    going_to \n    1.00 \n    formal \n    1 \n    c \n    1.000 \n    1.000 \n  \n\n\n\n\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/logistic-regression.html#scatterplot",
    "href": "slides/logistic-regression.html#scatterplot",
    "title": "Logistic regression analysis",
    "section": "Scatterplot",
    "text": "Scatterplot\nWhat is the relationship between the length of the complement and the choice of going to/gonna?\n\n\nCode\ngonna_plot\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/logistic-regression.html#linear-fit",
    "href": "slides/logistic-regression.html#linear-fit",
    "title": "Logistic regression analysis",
    "section": "Linear fit",
    "text": "Linear fit\nThe relationship is not linear.\n\n\n\n\nCode\ngonna_plot + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkolivegreen4\")\n\n\n\n\n\n\n\nThe residuals are not normally distributed.\nNot all values are possible (probabilities go between 0 and 1).\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/logistic-regression.html#logistic-fit",
    "href": "slides/logistic-regression.html#logistic-fit",
    "title": "Logistic regression analysis",
    "section": "Logistic fit",
    "text": "Logistic fit\nThe fit has an S shape.\n\n\nCode\ngonna_plot +\n  geom_line(aes(y = fit1), color = \"goldenrod\", size = 2)\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/logistic-regression.html#model",
    "href": "slides/logistic-regression.html#model",
    "title": "Logistic regression analysis",
    "section": "Model",
    "text": "Model\n\n\nCode\nsummary(m2)\n\n\n\nCall:\nglm(formula = variant ~ comp_length + register, family = binomial(logit), \n    data = gt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9876  -0.2053  -0.0209   0.1067   2.1787  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      11.582      2.899    4.00  6.5e-05 ***\ncomp_length      -2.651      0.613   -4.32  1.5e-05 ***\nregisterformal    3.284      1.068    3.08   0.0021 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.63  on 99  degrees of freedom\nResidual deviance:  34.02  on 97  degrees of freedom\nAIC: 40.02\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/logistic-regression.html#interpretation",
    "href": "slides/logistic-regression.html#interpretation",
    "title": "Logistic regression analysis",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nIntercept\n\nlog odds of outcome (“going to”) when all predictors are at 0 (comp_length = 0).\nodds = exp(11.582) = 107138\nprob = odds/(odds+1) \\(\\approx\\) 1\n\n\nCoefficients\n\nlog odds ratios: positive increases chances of “going to”, negative of “gonna”.\nodds ratio\n\nof comp_length = exp(-2.651) = 0.071\nof register = exp(3.284) = 26.676\n\n\n\n\nThe odds of going to vs gonna in the formal register are 26.676 times higher than those in the informal register, other variables being controlled for.\n\n\n\n\n\nIntroduction"
  },
  {
    "objectID": "slides/logistic-regression.html#probabilities",
    "href": "slides/logistic-regression.html#probabilities",
    "title": "Logistic regression analysis",
    "section": "Probabilities",
    "text": "Probabilities\n\n\nprobs.\n\n\\(P\\)\n0 - 0.5 - 1\nNumber of successes divided by number of trials.\n\n\n\n\n\n\nProbabilities, odds and logit"
  },
  {
    "objectID": "slides/logistic-regression.html#odds",
    "href": "slides/logistic-regression.html#odds",
    "title": "Logistic regression analysis",
    "section": "Odds",
    "text": "Odds\n\n\nprobs.\n\n\\(P\\)\n0 - 0.5 - 1\nNumber of successes divided by number of trials.\n\n\nodds\n\n\\(\\frac{P}{1-P}\\)\n0 - 1 - \\(\\infty\\)\nProbability of success divided by the probability of failure.\nUndefined for \\(P=1\\).\n\n\n\n\n\n\nProbabilities, odds and logit"
  },
  {
    "objectID": "slides/logistic-regression.html#logit",
    "href": "slides/logistic-regression.html#logit",
    "title": "Logistic regression analysis",
    "section": "Logit",
    "text": "Logit\n\n\nprobs.\n\n\\(P\\)\n0 - 0.5 - 1\nNumber of successes divided by number of trials.\n\n\nodds\n\n\\(\\frac{P}{1-P}\\)\n0 - 1 - \\(\\infty\\)\nProbability of success divided by the probability of failure.\nUndefined for \\(P=1\\).\n\n\nlogit\n\n\\(\\log\\left(\\frac{P}{1-P}\\right)\\)\n\\(-\\infty\\) - 0 - \\(\\infty\\)\nIf positive, success is more likely; if negative failure is more likely.\nUndefined for \\(P=0\\) and for \\(P=1\\).\n\n\n\n\nProbabilities, odds and logit"
  },
  {
    "objectID": "slides/logistic-regression.html#simulation",
    "href": "slides/logistic-regression.html#simulation",
    "title": "Logistic regression analysis",
    "section": "Simulation",
    "text": "Simulation\n\n\nlibrary(MASS) # to print fractions\n\nprobabilities <- c(1/c(7:2),\n                   1-(1/c(3:7)))\nprobs <- tibble(\n  P = probabilities,\n  P_frac = as.character(fractions(P)),\n  odds = P/(1-P),\n  odds_frac = as.character(fractions(odds)),\n  logit = log(odds)\n)\nkbl(probs) %>% kable_paper(font_size = 22) %>% \n  row_spec(6, bold = TRUE)\n\n\n\n\n \n  \n    P \n    P_frac \n    odds \n    odds_frac \n    logit \n  \n \n\n  \n    0.143 \n    1/7 \n    0.167 \n    1/6 \n    -1.792 \n  \n  \n    0.167 \n    1/6 \n    0.200 \n    1/5 \n    -1.609 \n  \n  \n    0.200 \n    1/5 \n    0.250 \n    1/4 \n    -1.386 \n  \n  \n    0.250 \n    1/4 \n    0.333 \n    1/3 \n    -1.099 \n  \n  \n    0.333 \n    1/3 \n    0.500 \n    1/2 \n    -0.693 \n  \n  \n    0.500 \n    1/2 \n    1.000 \n    1 \n    0.000 \n  \n  \n    0.667 \n    2/3 \n    2.000 \n    2 \n    0.693 \n  \n  \n    0.750 \n    3/4 \n    3.000 \n    3 \n    1.099 \n  \n  \n    0.800 \n    4/5 \n    4.000 \n    4 \n    1.386 \n  \n  \n    0.833 \n    5/6 \n    5.000 \n    5 \n    1.609 \n  \n  \n    0.857 \n    6/7 \n    6.000 \n    6 \n    1.792 \n  \n\n\n\n\n\n\n\nWe’ll create a vector probabilities with the values of fractions from \\(\\frac{1}{7}\\) to \\(\\frac{1}{2}\\) and then from \\(1-\\frac{1}{3}\\) to \\(1-\\frac{1}{7}\\).\nMASS::fractions() prints them as fractions.\nFrom there we compute odds and logit.\n\n\nProbabilities, odds and logit"
  },
  {
    "objectID": "slides/logistic-regression.html#linear-vs-logistic-relationships",
    "href": "slides/logistic-regression.html#linear-vs-logistic-relationships",
    "title": "Logistic regression analysis",
    "section": "Linear vs logistic relationships",
    "text": "Linear vs logistic relationships\nLinear relation logit ~ x entails logistic curve p ~ x.\n\n\nCode\nwith_x <- tibble(x = 1:30, logit = -3.5 + 0.3*x, odds = exp(logit), P = odds/(1+odds))\n\n\n\n\n\n\nCode\nlnplot(with_x, x, logit, \"logit ~ X\")\n\n\n\n\n\n\n\n\nCode\nlnplot(with_x, x, P, \"P ~ X\")\n\n\n\n\n\n\n\n\nProbabilities, odds and logit"
  },
  {
    "objectID": "slides/logistic-regression.html#model-1",
    "href": "slides/logistic-regression.html#model-1",
    "title": "Logistic regression analysis",
    "section": "Model",
    "text": "Model\n\n\nCode\nsummary(m2)\n\n\n\nCall:\nglm(formula = variant ~ comp_length + register, family = binomial(logit), \n    data = gt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9876  -0.2053  -0.0209   0.1067   2.1787  \n\nCoefficients:\n               Estimate Std. Error z value Pr(>|z|)    \n(Intercept)      11.582      2.899    4.00  6.5e-05 ***\ncomp_length      -2.651      0.613   -4.32  1.5e-05 ***\nregisterformal    3.284      1.068    3.08   0.0021 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.63  on 99  degrees of freedom\nResidual deviance:  34.02  on 97  degrees of freedom\nAIC: 40.02\n\nNumber of Fisher Scoring iterations: 7\n\n\n\nEvaluation"
  },
  {
    "objectID": "slides/logistic-regression.html#deviance",
    "href": "slides/logistic-regression.html#deviance",
    "title": "Logistic regression analysis",
    "section": "Deviance",
    "text": "Deviance\n\nNull deviance\n\ndiscrepancy between data and intercept-only model (like SST)\n\nResidual deviance\n\ndiscrepancy between data and fitted model (like SSE)\n\n\n\n\n\n\n\n\nComputation\n\n\n\\(-2 \\log(L)\\) with \\(L\\) the likelihood of encountering the data if the model is true.\n\n\n\n\nAIC (Akaike Information Criterion)\n\ncorrected residual deviance (to compare models with different N. of predictors) - the smaller the better\n\nDeviance residuals\n\ncontribution of each observation to the residual deviance.\n\n\n\nEvaluation"
  },
  {
    "objectID": "slides/logistic-regression.html#predicted-probability",
    "href": "slides/logistic-regression.html#predicted-probability",
    "title": "Logistic regression analysis",
    "section": "Predicted probability",
    "text": "Predicted probability\n\\[g(x) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...\\]\nLet’s predict the probability of going to when complement length is 4 and register is formal.\n\n\\[g(x) = 11.582 + -2.651\\times 4 + 3.284 \\times 1 = 4.26\\]\n\n\nodds = exp(4.26) = 70.802\n\n\n\\[P(x) = \\frac{70.802}{70.802 + 1} = 0.986\\]\n\nEvaluation"
  },
  {
    "objectID": "slides/logistic-regression.html#measures-of-predictive-power",
    "href": "slides/logistic-regression.html#measures-of-predictive-power",
    "title": "Logistic regression analysis",
    "section": "Measures of predictive power",
    "text": "Measures of predictive power\n\n\nWe take all pairs of observations in which the response variable is different (one of gonna, one of going to).\n\ngt0 is what the model predicted for the “gonna” element.\ngt1 is what the model predicted for the “going to” element.\n\n\n\n\nCode\nset.seed(0)\npred_power <- tibble(\n  gt0 = sample(filter(gt, variant == \"gonna\")$fit2, 15, replace = FALSE),\n  gt1 = sample(filter(gt, variant == \"going_to\")$fit2, 15, replace = FALSE))\nkb_style(pred_power, n = 15)\n\n\n\n\n \n  \n    gt0 \n    gt1 \n  \n \n\n  \n    0.011 \n    0.999 \n  \n  \n    0.221 \n    1.000 \n  \n  \n    0.009 \n    0.999 \n  \n  \n    0.050 \n    1.000 \n  \n  \n    0.021 \n    0.895 \n  \n  \n    0.001 \n    1.000 \n  \n  \n    0.793 \n    1.000 \n  \n  \n    0.006 \n    0.942 \n  \n  \n    0.090 \n    0.998 \n  \n  \n    0.265 \n    1.000 \n  \n  \n    0.006 \n    0.514 \n  \n  \n    0.019 \n    1.000 \n  \n  \n    0.174 \n    0.093 \n  \n  \n    0.111 \n    0.420 \n  \n  \n    0.003 \n    1.000 \n  \n\n\n\n\n\n\n\n\nEvaluation"
  },
  {
    "objectID": "slides/logistic-regression.html#c-d-t",
    "href": "slides/logistic-regression.html#c-d-t",
    "title": "Logistic regression analysis",
    "section": "C, D, T",
    "text": "C, D, T\n\n\n\nC = concordant pair = prediction is higher for “going to” 😁\nD = discordant pair = the prediction is higher for “gonna” ☹️\nT = tie = the prediction is the same for both 😐\n\n\n\n\nCode\npred_power <- pred_power %>%\n  mutate(C = gt1 > gt0, D = gt0 > gt1, T = gt1 == gt0)\nkb_style(pred_power, n = 15) %>%\n  column_spec(3, background = spec_color(pred_power$C, option = \"H\", begin = 0.8, end = 0.45))\n\n\n\n\n \n  \n    gt0 \n    gt1 \n    C \n    D \n    T \n  \n \n\n  \n    0.011 \n    0.999 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.221 \n    1.000 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.009 \n    0.999 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.050 \n    1.000 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.021 \n    0.895 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.001 \n    1.000 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.793 \n    1.000 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.006 \n    0.942 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.090 \n    0.998 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.265 \n    1.000 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.006 \n    0.514 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.019 \n    1.000 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.174 \n    0.093 \n    FALSE \n    TRUE \n    FALSE \n  \n  \n    0.111 \n    0.420 \n    TRUE \n    FALSE \n    FALSE \n  \n  \n    0.003 \n    1.000 \n    TRUE \n    FALSE \n    FALSE \n  \n\n\n\n\n\n\n\n\nEvaluation"
  },
  {
    "objectID": "slides/logistic-regression.html#c-value",
    "href": "slides/logistic-regression.html#c-value",
    "title": "Logistic regression analysis",
    "section": "C-value",
    "text": "C-value\n\n\nCode\npred_power <- pred_power %>%\n  summarize(C = sum(C), D = sum(D), T = sum(T))\nkb_style(pred_power)\n\n\n\n\n \n  \n    C \n    D \n    T \n  \n \n\n  \n    14 \n    1 \n    0 \n  \n\n\n\n\n\n\n\\[c = \\frac{C+T/2}{C+D+T} = \\frac{14+0/2}{14+1+0} = 0.933\\]\n\n\n\nHmisc::somers2(gt$fit2, gt$variant_num)[[\"C\"]]\n\n[1] 0.984\n\n\n\nEvaluation"
  },
  {
    "objectID": "slides/logistic-regression.html#linear-vs-logistic-regression",
    "href": "slides/logistic-regression.html#linear-vs-logistic-regression",
    "title": "Logistic regression analysis",
    "section": "Linear vs logistic regression",
    "text": "Linear vs logistic regression\n\n\n\n\n\n\n\n\n\nLinear regression\nLogistic regression\n\n\n\n\nResponse variable\nNumerical\nCategorical\n\n\nRelationship between predictor estimate and response\nLinear\nLogistic1\n\n\nFitting function\nOSL (ordinary least squares)\nMLE (maximum likelihood estimation)\n\n\nModel comparison\nF-test\nAIC\n\n\nEvaluation metric\n\\(R^2\\)\n\\(C\\)\n\n\nBase R function\nlm()\nglm()\n\n\n\n\nEvaluation\n\nLinear relationship between estimate and log odds of response."
  },
  {
    "objectID": "slides/logistic-regression.html#what-is-an-interaction",
    "href": "slides/logistic-regression.html#what-is-an-interaction",
    "title": "Logistic regression analysis",
    "section": "What is an interaction?",
    "text": "What is an interaction?\n\nWhen the effect of a predictor depends on the value of another predictor.\n\nFormula: y ~ x1 + x2 + x1:x2 = y ~ x1*x2\n\nInteractions"
  },
  {
    "objectID": "slides/logistic-regression.html#examples",
    "href": "slides/logistic-regression.html#examples",
    "title": "Logistic regression analysis",
    "section": "Examples",
    "text": "Examples\nPossible interactions of complement length and register (is the text formal or informal?) on choice of gonna.\n\nThe effect of complement length is also positive/negative but stronger/weaker when texts are informal.\nThe effect of complement length is positive/negative when texts are informal, reversed when texts are formal.\n\n\nInteractions"
  },
  {
    "objectID": "slides/logistic-regression.html#model-with-interactions",
    "href": "slides/logistic-regression.html#model-with-interactions",
    "title": "Logistic regression analysis",
    "section": "Model with interactions",
    "text": "Model with interactions\n\n\n\nm3 <- glm(variant ~ comp_length*register, data = gt, family = binomial(logit))\nsummary(m3)\n\n\nCall:\nglm(formula = variant ~ comp_length * register, family = binomial(logit), \n    data = gt)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5689  -0.0792  -0.0030   0.1584   1.8033  \n\nCoefficients:\n                           Estimate Std. Error z value Pr(>|z|)   \n(Intercept)                   18.78       7.10    2.64   0.0082 **\ncomp_length                   -4.22       1.55   -2.72   0.0065 **\nregisterformal                -8.39       7.89   -1.06   0.2878   \ncomp_length:registerformal     2.39       1.67    1.43   0.1529   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 138.629  on 99  degrees of freedom\nResidual deviance:  31.103  on 96  degrees of freedom\nAIC: 39.1\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nInterpretation\n\nIntercept is interpreted in the same way.\ncomp_length is the effect of complement length in informal register.\nregisterformal is the effect of the formal register at 0 complement length.\ncomp_length:registerformal is the difference in the effect of complement length in formal register, compared to informal register.\n\n\n\n\nInteractions"
  },
  {
    "objectID": "slides/logistic-regression.html#visualization",
    "href": "slides/logistic-regression.html#visualization",
    "title": "Logistic regression analysis",
    "section": "Visualization",
    "text": "Visualization\n\n\nCode\nlibrary(ggeffects)\nplot(ggpredict(m3, c(\"comp_length\", \"register\")))\n\n\n\n\nInteractions"
  },
  {
    "objectID": "slides/logistic-regression.html#fixed-vs-random-effects",
    "href": "slides/logistic-regression.html#fixed-vs-random-effects",
    "title": "Logistic regression analysis",
    "section": "Fixed vs random effects",
    "text": "Fixed vs random effects\nUnlike fixed effects (what we have dealt with up to now), random effects:\n\nMay not be replicable in a different dataset.\nInvolve a sample of the possible values in the population.\n\nExamples: speaker, lexical effects, topic… → often operationalized as filename.\n\nRandom effects"
  },
  {
    "objectID": "slides/logistic-regression.html#random-intercept",
    "href": "slides/logistic-regression.html#random-intercept",
    "title": "Logistic regression analysis",
    "section": "Random intercept",
    "text": "Random intercept\n\nThe intercept can vary.\n\n\n\nDifferent individuals prefer gonna over going to regardless of complement length.\nCertain children always produce longer utterances than others of the same age.\n\n\n\n\n\n\n\n\n\nNested random intercept\n\n\nE.g. random intercept for filename and random intercept for register; each filename occurs in only one register.\n\n\n\n\nRandom effects"
  },
  {
    "objectID": "slides/logistic-regression.html#random-slope",
    "href": "slides/logistic-regression.html#random-slope",
    "title": "Logistic regression analysis",
    "section": "Random slope",
    "text": "Random slope\n\nThe slope of a fixed effect can vary.\n\nA given predictor can have different effects in different texts.\n\n\nComplement length is more influential in the choice of gonna for some individuals than for others.\nFor certain children, utterance length increases with age at a higher pace than of others.\n\n\nRandom effects"
  },
  {
    "objectID": "slides/logistic-regression.html#how-to",
    "href": "slides/logistic-regression.html#how-to",
    "title": "Logistic regression analysis",
    "section": "How to",
    "text": "How to\n\nlme4::glmer() instead of glm(); lme4::lmer() instead of lm().\n\nThe control argument helps us deal with convergence issues.\n\n\n\n\nExtension of the R formula:\n\n(1 | filename) for filename as random effect.\n(age | child) for child as a random slope for age.\n\n\n\nRandom effects"
  },
  {
    "objectID": "slides/logistic-regression.html#model-with-random-effects",
    "href": "slides/logistic-regression.html#model-with-random-effects",
    "title": "Logistic regression analysis",
    "section": "Model with random effects",
    "text": "Model with random effects\n\nm4 <- lme4::glmer(variant ~ comp_length*register + (1 | source), data = gt, family = binomial)\nsummary(m4)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: variant ~ comp_length * register + (1 | source)\n   Data: gt\n\n     AIC      BIC   logLik deviance df.resid \n    40.7     53.7    -15.3     30.7       95 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.041 -0.031 -0.001  0.063  1.343 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n source (Intercept) 1.6      1.26    \nNumber of obs: 100, groups:  source, 26\n\nFixed effects:\n                           Estimate Std. Error z value Pr(>|z|)  \n(Intercept)                   23.19      11.72    1.98    0.048 *\ncomp_length                   -5.18       2.56   -2.03    0.043 *\nregisterformal               -10.30       9.77   -1.05    0.292  \ncomp_length:registerformal     2.94       2.18    1.35    0.177  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) cmp_ln rgstrf\ncomp_length -0.997              \nregistrfrml -0.829  0.827       \ncmp_lngth:r  0.890 -0.894 -0.985\n\n\n\nRandom effects"
  },
  {
    "objectID": "slides/logistic-regression.html#compare-models",
    "href": "slides/logistic-regression.html#compare-models",
    "title": "Logistic regression analysis",
    "section": "Compare models",
    "text": "Compare models\n\n\nCode\nperformance::compare_performance(m1, m2, m3, m4, metrics = \"AIC\")\n\n\n# Comparison of Model Performance Indices\n\nName |    Model | AIC (weights)\n-------------------------------\nm1   |      glm |  51.6 (<.001)\nm2   |      glm |  40.0 (0.303)\nm3   |      glm |  39.1 (0.479)\nm4   | glmerMod |  40.7 (0.217)\n\n\n\nRandom effects"
  },
  {
    "objectID": "studies/base.html",
    "href": "studies/base.html",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "",
    "text": "This document illustrates a keyword analysis and a collocation analysis. They both use the BASE corpus and the same criteria for association strength.\nFor the analysis we’ll mainly use the tidyverse package and the mclm package. We’ll also use here to find the corpus and, for the report itself when printing tables, kableExtra.\n\nlibrary(tidyverse)\nlibrary(mclm)\nlibrary(here)\nlibrary(kableExtra) # for the report\noptions(digits = 3) # to print up to 3 digits\n\n\nBoth of the studies rely on the assoc_scores objects of the mclm package, which return frequencies and association scores, i.e. measures based on the frequency of a certain event in a target context and in a reference context. For keyword analysis, the target context is a target (sub)corpus and the reference context is a reference (sub)corpus. For collocation analysis based on surface co-occurrences, the target context is the text surrounding the occurrences of the node term and the reference context is all the other text in the corpus.\nIn order to define keywords (for the first study) or collocations (for the second study) we will filter the output of assoc_scores() based on the following three criteria:\n\nA frequency of three or higher in the target context.\nA PMI score of two or higher.\nA signed G score of two or higher.\n\nA PMI score of two or higher means that the probability of the keyword in the target context is at least four times higher than its probability in all the data taken together. Incidentally: a PMI of one indicates that this probability is twice as high, a PMI of two indicates it is four times as high, a PMI of three indicates it is eight times as high, a PMI of four indicates it is sixteen times as high, etc.\nRemember that, as a strength of evidence measure, \\(G^2\\) is higher when there is more evidence that two words are not independent, but it does not distinguish between attraction and repulsion. Therefore, a signed G score as returned by assoc_scores() is a modification that adds a minus to \\(G^2\\) when the observed frequency of the event is lower than its expected frequency. A threshold of two or higher is not a very strict criterion. With a signed \\(G^2\\) of 2 there is only mild evidence for attraction. In a traditional \\(G\\) test (log likelihood ratio test) for a two-by-two contingency table, a \\(G^2\\) score of 1 would indicate no evidence for any association whatsoever and a \\(G^2\\) score of 3.84 would be needed for the test to indicate a significant association (at a 95% confidence level). So a value of 2 would indicate that there is not enough evidence to establish significance.\n\n\n\n\n\n\nSelection criteria\n\n\n\nIn short, the selection criteria that were chosen here value both the effect size and amount of evidence, but could be said to be more demanding with respect to the former than with respect to the latter. This choice is acceptable, as long as the researcher is aware to be prioritizing (to some extent) effect size over amount of evidence.\n\n\n\nThe first step to analyzing the data is to read the corpus. The first line of the code below sets the path to the “BASE” directory where all the corpus files are stored, in this case inside a “_corpora” folder, inside a “studies” folder at the top level of the project. In order to run this in your own computer, set corpus_folder to the path to where your copy of the corpus is stored (inside your project).\nThe second line of the code collects all the file names in that folder in an fnames object and keeps those with the “txt” extension.\nThe hide_path argument in the print() method for an fnames object allows us to hide a (redundant) bit of the filenames when printing them.\n\ncorpus_folder <- here(\"studies\", \"_corpora\", \"BASE\")\nfnames_BASE <- get_fnames(corpus_folder) %>% \n  keep_re(\"[.]txt\")\n\nprint(fnames_BASE, 10, hide_path = corpus_folder)\n\nFilename collection of length 198\n              filename\n   -------------------\n 1 txt/ah/ahlct001.txt\n 2 txt/ah/ahlct002.txt\n 3 txt/ah/ahlct003.txt\n 4 txt/ah/ahlct004.txt\n 5 txt/ah/ahlct005.txt\n 6 txt/ah/ahlct006.txt\n 7 txt/ah/ahlct007.txt\n 8 txt/ah/ahlct008.txt\n 9 txt/ah/ahlct009.txt\n10 txt/ah/ahlct010.txt\n...\n\n\nThe functions to actually read the corpus for the analyses will be freqlist() and surf_cooc(). In both cases we’ll use three non-default settings that are more appropriate for the format of the BASE corpus.\n\nIn re_token_splitter we use the regular expression \\s+; in other words, we treat all chunks of whitespace as token separators. We do this because the default tokenizer, which roughly identifies the chunks of alphanumeric characters as tokens, would e.g. cut up the corpus snippet a simple [0.4] example into the tokens a, simple, 0, 4, and example, which is not what we want. The tokenizer we do use would cut the same snippet up in the tokens a, simple, [0.4], and example. This is still not exactly what we want, but see the next point.\nIn re_drop_token we use [:\\[\\]] in order to drop all the tokens that match this regular expression; in other words we drop all tokens that contain either a colon, an opening square bracket or a closing square bracket. So, in the aforementioned example, the pseudo-token [0.4], which actually is a pause indication, would be dropped eventually. Tokens that contain a colon are also dropped, because those are speaker identifiers in the BASE corpus, not real tokens.\nIn file_encoding we specify windows-1252, which indeed is the encoding used in the BASE corpus.\n\nThe main function we will use is assoc_scores(), which creates an object of class assoc_scores, i.e. a special kind of dataframe with association scores information. In the case of keyword analysis (Section 2) we’ll run it with two frequency lists created from different subcorpora, whereas for collocation analysis (Section 3) we’ll provide a cooc_info object created with surf_cooc().\nBy default, assoc_scores() will not return values where the frequency in the target context was lower than 3, so we don’t need to do anything else to define our first criterion. For the other two criteria, instead, we’ll need to filter the assoc_scores object to only retain elements with a high enough PMI and G signed. Section 4 will illustrates steps to follow that are common to both workflows."
  },
  {
    "objectID": "studies/base.html#frequency-lists",
    "href": "studies/base.html#frequency-lists",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n2.1 Frequency lists",
    "text": "2.1 Frequency lists\nNext, we build the frequency lists, both for the target corpus and for the reference corpus. The former we store in a variable flist_target and the latter in a variable flist_ref.\n\n\nIn both cases we’ll use raw strings for the regular expressions, although they are a bit of an overkill with such simple expressions. We do it out of principle, to get used to their syntax.\n\n# build frequency list for target corpus\nflist_target <- fnames_target %>%\n  freqlist(re_token_splitter = r\"--[(?xi)    \\s+   ]--\", # whitespace as token splitter\n           re_drop_token     = r\"--[(?xi)  [:\\[\\]] ]--\", # drop tokens with :, [ or ]\n           file_encoding     = \"windows-1252\") %>%\n  print()\n# build frequency list for reference corpus\nflist_ref <- fnames_ref %>%\n  freqlist(re_token_splitter = r\"--[(?xi)  \\s+   ]--\",\n           re_drop_token     = r\"--[(?xi)  [:\\[\\]] ]--\",\n           file_encoding     = \"windows-1252\") %>%\n  print()\n\n\n\nFrequency list (types in list: 2528, tokens in list: 10361)\nrank type abs_freq nrm_freq\n---- ---- -------- --------\n   1  the      595    574.3\n   2   of      336    324.3\n   3  and      319    307.9\n   4    a      277    267.3\n   5   to      248    239.4\n   6   in      216    208.5\n   7   er      175    168.9\n   8    i      168    162.1\n   9  you      118    113.9\n  10  her      106    102.3\n  11   he      102     98.4\n  12  was      102     98.4\n  13   is       96     92.7\n  14  she       81     78.2\n  15 that       81     78.2\n  16   it       78     75.3\n  17 with       71     68.5\n  18   as       64     61.8\n  19  for       64     61.8\n  20  his       64     61.8\n...\n\n\nFrequency list (types in list: 36491, tokens in list: 1614252)\nrank type abs_freq nrm_freq\n---- ---- -------- --------\n   1  the    86625    536.6\n   2   of    48929    303.1\n   3  and    44915    278.2\n   4   to    42614    264.0\n   5   er    39352    243.8\n   6    a    36154    224.0\n   7 that    31807    197.0\n   8   in    30177    186.9\n   9  you    29426    182.3\n  10   is    25986    161.0\n  11   it    20365    126.2\n  12   so    16849    104.4\n  13    i    16757    103.8\n  14 this    14907     92.3\n  15   we    13097     81.1\n  16 have    10326     64.0\n  17 what    10261     63.6\n  18   on    10183     63.1\n  19   be    10177     63.0\n  20  but    10134     62.8\n...\n\n\n\n\nWe can turn freqlist objects into tibbles and print them nicely with kableExtra. When the output is HTML, we can also print the table in a scrollable box, like in Table 1.\n\nflist_target %>% \n  as_tibble() %>% \n  kbl(col.names = c(\"Rank\", \"Type\", \"Absolute\", \"Relative\")) %>% \n  kable_minimal(full_width = FALSE) %>% \n  add_header_above(c(\" \" = 2, \"Frequency\" = 2)) %>% \n  scroll_box(height = \"400px\")\n\n\n\n\nTable 1:  Frequency list of the target corpus. \n \n\n\nFrequency\n\n\n Rank \n    Type \n    Absolute \n    Relative \n  \n\n\n\n 1 \n    the \n    595 \n    574.269 \n  \n\n 2 \n    of \n    336 \n    324.293 \n  \n\n 3 \n    and \n    319 \n    307.885 \n  \n\n 4 \n    a \n    277 \n    267.349 \n  \n\n 5 \n    to \n    248 \n    239.359 \n  \n\n 6 \n    in \n    216 \n    208.474 \n  \n\n 7 \n    er \n    175 \n    168.903 \n  \n\n 8 \n    i \n    168 \n    162.147 \n  \n\n 9 \n    you \n    118 \n    113.889 \n  \n\n 10 \n    her \n    106 \n    102.307 \n  \n\n 11 \n    he \n    102 \n    98.446 \n  \n\n 12 \n    was \n    102 \n    98.446 \n  \n\n 13 \n    is \n    96 \n    92.655 \n  \n\n 14 \n    she \n    81 \n    78.178 \n  \n\n 15 \n    that \n    81 \n    78.178 \n  \n\n 16 \n    it \n    78 \n    75.282 \n  \n\n 17 \n    with \n    71 \n    68.526 \n  \n\n 18 \n    as \n    64 \n    61.770 \n  \n\n 19 \n    for \n    64 \n    61.770 \n  \n\n 20 \n    his \n    64 \n    61.770 \n  \n\n 21 \n    my \n    62 \n    59.840 \n  \n\n 22 \n    this \n    61 \n    58.875 \n  \n\n 23 \n    at \n    59 \n    56.944 \n  \n\n 24 \n    from \n    58 \n    55.979 \n  \n\n 25 \n    we \n    58 \n    55.979 \n  \n\n 26 \n    about \n    56 \n    54.049 \n  \n\n 27 \n    so \n    56 \n    54.049 \n  \n\n 28 \n    by \n    53 \n    51.153 \n  \n\n 29 \n    it's \n    48 \n    46.328 \n  \n\n 30 \n    on \n    47 \n    45.362 \n  \n\n 31 \n    but \n    46 \n    44.397 \n  \n\n 32 \n    know \n    46 \n    44.397 \n  \n\n 33 \n    up \n    44 \n    42.467 \n  \n\n 34 \n    when \n    44 \n    42.467 \n  \n\n 35 \n    have \n    37 \n    35.711 \n  \n\n 36 \n    all \n    36 \n    34.746 \n  \n\n 37 \n    him \n    36 \n    34.746 \n  \n\n 38 \n    like \n    35 \n    33.781 \n  \n\n 39 \n    me \n    34 \n    32.815 \n  \n\n 40 \n    they \n    34 \n    32.815 \n  \n\n 41 \n    or \n    32 \n    30.885 \n  \n\n 42 \n    what \n    32 \n    30.885 \n  \n\n 43 \n    which \n    32 \n    30.885 \n  \n\n 44 \n    no \n    31 \n    29.920 \n  \n\n 45 \n    not \n    30 \n    28.955 \n  \n\n 46 \n    had \n    29 \n    27.990 \n  \n\n 47 \n    their \n    29 \n    27.990 \n  \n\n 48 \n    who \n    29 \n    27.990 \n  \n\n 49 \n    into \n    28 \n    27.024 \n  \n\n 50 \n    sea \n    28 \n    27.024 \n  \n\n 51 \n    be \n    27 \n    26.059 \n  \n\n 52 \n    do \n    27 \n    26.059 \n  \n\n 53 \n    kind \n    27 \n    26.059 \n  \n\n 54 \n    out \n    26 \n    25.094 \n  \n\n 55 \n    write \n    26 \n    25.094 \n  \n\n 56 \n    our \n    25 \n    24.129 \n  \n\n 57 \n    right \n    25 \n    24.129 \n  \n\n 58 \n    slavery \n    24 \n    23.164 \n  \n\n 59 \n    them \n    24 \n    23.164 \n  \n\n 60 \n    will \n    24 \n    23.164 \n  \n\n 61 \n    an \n    23 \n    22.199 \n  \n\n 62 \n    more \n    23 \n    22.199 \n  \n\n 63 \n    yeah \n    23 \n    22.199 \n  \n\n 64 \n    could \n    22 \n    21.233 \n  \n\n 65 \n    very \n    22 \n    21.233 \n  \n\n 66 \n    because \n    21 \n    20.268 \n  \n\n 67 \n    called \n    21 \n    20.268 \n  \n\n 68 \n    where \n    21 \n    20.268 \n  \n\n 69 \n    would \n    21 \n    20.268 \n  \n\n 70 \n    are \n    20 \n    19.303 \n  \n\n 71 \n    man \n    20 \n    19.303 \n  \n\n 72 \n    only \n    20 \n    19.303 \n  \n\n 73 \n    some \n    20 \n    19.303 \n  \n\n 74 \n    then \n    20 \n    19.303 \n  \n\n 75 \n    dead \n    19 \n    18.338 \n  \n\n 76 \n    each \n    19 \n    18.338 \n  \n\n 77 \n    now \n    19 \n    18.338 \n  \n\n 78 \n    us \n    19 \n    18.338 \n  \n\n 79 \n    way \n    19 \n    18.338 \n  \n\n 80 \n    has \n    18 \n    17.373 \n  \n\n 81 \n    land \n    18 \n    17.373 \n  \n\n 82 \n    there \n    18 \n    17.373 \n  \n\n 83 \n    white \n    18 \n    17.373 \n  \n\n 84 \n    you're \n    18 \n    17.373 \n  \n\n 85 \n    first \n    17 \n    16.408 \n  \n\n 86 \n    if \n    17 \n    16.408 \n  \n\n 87 \n    one \n    17 \n    16.408 \n  \n\n 88 \n    turner \n    17 \n    16.408 \n  \n\n 89 \n    another \n    16 \n    15.443 \n  \n\n 90 \n    before \n    16 \n    15.443 \n  \n\n 91 \n    down \n    16 \n    15.443 \n  \n\n 92 \n    just \n    16 \n    15.443 \n  \n\n 93 \n    through \n    16 \n    15.443 \n  \n\n 94 \n    african \n    15 \n    14.477 \n  \n\n 95 \n    come \n    15 \n    14.477 \n  \n\n 96 \n    great \n    15 \n    14.477 \n  \n\n 97 \n    manu \n    15 \n    14.477 \n  \n\n 98 \n    poem \n    15 \n    14.477 \n  \n\n 99 \n    shop \n    15 \n    14.477 \n  \n\n 100 \n    even \n    14 \n    13.512 \n  \n\n 101 \n    face \n    14 \n    13.512 \n  \n\n 102 \n    he's \n    14 \n    13.512 \n  \n\n 103 \n    say \n    14 \n    13.512 \n  \n\n 104 \n    wanted \n    14 \n    13.512 \n  \n\n 105 \n    can't \n    13 \n    12.547 \n  \n\n 106 \n    fish \n    13 \n    12.547 \n  \n\n 107 \n    gladstone \n    13 \n    12.547 \n  \n\n 108 \n    men \n    13 \n    12.547 \n  \n\n 109 \n    mouth \n    13 \n    12.547 \n  \n\n 110 \n    over \n    13 \n    12.547 \n  \n\n 111 \n    than \n    13 \n    12.547 \n  \n\n 112 \n    your \n    13 \n    12.547 \n  \n\n 113 \n    after \n    12 \n    11.582 \n  \n\n 114 \n    can \n    12 \n    11.582 \n  \n\n 115 \n    going \n    12 \n    11.582 \n  \n\n 116 \n    hand \n    12 \n    11.582 \n  \n\n 117 \n    other \n    12 \n    11.582 \n  \n\n 118 \n    think \n    12 \n    11.582 \n  \n\n 119 \n    time \n    12 \n    11.582 \n  \n\n 120 \n    were \n    12 \n    11.582 \n  \n\n 121 \n    writing \n    12 \n    11.582 \n  \n\n 122 \n    don't \n    11 \n    10.617 \n  \n\n 123 \n    go \n    11 \n    10.617 \n  \n\n 124 \n    language \n    11 \n    10.617 \n  \n\n 125 \n    love \n    11 \n    10.617 \n  \n\n 126 \n    nigger \n    11 \n    10.617 \n  \n\n 127 \n    novel \n    11 \n    10.617 \n  \n\n 128 \n    off \n    11 \n    10.617 \n  \n\n 129 \n    painting \n    11 \n    10.617 \n  \n\n 130 \n    sense \n    11 \n    10.617 \n  \n\n 131 \n    something \n    11 \n    10.617 \n  \n\n 132 \n    well \n    11 \n    10.617 \n  \n\n 133 \n    get \n    10 \n    9.652 \n  \n\n 134 \n    passage \n    10 \n    9.652 \n  \n\n 135 \n    people \n    10 \n    9.652 \n  \n\n 136 \n    really \n    10 \n    9.652 \n  \n\n 137 \n    rohini \n    10 \n    9.652 \n  \n\n 138 \n    see \n    10 \n    9.652 \n  \n\n 139 \n    sex \n    10 \n    9.652 \n  \n\n 140 \n    shah \n    10 \n    9.652 \n  \n\n 141 \n    still \n    10 \n    9.652 \n  \n\n 142 \n    words \n    10 \n    9.652 \n  \n\n 143 \n    years \n    10 \n    9.652 \n  \n\n 144 \n    back \n    9 \n    8.686 \n  \n\n 145 \n    being \n    9 \n    8.686 \n  \n\n 146 \n    black \n    9 \n    8.686 \n  \n\n 147 \n    century \n    9 \n    8.686 \n  \n\n 148 \n    child \n    9 \n    8.686 \n  \n\n 149 \n    did \n    9 \n    8.686 \n  \n\n 150 \n    end \n    9 \n    8.686 \n  \n\n 151 \n    guyana \n    9 \n    8.686 \n  \n\n 152 \n    here \n    9 \n    8.686 \n  \n\n 153 \n    reading \n    9 \n    8.686 \n  \n\n 154 \n    said \n    9 \n    8.686 \n  \n\n 155 \n    should \n    9 \n    8.686 \n  \n\n 156 \n    that's \n    9 \n    8.686 \n  \n\n 157 \n    there's \n    9 \n    8.686 \n  \n\n 158 \n    thought \n    9 \n    8.686 \n  \n\n 159 \n    troilus \n    9 \n    8.686 \n  \n\n 160 \n    two \n    9 \n    8.686 \n  \n\n 161 \n    until \n    9 \n    8.686 \n  \n\n 162 \n    whole \n    9 \n    8.686 \n  \n\n 163 \n    yes \n    9 \n    8.686 \n  \n\n 164 \n    against \n    8 \n    7.721 \n  \n\n 165 \n    around \n    8 \n    7.721 \n  \n\n 166 \n    behind \n    8 \n    7.721 \n  \n\n 167 \n    felt \n    8 \n    7.721 \n  \n\n 168 \n    hands \n    8 \n    7.721 \n  \n\n 169 \n    himself \n    8 \n    7.721 \n  \n\n 170 \n    i'm \n    8 \n    7.721 \n  \n\n 171 \n    its \n    8 \n    7.721 \n  \n\n 172 \n    most \n    8 \n    7.721 \n  \n\n 173 \n    mother \n    8 \n    7.721 \n  \n\n 174 \n    old \n    8 \n    7.721 \n  \n\n 175 \n    own \n    8 \n    7.721 \n  \n\n 176 \n    read \n    8 \n    7.721 \n  \n\n 177 \n    these \n    8 \n    7.721 \n  \n\n 178 \n    too \n    8 \n    7.721 \n  \n\n 179 \n    took \n    8 \n    7.721 \n  \n\n 180 \n    why \n    8 \n    7.721 \n  \n\n 181 \n    woman \n    8 \n    7.721 \n  \n\n 182 \n    work \n    8 \n    7.721 \n  \n\n 183 \n    awakened \n    7 \n    6.756 \n  \n\n 184 \n    been \n    7 \n    6.756 \n  \n\n 185 \n    between \n    7 \n    6.756 \n  \n\n 186 \n    blood \n    7 \n    6.756 \n  \n\n 187 \n    body \n    7 \n    6.756 \n  \n\n 188 \n    came \n    7 \n    6.756 \n  \n\n 189 \n    caught \n    7 \n    6.756 \n  \n\n 190 \n    earth \n    7 \n    6.756 \n  \n\n 191 \n    got \n    7 \n    6.756 \n  \n\n 192 \n    head \n    7 \n    6.756 \n  \n\n 193 \n    home \n    7 \n    6.756 \n  \n\n 194 \n    latin \n    7 \n    6.756 \n  \n\n 195 \n    left \n    7 \n    6.756 \n  \n\n 196 \n    live \n    7 \n    6.756 \n  \n\n 197 \n    many \n    7 \n    6.756 \n  \n\n 198 \n    mean \n    7 \n    6.756 \n  \n\n 199 \n    much \n    7 \n    6.756 \n  \n\n 200 \n    myself \n    7 \n    6.756 \n  \n\n 201 \n    reached \n    7 \n    6.756 \n  \n\n 202 \n    saw \n    7 \n    6.756 \n  \n\n 203 \n    ship \n    7 \n    6.756 \n  \n\n 204 \n    space \n    7 \n    6.756 \n  \n\n 205 \n    things \n    7 \n    6.756 \n  \n\n 206 \n    thistlewood \n    7 \n    6.756 \n  \n\n 207 \n    used \n    7 \n    6.756 \n  \n\n 208 \n    village \n    7 \n    6.756 \n  \n\n 209 \n    we're \n    7 \n    6.756 \n  \n\n 210 \n    again \n    6 \n    5.791 \n  \n\n 211 \n    always \n    6 \n    5.791 \n  \n\n 212 \n    anyway \n    6 \n    5.791 \n  \n\n 213 \n    away \n    6 \n    5.791 \n  \n\n 214 \n    beauty \n    6 \n    5.791 \n  \n\n 215 \n    big \n    6 \n    5.791 \n  \n\n 216 \n    book \n    6 \n    5.791 \n  \n\n 217 \n    breath \n    6 \n    5.791 \n  \n\n 218 \n    british \n    6 \n    5.791 \n  \n\n 219 \n    centre \n    6 \n    5.791 \n  \n\n 220 \n    classics \n    6 \n    5.791 \n  \n\n 221 \n    desire \n    6 \n    5.791 \n  \n\n 222 \n    eighteenth \n    6 \n    5.791 \n  \n\n 223 \n    english \n    6 \n    5.791 \n  \n\n 224 \n    estate \n    6 \n    5.791 \n  \n\n 225 \n    eyes \n    6 \n    5.791 \n  \n\n 226 \n    family \n    6 \n    5.791 \n  \n\n 227 \n    god \n    6 \n    5.791 \n  \n\n 228 \n    guyanese \n    6 \n    5.791 \n  \n\n 229 \n    house \n    6 \n    5.791 \n  \n\n 230 \n    how \n    6 \n    5.791 \n  \n\n 231 \n    idea \n    6 \n    5.791 \n  \n\n 232 \n    instead \n    6 \n    5.791 \n  \n\n 233 \n    last \n    6 \n    5.791 \n  \n\n 234 \n    life \n    6 \n    5.791 \n  \n\n 235 \n    look \n    6 \n    5.791 \n  \n\n 236 \n    looked \n    6 \n    5.791 \n  \n\n 237 \n    made \n    6 \n    5.791 \n  \n\n 238 \n    makes \n    6 \n    5.791 \n  \n\n 239 \n    miriam \n    6 \n    5.791 \n  \n\n 240 \n    moon \n    6 \n    5.791 \n  \n\n 241 \n    namex \n    6 \n    5.791 \n  \n\n 242 \n    new \n    6 \n    5.791 \n  \n\n 243 \n    night \n    6 \n    5.791 \n  \n\n 244 \n    once \n    6 \n    5.791 \n  \n\n 245 \n    place \n    6 \n    5.791 \n  \n\n 246 \n    pounds \n    6 \n    5.791 \n  \n\n 247 \n    stillborn \n    6 \n    5.791 \n  \n\n 248 \n    utterly \n    6 \n    5.791 \n  \n\n 249 \n    want \n    6 \n    5.791 \n  \n\n 250 \n    water \n    6 \n    5.791 \n  \n\n 251 \n    went \n    6 \n    5.791 \n  \n\n 252 \n    without \n    6 \n    5.791 \n  \n\n 253 \n    women \n    6 \n    5.791 \n  \n\n 254 \n    wrote \n    6 \n    5.791 \n  \n\n 255 \n    'cause \n    5 \n    4.826 \n  \n\n 256 \n    abandoned \n    5 \n    4.826 \n  \n\n 257 \n    across \n    5 \n    4.826 \n  \n\n 258 \n    among \n    5 \n    4.826 \n  \n\n 259 \n    ancestry \n    5 \n    4.826 \n  \n\n 260 \n    animals \n    5 \n    4.826 \n  \n\n 261 \n    becomes \n    5 \n    4.826 \n  \n\n 262 \n    boy \n    5 \n    4.826 \n  \n\n 263 \n    bush \n    5 \n    4.826 \n  \n\n 264 \n    cabin \n    5 \n    4.826 \n  \n\n 265 \n    captain \n    5 \n    4.826 \n  \n\n 266 \n    colours \n    5 \n    4.826 \n  \n\n 267 \n    curtain \n    5 \n    4.826 \n  \n\n 268 \n    day \n    5 \n    4.826 \n  \n\n 269 \n    different \n    5 \n    4.826 \n  \n\n 270 \n    doing \n    5 \n    4.826 \n  \n\n 271 \n    drowned \n    5 \n    4.826 \n  \n\n 272 \n    except \n    5 \n    4.826 \n  \n\n 273 \n    flesh \n    5 \n    4.826 \n  \n\n 274 \n    gave \n    5 \n    4.826 \n  \n\n 275 \n    imagination \n    5 \n    4.826 \n  \n\n 276 \n    indian \n    5 \n    4.826 \n  \n\n 277 \n    kampta \n    5 \n    4.826 \n  \n\n 278 \n    looking \n    5 \n    4.826 \n  \n\n 279 \n    magazine \n    5 \n    4.826 \n  \n\n 280 \n    marginal \n    5 \n    4.826 \n  \n\n 281 \n    matter \n    5 \n    4.826 \n  \n\n 282 \n    miles \n    5 \n    4.826 \n  \n\n 283 \n    nothing \n    5 \n    4.826 \n  \n\n 284 \n    pages \n    5 \n    4.826 \n  \n\n 285 \n    passages \n    5 \n    4.826 \n  \n\n 286 \n    peripheral \n    5 \n    4.826 \n  \n\n 287 \n    plantation \n    5 \n    4.826 \n  \n\n 288 \n    plot \n    5 \n    4.826 \n  \n\n 289 \n    put \n    5 \n    4.826 \n  \n\n 290 \n    rage \n    5 \n    4.826 \n  \n\n 291 \n    servant \n    5 \n    4.826 \n  \n\n 292 \n    set \n    5 \n    4.826 \n  \n\n 293 \n    skin \n    5 \n    4.826 \n  \n\n 294 \n    sky \n    5 \n    4.826 \n  \n\n 295 \n    slave \n    5 \n    4.826 \n  \n\n 296 \n    slipped \n    5 \n    4.826 \n  \n\n 297 \n    small \n    5 \n    4.826 \n  \n\n 298 \n    sometimes \n    5 \n    4.826 \n  \n\n 299 \n    source \n    5 \n    4.826 \n  \n\n 300 \n    stone \n    5 \n    4.826 \n  \n\n 301 \n    storm \n    5 \n    4.826 \n  \n\n 302 \n    story \n    5 \n    4.826 \n  \n\n 303 \n    subject \n    5 \n    4.826 \n  \n\n 304 \n    sun \n    5 \n    4.826 \n  \n\n 305 \n    terms \n    5 \n    4.826 \n  \n\n 306 \n    they're \n    5 \n    4.826 \n  \n\n 307 \n    today \n    5 \n    4.826 \n  \n\n 308 \n    towards \n    5 \n    4.826 \n  \n\n 309 \n    tribe \n    5 \n    4.826 \n  \n\n 310 \n    upon \n    5 \n    4.826 \n  \n\n 311 \n    whatever \n    5 \n    4.826 \n  \n\n 312 \n    within \n    5 \n    4.826 \n  \n\n 313 \n    actually \n    4 \n    3.861 \n  \n\n 314 \n    air \n    4 \n    3.861 \n  \n\n 315 \n    am \n    4 \n    3.861 \n  \n\n 316 \n    amazonian \n    4 \n    3.861 \n  \n\n 317 \n    anarch \n    4 \n    3.861 \n  \n\n 318 \n    aperture \n    4 \n    3.861 \n  \n\n 319 \n    art \n    4 \n    3.861 \n  \n\n 320 \n    asked \n    4 \n    3.861 \n  \n\n 321 \n    basically \n    4 \n    3.861 \n  \n\n 322 \n    bearing \n    4 \n    3.861 \n  \n\n 323 \n    beyond \n    4 \n    3.861 \n  \n\n 324 \n    break \n    4 \n    3.861 \n  \n\n 325 \n    caesar \n    4 \n    3.861 \n  \n\n 326 \n    call \n    4 \n    3.861 \n  \n\n 327 \n    case \n    4 \n    3.861 \n  \n\n 328 \n    coins \n    4 \n    3.861 \n  \n\n 329 \n    colour \n    4 \n    3.861 \n  \n\n 330 \n    coming \n    4 \n    3.861 \n  \n\n 331 \n    coolie \n    4 \n    3.861 \n  \n\n 332 \n    course \n    4 \n    3.861 \n  \n\n 333 \n    cries \n    4 \n    3.861 \n  \n\n 334 \n    criseyde \n    4 \n    3.861 \n  \n\n 335 \n    darkness \n    4 \n    3.861 \n  \n\n 336 \n    deep \n    4 \n    3.861 \n  \n\n 337 \n    didn't \n    4 \n    3.861 \n  \n\n 338 \n    ears \n    4 \n    3.861 \n  \n\n 339 \n    ellar \n    4 \n    3.861 \n  \n\n 340 \n    entered \n    4 \n    3.861 \n  \n\n 341 \n    fall \n    4 \n    3.861 \n  \n\n 342 \n    five \n    4 \n    3.861 \n  \n\n 343 \n    flooded \n    4 \n    3.861 \n  \n\n 344 \n    floor \n    4 \n    3.861 \n  \n\n 345 \n    future \n    4 \n    3.861 \n  \n\n 346 \n    good \n    4 \n    3.861 \n  \n\n 347 \n    grandpa \n    4 \n    3.861 \n  \n\n 348 \n    greatest \n    4 \n    3.861 \n  \n\n 349 \n    ground \n    4 \n    3.861 \n  \n\n 350 \n    i'll \n    4 \n    3.861 \n  \n\n 351 \n    i-, \n    4 \n    3.861 \n  \n\n 352 \n    jungle \n    4 \n    3.861 \n  \n\n 353 \n    kaka \n    4 \n    3.861 \n  \n\n 354 \n    knew \n    4 \n    3.861 \n  \n\n 355 \n    lachrimae \n    4 \n    3.861 \n  \n\n 356 \n    lay \n    4 \n    3.861 \n  \n\n 357 \n    let \n    4 \n    3.861 \n  \n\n 358 \n    lets \n    4 \n    3.861 \n  \n\n 359 \n    lies \n    4 \n    3.861 \n  \n\n 360 \n    little \n    4 \n    3.861 \n  \n\n 361 \n    long \n    4 \n    3.861 \n  \n\n 362 \n    longer \n    4 \n    3.861 \n  \n\n 363 \n    lot \n    4 \n    3.861 \n  \n\n 364 \n    might \n    4 \n    3.861 \n  \n\n 365 \n    mind \n    4 \n    3.861 \n  \n\n 366 \n    mine \n    4 \n    3.861 \n  \n\n 367 \n    must \n    4 \n    3.861 \n  \n\n 368 \n    never \n    4 \n    3.861 \n  \n\n 369 \n    nor \n    4 \n    3.861 \n  \n\n 370 \n    obviously \n    4 \n    3.861 \n  \n\n 371 \n    overboard \n    4 \n    3.861 \n  \n\n 372 \n    perhaps \n    4 \n    3.861 \n  \n\n 373 \n    periphery \n    4 \n    3.861 \n  \n\n 374 \n    pond \n    4 \n    3.861 \n  \n\n 375 \n    probably \n    4 \n    3.861 \n  \n\n 376 \n    rains \n    4 \n    3.861 \n  \n\n 377 \n    reach \n    4 \n    3.861 \n  \n\n 378 \n    s-, \n    4 \n    3.861 \n  \n\n 379 \n    slaves \n    4 \n    3.861 \n  \n\n 380 \n    someone \n    4 \n    3.861 \n  \n\n 381 \n    speak \n    4 \n    3.861 \n  \n\n 382 \n    speech \n    4 \n    3.861 \n  \n\n 383 \n    stars \n    4 \n    3.861 \n  \n\n 384 \n    strength \n    4 \n    3.861 \n  \n\n 385 \n    sudden \n    4 \n    3.861 \n  \n\n 386 \n    surface \n    4 \n    3.861 \n  \n\n 387 \n    tell \n    4 \n    3.861 \n  \n\n 388 \n    therefore \n    4 \n    3.861 \n  \n\n 389 \n    those \n    4 \n    3.861 \n  \n\n 390 \n    trying \n    4 \n    3.861 \n  \n\n 391 \n    turner's \n    4 \n    3.861 \n  \n\n 392 \n    use \n    4 \n    3.861 \n  \n\n 393 \n    waters \n    4 \n    3.861 \n  \n\n 394 \n    ways \n    4 \n    3.861 \n  \n\n 395 \n    we'll \n    4 \n    3.861 \n  \n\n 396 \n    who's \n    4 \n    3.861 \n  \n\n 397 \n    abandons \n    3 \n    2.895 \n  \n\n 398 \n    accompany \n    3 \n    2.895 \n  \n\n 399 \n    accountant \n    3 \n    2.895 \n  \n\n 400 \n    africa \n    3 \n    2.895 \n  \n\n 401 \n    afterwards \n    3 \n    2.895 \n  \n\n 402 \n    aggressive \n    3 \n    2.895 \n  \n\n 403 \n    almost \n    3 \n    2.895 \n  \n\n 404 \n    along \n    3 \n    2.895 \n  \n\n 405 \n    any \n    3 \n    2.895 \n  \n\n 406 \n    area \n    3 \n    2.895 \n  \n\n 407 \n    beatings \n    3 \n    2.895 \n  \n\n 408 \n    beautiful \n    3 \n    2.895 \n  \n\n 409 \n    begin \n    3 \n    2.895 \n  \n\n 410 \n    below \n    3 \n    2.895 \n  \n\n 411 \n    bleak \n    3 \n    2.895 \n  \n\n 412 \n    bloody \n    3 \n    2.895 \n  \n\n 413 \n    booths \n    3 \n    2.895 \n  \n\n 414 \n    boys \n    3 \n    2.895 \n  \n\n 415 \n    bright \n    3 \n    2.895 \n  \n\n 416 \n    brown \n    3 \n    2.895 \n  \n\n 417 \n    bury \n    3 \n    2.895 \n  \n\n 418 \n    caribbean \n    3 \n    2.895 \n  \n\n 419 \n    carries \n    3 \n    2.895 \n  \n\n 420 \n    catch \n    3 \n    2.895 \n  \n\n 421 \n    cemetery \n    3 \n    2.895 \n  \n\n 422 \n    close \n    3 \n    2.895 \n  \n\n 423 \n    closed \n    3 \n    2.895 \n  \n\n 424 \n    collapsed \n    3 \n    2.895 \n  \n\n 425 \n    coloured \n    3 \n    2.895 \n  \n\n 426 \n    counting \n    3 \n    2.895 \n  \n\n 427 \n    culture \n    3 \n    2.895 \n  \n\n 428 \n    curse \n    3 \n    2.895 \n  \n\n 429 \n    dangerous \n    3 \n    2.895 \n  \n\n 430 \n    desperation \n    3 \n    2.895 \n  \n\n 431 \n    die \n    3 \n    2.895 \n  \n\n 432 \n    diomede \n    3 \n    2.895 \n  \n\n 433 \n    direction \n    3 \n    2.895 \n  \n\n 434 \n    distance \n    3 \n    2.895 \n  \n\n 435 \n    dragged \n    3 \n    2.895 \n  \n\n 436 \n    dry \n    3 \n    2.895 \n  \n\n 437 \n    early \n    3 \n    2.895 \n  \n\n 438 \n    effort \n    3 \n    2.895 \n  \n\n 439 \n    eighteen-forty \n    3 \n    2.895 \n  \n\n 440 \n    ellar's \n    3 \n    2.895 \n  \n\n 441 \n    endlessly \n    3 \n    2.895 \n  \n\n 442 \n    england \n    3 \n    2.895 \n  \n\n 443 \n    enormously \n    3 \n    2.895 \n  \n\n 444 \n    enough \n    3 \n    2.895 \n  \n\n 445 \n    entering \n    3 \n    2.895 \n  \n\n 446 \n    examiner \n    3 \n    2.895 \n  \n\n 447 \n    eye \n    3 \n    2.895 \n  \n\n 448 \n    faces \n    3 \n    2.895 \n  \n\n 449 \n    fat \n    3 \n    2.895 \n  \n\n 450 \n    father \n    3 \n    2.895 \n  \n\n 451 \n    feet \n    3 \n    2.895 \n  \n\n 452 \n    few \n    3 \n    2.895 \n  \n\n 453 \n    fifty \n    3 \n    2.895 \n  \n\n 454 \n    footnotes \n    3 \n    2.895 \n  \n\n 455 \n    form \n    3 \n    2.895 \n  \n\n 456 \n    found \n    3 \n    2.895 \n  \n\n 457 \n    four \n    3 \n    2.895 \n  \n\n 458 \n    friend \n    3 \n    2.895 \n  \n\n 459 \n    full \n    3 \n    2.895 \n  \n\n 460 \n    gets \n    3 \n    2.895 \n  \n\n 461 \n    giving \n    3 \n    2.895 \n  \n\n 462 \n    gladstone's \n    3 \n    2.895 \n  \n\n 463 \n    gods \n    3 \n    2.895 \n  \n\n 464 \n    goes \n    3 \n    2.895 \n  \n\n 465 \n    grasp \n    3 \n    2.895 \n  \n\n 466 \n    grave \n    3 \n    2.895 \n  \n\n 467 \n    grotesque \n    3 \n    2.895 \n  \n\n 468 \n    grows \n    3 \n    2.895 \n  \n\n 469 \n    guidance \n    3 \n    2.895 \n  \n\n 470 \n    happened \n    3 \n    2.895 \n  \n\n 471 \n    hear \n    3 \n    2.895 \n  \n\n 472 \n    historic \n    3 \n    2.895 \n  \n\n 473 \n    history \n    3 \n    2.895 \n  \n\n 474 \n    hundred \n    3 \n    2.895 \n  \n\n 475 \n    i've \n    3 \n    2.895 \n  \n\n 476 \n    jamaican \n    3 \n    2.895 \n  \n\n 477 \n    killed \n    3 \n    2.895 \n  \n\n 478 \n    learn \n    3 \n    2.895 \n  \n\n 479 \n    learning \n    3 \n    2.895 \n  \n\n 480 \n    least \n    3 \n    2.895 \n  \n\n 481 \n    leave \n    3 \n    2.895 \n  \n\n 482 \n    legs \n    3 \n    2.895 \n  \n\n 483 \n    less \n    3 \n    2.895 \n  \n\n 484 \n    light \n    3 \n    2.895 \n  \n\n 485 \n    london \n    3 \n    2.895 \n  \n\n 486 \n    make \n    3 \n    2.895 \n  \n\n 487 \n    melody \n    3 \n    2.895 \n  \n\n 488 \n    memory \n    3 \n    2.895 \n  \n\n 489 \n    minutes \n    3 \n    2.895 \n  \n\n 490 \n    miriam's \n    3 \n    2.895 \n  \n\n 491 \n    mist \n    3 \n    2.895 \n  \n\n 492 \n    money \n    3 \n    2.895 \n  \n\n 493 \n    motive \n    3 \n    2.895 \n  \n\n 494 \n    mrs \n    3 \n    2.895 \n  \n\n 495 \n    name \n    3 \n    2.895 \n  \n\n 496 \n    neck \n    3 \n    2.895 \n  \n\n 497 \n    onto \n    3 \n    2.895 \n  \n\n 498 \n    open \n    3 \n    2.895 \n  \n\n 499 \n    opens \n    3 \n    2.895 \n  \n\n 500 \n    oxford \n    3 \n    2.895 \n  \n\n 501 \n    pain \n    3 \n    2.895 \n  \n\n 502 \n    paki \n    3 \n    2.895 \n  \n\n 503 \n    part \n    3 \n    2.895 \n  \n\n 504 \n    passed \n    3 \n    2.895 \n  \n\n 505 \n    period \n    3 \n    2.895 \n  \n\n 506 \n    picked \n    3 \n    2.895 \n  \n\n 507 \n    poetry \n    3 \n    2.895 \n  \n\n 508 \n    presence \n    3 \n    2.895 \n  \n\n 509 \n    pressed \n    3 \n    2.895 \n  \n\n 510 \n    prose \n    3 \n    2.895 \n  \n\n 511 \n    pushed \n    3 \n    2.895 \n  \n\n 512 \n    putting \n    3 \n    2.895 \n  \n\n 513 \n    rainy \n    3 \n    2.895 \n  \n\n 514 \n    remember \n    3 \n    2.895 \n  \n\n 515 \n    rerum \n    3 \n    2.895 \n  \n\n 516 \n    rest \n    3 \n    2.895 \n  \n\n 517 \n    returned \n    3 \n    2.895 \n  \n\n 518 \n    rima \n    3 \n    2.895 \n  \n\n 519 \n    rum \n    3 \n    2.895 \n  \n\n 520 \n    sailors \n    3 \n    2.895 \n  \n\n 521 \n    same \n    3 \n    2.895 \n  \n\n 522 \n    savannah \n    3 \n    2.895 \n  \n\n 523 \n    saying \n    3 \n    2.895 \n  \n\n 524 \n    scars \n    3 \n    2.895 \n  \n\n 525 \n    seascape \n    3 \n    2.895 \n  \n\n 526 \n    season \n    3 \n    2.895 \n  \n\n 527 \n    sexually \n    3 \n    2.895 \n  \n\n 528 \n    shame \n    3 \n    2.895 \n  \n\n 529 \n    shape \n    3 \n    2.895 \n  \n\n 530 \n    shock \n    3 \n    2.895 \n  \n\n 531 \n    sight \n    3 \n    2.895 \n  \n\n 532 \n    sign \n    3 \n    2.895 \n  \n\n 533 \n    since \n    3 \n    2.895 \n  \n\n 534 \n    sisters \n    3 \n    2.895 \n  \n\n 535 \n    slot \n    3 \n    2.895 \n  \n\n 536 \n    song \n    3 \n    2.895 \n  \n\n 537 \n    soon \n    3 \n    2.895 \n  \n\n 538 \n    spaces \n    3 \n    2.895 \n  \n\n 539 \n    special \n    3 \n    2.895 \n  \n\n 540 \n    status \n    3 \n    2.895 \n  \n\n 541 \n    stood \n    3 \n    2.895 \n  \n\n 542 \n    such \n    3 \n    2.895 \n  \n\n 543 \n    sunt \n    3 \n    2.895 \n  \n\n 544 \n    take \n    3 \n    2.895 \n  \n\n 545 \n    talk \n    3 \n    2.895 \n  \n\n 546 \n    teeth \n    3 \n    2.895 \n  \n\n 547 \n    thing \n    3 \n    2.895 \n  \n\n 548 \n    thirty \n    3 \n    2.895 \n  \n\n 549 \n    though \n    3 \n    2.895 \n  \n\n 550 \n    three \n    3 \n    2.895 \n  \n\n 551 \n    throughout \n    3 \n    2.895 \n  \n\n 552 \n    thy \n    3 \n    2.895 \n  \n\n 553 \n    top \n    3 \n    2.895 \n  \n\n 554 \n    tries \n    3 \n    2.895 \n  \n\n 555 \n    turn \n    3 \n    2.895 \n  \n\n 556 \n    turned \n    3 \n    2.895 \n  \n\n 557 \n    turns \n    3 \n    2.895 \n  \n\n 558 \n    universe \n    3 \n    2.895 \n  \n\n 559 \n    university \n    3 \n    2.895 \n  \n\n 560 \n    uttered \n    3 \n    2.895 \n  \n\n 561 \n    valleys \n    3 \n    2.895 \n  \n\n 562 \n    vast \n    3 \n    2.895 \n  \n\n 563 \n    voice \n    3 \n    2.895 \n  \n\n 564 \n    waited \n    3 \n    2.895 \n  \n\n 565 \n    waiting \n    3 \n    2.895 \n  \n\n 566 \n    wall \n    3 \n    2.895 \n  \n\n 567 \n    wanting \n    3 \n    2.895 \n  \n\n 568 \n    welcome \n    3 \n    2.895 \n  \n\n 569 \n    west \n    3 \n    2.895 \n  \n\n 570 \n    whom \n    3 \n    2.895 \n  \n\n 571 \n    word \n    3 \n    2.895 \n  \n\n 572 \n    world \n    3 \n    2.895 \n  \n\n 573 \n    writer \n    3 \n    2.895 \n  \n\n 574 \n    young \n    3 \n    2.895 \n  \n\n 575 \n    younger \n    3 \n    2.895 \n  \n\n 576 \n    a- \n    2 \n    1.930 \n  \n\n 577 \n    able \n    2 \n    1.930 \n  \n\n 578 \n    above \n    2 \n    1.930 \n  \n\n 579 \n    afraid \n    2 \n    1.930 \n  \n\n 580 \n    africans \n    2 \n    1.930 \n  \n\n 581 \n    also \n    2 \n    1.930 \n  \n\n 582 \n    amazon \n    2 \n    1.930 \n  \n\n 583 \n    anger \n    2 \n    1.930 \n  \n\n 584 \n    arms \n    2 \n    1.930 \n  \n\n 585 \n    aside \n    2 \n    1.930 \n  \n\n 586 \n    atlantic \n    2 \n    1.930 \n  \n\n 587 \n    authoritarian \n    2 \n    1.930 \n  \n\n 588 \n    authority \n    2 \n    1.930 \n  \n\n 589 \n    autobiographical \n    2 \n    1.930 \n  \n\n 590 \n    automatically \n    2 \n    1.930 \n  \n\n 591 \n    basest \n    2 \n    1.930 \n  \n\n 592 \n    beadless \n    2 \n    1.930 \n  \n\n 593 \n    beads \n    2 \n    1.930 \n  \n\n 594 \n    became \n    2 \n    1.930 \n  \n\n 595 \n    beckon \n    2 \n    1.930 \n  \n\n 596 \n    bed \n    2 \n    1.930 \n  \n\n 597 \n    believed \n    2 \n    1.930 \n  \n\n 598 \n    belly \n    2 \n    1.930 \n  \n\n 599 \n    beside \n    2 \n    1.930 \n  \n\n 600 \n    bewildered \n    2 \n    1.930 \n  \n\n 601 \n    birth \n    2 \n    1.930 \n  \n\n 602 \n    bit \n    2 \n    1.930 \n  \n\n 603 \n    blackness \n    2 \n    1.930 \n  \n\n 604 \n    blindly \n    2 \n    1.930 \n  \n\n 605 \n    blue \n    2 \n    1.930 \n  \n\n 606 \n    board \n    2 \n    1.930 \n  \n\n 607 \n    boldest \n    2 \n    1.930 \n  \n\n 608 \n    bone \n    2 \n    1.930 \n  \n\n 609 \n    booth \n    2 \n    1.930 \n  \n\n 610 \n    bottom \n    2 \n    1.930 \n  \n\n 611 \n    boundaries \n    2 \n    1.930 \n  \n\n 612 \n    bow \n    2 \n    1.930 \n  \n\n 613 \n    brilliant \n    2 \n    1.930 \n  \n\n 614 \n    brooding \n    2 \n    1.930 \n  \n\n 615 \n    brothers \n    2 \n    1.930 \n  \n\n 616 \n    bundle \n    2 \n    1.930 \n  \n\n 617 \n    bus \n    2 \n    1.930 \n  \n\n 618 \n    calf \n    2 \n    1.930 \n  \n\n 619 \n    care \n    2 \n    1.930 \n  \n\n 620 \n    centuries \n    2 \n    1.930 \n  \n\n 621 \n    certain \n    2 \n    1.930 \n  \n\n 622 \n    chap \n    2 \n    1.930 \n  \n\n 623 \n    cheeks \n    2 \n    1.930 \n  \n\n 624 \n    childbirth \n    2 \n    1.930 \n  \n\n 625 \n    children \n    2 \n    1.930 \n  \n\n 626 \n    cigarette \n    2 \n    1.930 \n  \n\n 627 \n    circus \n    2 \n    1.930 \n  \n\n 628 \n    class \n    2 \n    1.930 \n  \n\n 629 \n    clear \n    2 \n    1.930 \n  \n\n 630 \n    closes \n    2 \n    1.930 \n  \n\n 631 \n    coastal \n    2 \n    1.930 \n  \n\n 632 \n    coconuts \n    2 \n    1.930 \n  \n\n 633 \n    coin \n    2 \n    1.930 \n  \n\n 634 \n    cold \n    2 \n    1.930 \n  \n\n 635 \n    comes \n    2 \n    1.930 \n  \n\n 636 \n    commerce \n    2 \n    1.930 \n  \n\n 637 \n    community \n    2 \n    1.930 \n  \n\n 638 \n    constant \n    2 \n    1.930 \n  \n\n 639 \n    corn \n    2 \n    1.930 \n  \n\n 640 \n    corner \n    2 \n    1.930 \n  \n\n 641 \n    corners \n    2 \n    1.930 \n  \n\n 642 \n    couch \n    2 \n    1.930 \n  \n\n 643 \n    couple \n    2 \n    1.930 \n  \n\n 644 \n    courtly \n    2 \n    1.930 \n  \n\n 645 \n    crammed \n    2 \n    1.930 \n  \n\n 646 \n    crashing \n    2 \n    1.930 \n  \n\n 647 \n    creatures \n    2 \n    1.930 \n  \n\n 648 \n    cry \n    2 \n    1.930 \n  \n\n 649 \n    customers \n    2 \n    1.930 \n  \n\n 650 \n    dared \n    2 \n    1.930 \n  \n\n 651 \n    days \n    2 \n    1.930 \n  \n\n 652 \n    dazed \n    2 \n    1.930 \n  \n\n 653 \n    death \n    2 \n    1.930 \n  \n\n 654 \n    debased \n    2 \n    1.930 \n  \n\n 655 \n    deserving \n    2 \n    1.930 \n  \n\n 656 \n    desperate \n    2 \n    1.930 \n  \n\n 657 \n    died \n    2 \n    1.930 \n  \n\n 658 \n    direct \n    2 \n    1.930 \n  \n\n 659 \n    does \n    2 \n    1.930 \n  \n\n 660 \n    doesn't \n    2 \n    1.930 \n  \n\n 661 \n    doubts \n    2 \n    1.930 \n  \n\n 662 \n    drowning \n    2 \n    1.930 \n  \n\n 663 \n    drunk \n    2 \n    1.930 \n  \n\n 664 \n    dunciad \n    2 \n    1.930 \n  \n\n 665 \n    dust \n    2 \n    1.930 \n  \n\n 666 \n    easily \n    2 \n    1.930 \n  \n\n 667 \n    easy \n    2 \n    1.930 \n  \n\n 668 \n    edge \n    2 \n    1.930 \n  \n\n 669 \n    eh \n    2 \n    1.930 \n  \n\n 670 \n    elders \n    2 \n    1.930 \n  \n\n 671 \n    ends \n    2 \n    1.930 \n  \n\n 672 \n    equiano \n    2 \n    1.930 \n  \n\n 673 \n    eschatological \n    2 \n    1.930 \n  \n\n 674 \n    especially \n    2 \n    1.930 \n  \n\n 675 \n    eventually \n    2 \n    1.930 \n  \n\n 676 \n    ever \n    2 \n    1.930 \n  \n\n 677 \n    every \n    2 \n    1.930 \n  \n\n 678 \n    everything \n    2 \n    1.930 \n  \n\n 679 \n    exactly \n    2 \n    1.930 \n  \n\n 680 \n    existence \n    2 \n    1.930 \n  \n\n 681 \n    expecting \n    2 \n    1.930 \n  \n\n 682 \n    experience \n    2 \n    1.930 \n  \n\n 683 \n    external \n    2 \n    1.930 \n  \n\n 684 \n    faith \n    2 \n    1.930 \n  \n\n 685 \n    falls \n    2 \n    1.930 \n  \n\n 686 \n    far \n    2 \n    1.930 \n  \n\n 687 \n    fields \n    2 \n    1.930 \n  \n\n 688 \n    figure \n    2 \n    1.930 \n  \n\n 689 \n    find \n    2 \n    1.930 \n  \n\n 690 \n    flow \n    2 \n    1.930 \n  \n\n 691 \n    folds \n    2 \n    1.930 \n  \n\n 692 \n    foot \n    2 \n    1.930 \n  \n\n 693 \n    foreign \n    2 \n    1.930 \n  \n\n 694 \n    foretell \n    2 \n    1.930 \n  \n\n 695 \n    fortune \n    2 \n    1.930 \n  \n\n 696 \n    frantically \n    2 \n    1.930 \n  \n\n 697 \n    free \n    2 \n    1.930 \n  \n\n 698 \n    freedom \n    2 \n    1.930 \n  \n\n 699 \n    fright \n    2 \n    1.930 \n  \n\n 700 \n    fruit \n    2 \n    1.930 \n  \n\n 701 \n    g-, \n    2 \n    1.930 \n  \n\n 702 \n    gathered \n    2 \n    1.930 \n  \n\n 703 \n    genuine \n    2 \n    1.930 \n  \n\n 704 \n    ghosts \n    2 \n    1.930 \n  \n\n 705 \n    girl \n    2 \n    1.930 \n  \n\n 706 \n    girls \n    2 \n    1.930 \n  \n\n 707 \n    glass \n    2 \n    1.930 \n  \n\n 708 \n    goat \n    2 \n    1.930 \n  \n\n 709 \n    gown \n    2 \n    1.930 \n  \n\n 710 \n    grain \n    2 \n    1.930 \n  \n\n 711 \n    greeks \n    2 \n    1.930 \n  \n\n 712 \n    grew \n    2 \n    1.930 \n  \n\n 713 \n    grown \n    2 \n    1.930 \n  \n\n 714 \n    guilt \n    2 \n    1.930 \n  \n\n 715 \n    guyana's \n    2 \n    1.930 \n  \n\n 716 \n    half \n    2 \n    1.930 \n  \n\n 717 \n    hard \n    2 \n    1.930 \n  \n\n 718 \n    hate \n    2 \n    1.930 \n  \n\n 719 \n    held \n    2 \n    1.930 \n  \n\n 720 \n    herds \n    2 \n    1.930 \n  \n\n 721 \n    herself \n    2 \n    1.930 \n  \n\n 722 \n    hidden \n    2 \n    1.930 \n  \n\n 723 \n    hindu \n    2 \n    1.930 \n  \n\n 724 \n    holes \n    2 \n    1.930 \n  \n\n 725 \n    holocaust \n    2 \n    1.930 \n  \n\n 726 \n    hook \n    2 \n    1.930 \n  \n\n 727 \n    huge \n    2 \n    1.930 \n  \n\n 728 \n    human \n    2 \n    1.930 \n  \n\n 729 \n    humiliation \n    2 \n    1.930 \n  \n\n 730 \n    hundred-and- \n    2 \n    1.930 \n  \n\n 731 \n    hut \n    2 \n    1.930 \n  \n\n 732 \n    idiot \n    2 \n    1.930 \n  \n\n 733 \n    ignorant \n    2 \n    1.930 \n  \n\n 734 \n    image \n    2 \n    1.930 \n  \n\n 735 \n    impatient \n    2 \n    1.930 \n  \n\n 736 \n    important \n    2 \n    1.930 \n  \n\n 737 \n    impose \n    2 \n    1.930 \n  \n\n 738 \n    inherently \n    2 \n    1.930 \n  \n\n 739 \n    instructions \n    2 \n    1.930 \n  \n\n 740 \n    involved \n    2 \n    1.930 \n  \n\n 741 \n    joke \n    2 \n    1.930 \n  \n\n 742 \n    jouti \n    2 \n    1.930 \n  \n\n 743 \n    kaka's \n    2 \n    1.930 \n  \n\n 744 \n    keeps \n    2 \n    1.930 \n  \n\n 745 \n    key \n    2 \n    1.930 \n  \n\n 746 \n    kinds \n    2 \n    1.930 \n  \n\n 747 \n    knock \n    2 \n    1.930 \n  \n\n 748 \n    knowing \n    2 \n    1.930 \n  \n\n 749 \n    laid \n    2 \n    1.930 \n  \n\n 750 \n    later \n    2 \n    1.930 \n  \n\n 751 \n    leaned \n    2 \n    1.930 \n  \n\n 752 \n    leaving \n    2 \n    1.930 \n  \n\n 753 \n    levels \n    2 \n    1.930 \n  \n\n 754 \n    lights \n    2 \n    1.930 \n  \n\n 755 \n    lips \n    2 \n    1.930 \n  \n\n 756 \n    lives \n    2 \n    1.930 \n  \n\n 757 \n    logies \n    2 \n    1.930 \n  \n\n 758 \n    loss \n    2 \n    1.930 \n  \n\n 759 \n    lost \n    2 \n    1.930 \n  \n\n 760 \n    magazines \n    2 \n    1.930 \n  \n\n 761 \n    magicians \n    2 \n    1.930 \n  \n\n 762 \n    making \n    2 \n    1.930 \n  \n\n 763 \n    mangoes \n    2 \n    1.930 \n  \n\n 764 \n    manu's \n    2 \n    1.930 \n  \n\n 765 \n    martin \n    2 \n    1.930 \n  \n\n 766 \n    marvelling \n    2 \n    1.930 \n  \n\n 767 \n    middle \n    2 \n    1.930 \n  \n\n 768 \n    migrate \n    2 \n    1.930 \n  \n\n 769 \n    mocking \n    2 \n    1.930 \n  \n\n 770 \n    moment \n    2 \n    1.930 \n  \n\n 771 \n    momentum \n    2 \n    1.930 \n  \n\n 772 \n    monotonously \n    2 \n    1.930 \n  \n\n 773 \n    months \n    2 \n    1.930 \n  \n\n 774 \n    morning \n    2 \n    1.930 \n  \n\n 775 \n    mortar \n    2 \n    1.930 \n  \n\n 776 \n    mouths \n    2 \n    1.930 \n  \n\n 777 \n    moved \n    2 \n    1.930 \n  \n\n 778 \n    mud \n    2 \n    1.930 \n  \n\n 779 \n    muddy \n    2 \n    1.930 \n  \n\n 780 \n    mythology \n    2 \n    1.930 \n  \n\n 781 \n    nameless \n    2 \n    1.930 \n  \n\n 782 \n    naming \n    2 \n    1.930 \n  \n\n 783 \n    nature \n    2 \n    1.930 \n  \n\n 784 \n    neither \n    2 \n    1.930 \n  \n\n 785 \n    neuroses \n    2 \n    1.930 \n  \n\n 786 \n    next \n    2 \n    1.930 \n  \n\n 787 \n    nineteen \n    2 \n    1.930 \n  \n\n 788 \n    noise \n    2 \n    1.930 \n  \n\n 789 \n    nose \n    2 \n    1.930 \n  \n\n 790 \n    odd \n    2 \n    1.930 \n  \n\n 791 \n    often \n    2 \n    1.930 \n  \n\n 792 \n    oh \n    2 \n    1.930 \n  \n\n 793 \n    ornamental \n    2 \n    1.930 \n  \n\n 794 \n    others \n    2 \n    1.930 \n  \n\n 795 \n    page \n    2 \n    1.930 \n  \n\n 796 \n    passionate \n    2 \n    1.930 \n  \n\n 797 \n    past \n    2 \n    1.930 \n  \n\n 798 \n    paths \n    2 \n    1.930 \n  \n\n 799 \n    pavement \n    2 \n    1.930 \n  \n\n 800 \n    pay \n    2 \n    1.930 \n  \n\n 801 \n    peculiar \n    2 \n    1.930 \n  \n\n 802 \n    peepshow \n    2 \n    1.930 \n  \n\n 803 \n    pence \n    2 \n    1.930 \n  \n\n 804 \n    perilous \n    2 \n    1.930 \n  \n\n 805 \n    perish \n    2 \n    1.930 \n  \n\n 806 \n    piccadilly \n    2 \n    1.930 \n  \n\n 807 \n    pictures \n    2 \n    1.930 \n  \n\n 808 \n    pincher \n    2 \n    1.930 \n  \n\n 809 \n    places \n    2 \n    1.930 \n  \n\n 810 \n    plague \n    2 \n    1.930 \n  \n\n 811 \n    plane \n    2 \n    1.930 \n  \n\n 812 \n    pleasure \n    2 \n    1.930 \n  \n\n 813 \n    pocket \n    2 \n    1.930 \n  \n\n 814 \n    pool \n    2 \n    1.930 \n  \n\n 815 \n    poor \n    2 \n    1.930 \n  \n\n 816 \n    possibility \n    2 \n    1.930 \n  \n\n 817 \n    previous \n    2 \n    1.930 \n  \n\n 818 \n    pride \n    2 \n    1.930 \n  \n\n 819 \n    promised \n    2 \n    1.930 \n  \n\n 820 \n    protection \n    2 \n    1.930 \n  \n\n 821 \n    pure \n    2 \n    1.930 \n  \n\n 822 \n    purse \n    2 \n    1.930 \n  \n\n 823 \n    quick \n    2 \n    1.930 \n  \n\n 824 \n    rack \n    2 \n    1.930 \n  \n\n 825 \n    rain \n    2 \n    1.930 \n  \n\n 826 \n    rainforest \n    2 \n    1.930 \n  \n\n 827 \n    raw \n    2 \n    1.930 \n  \n\n 828 \n    re-, \n    2 \n    1.930 \n  \n\n 829 \n    recognize \n    2 \n    1.930 \n  \n\n 830 \n    relationship \n    2 \n    1.930 \n  \n\n 831 \n    relax \n    2 \n    1.930 \n  \n\n 832 \n    remains \n    2 \n    1.930 \n  \n\n 833 \n    response \n    2 \n    1.930 \n  \n\n 834 \n    river \n    2 \n    1.930 \n  \n\n 835 \n    romance \n    2 \n    1.930 \n  \n\n 836 \n    round \n    2 \n    1.930 \n  \n\n 837 \n    rubbish \n    2 \n    1.930 \n  \n\n 838 \n    rudeness \n    2 \n    1.930 \n  \n\n 839 \n    sake \n    2 \n    1.930 \n  \n\n 840 \n    salt \n    2 \n    1.930 \n  \n\n 841 \n    school \n    2 \n    1.930 \n  \n\n 842 \n    scunt \n    2 \n    1.930 \n  \n\n 843 \n    search \n    2 \n    1.930 \n  \n\n 844 \n    searched \n    2 \n    1.930 \n  \n\n 845 \n    secret \n    2 \n    1.930 \n  \n\n 846 \n    secretly \n    2 \n    1.930 \n  \n\n 847 \n    secure \n    2 \n    1.930 \n  \n\n 848 \n    seemed \n    2 \n    1.930 \n  \n\n 849 \n    seeps \n    2 \n    1.930 \n  \n\n 850 \n    self \n    2 \n    1.930 \n  \n\n 851 \n    setting \n    2 \n    1.930 \n  \n\n 852 \n    settled \n    2 \n    1.930 \n  \n\n 853 \n    seven-and-a-half-thousand \n    2 \n    1.930 \n  \n\n 854 \n    shah's \n    2 \n    1.930 \n  \n\n 855 \n    she's \n    2 \n    1.930 \n  \n\n 856 \n    shores \n    2 \n    1.930 \n  \n\n 857 \n    show \n    2 \n    1.930 \n  \n\n 858 \n    shyness \n    2 \n    1.930 \n  \n\n 859 \n    silver \n    2 \n    1.930 \n  \n\n 860 \n    sits \n    2 \n    1.930 \n  \n\n 861 \n    slap \n    2 \n    1.930 \n  \n\n 862 \n    sleep \n    2 \n    1.930 \n  \n\n 863 \n    sought \n    2 \n    1.930 \n  \n\n 864 \n    sound \n    2 \n    1.930 \n  \n\n 865 \n    stare \n    2 \n    1.930 \n  \n\n 866 \n    stared \n    2 \n    1.930 \n  \n\n 867 \n    staring \n    2 \n    1.930 \n  \n\n 868 \n    stayed \n    2 \n    1.930 \n  \n\n 869 \n    steal \n    2 \n    1.930 \n  \n\n 870 \n    steel \n    2 \n    1.930 \n  \n\n 871 \n    strange \n    2 \n    1.930 \n  \n\n 872 \n    street \n    2 \n    1.930 \n  \n\n 873 \n    strip \n    2 \n    1.930 \n  \n\n 874 \n    struggle \n    2 \n    1.930 \n  \n\n 875 \n    sublime \n    2 \n    1.930 \n  \n\n 876 \n    suddenly \n    2 \n    1.930 \n  \n\n 877 \n    suggest \n    2 \n    1.930 \n  \n\n 878 \n    sunk \n    2 \n    1.930 \n  \n\n 879 \n    sure \n    2 \n    1.930 \n  \n\n 880 \n    swum \n    2 \n    1.930 \n  \n\n 881 \n    tabla \n    2 \n    1.930 \n  \n\n 882 \n    taken \n    2 \n    1.930 \n  \n\n 883 \n    talking \n    2 \n    1.930 \n  \n\n 884 \n    talks \n    2 \n    1.930 \n  \n\n 885 \n    tanda's \n    2 \n    1.930 \n  \n\n 886 \n    tax \n    2 \n    1.930 \n  \n\n 887 \n    tea \n    2 \n    1.930 \n  \n\n 888 \n    teach \n    2 \n    1.930 \n  \n\n 889 \n    tempts \n    2 \n    1.930 \n  \n\n 890 \n    ten \n    2 \n    1.930 \n  \n\n 891 \n    terrified \n    2 \n    1.930 \n  \n\n 892 \n    thomas \n    2 \n    1.930 \n  \n\n 893 \n    throwing \n    2 \n    1.930 \n  \n\n 894 \n    thrown \n    2 \n    1.930 \n  \n\n 895 \n    tilt \n    2 \n    1.930 \n  \n\n 896 \n    tongue \n    2 \n    1.930 \n  \n\n 897 \n    toys \n    2 \n    1.930 \n  \n\n 898 \n    trail \n    2 \n    1.930 \n  \n\n 899 \n    trance \n    2 \n    1.930 \n  \n\n 900 \n    transformed \n    2 \n    1.930 \n  \n\n 901 \n    treasures \n    2 \n    1.930 \n  \n\n 902 \n    trees \n    2 \n    1.930 \n  \n\n 903 \n    tribes \n    2 \n    1.930 \n  \n\n 904 \n    tropical \n    2 \n    1.930 \n  \n\n 905 \n    under \n    2 \n    1.930 \n  \n\n 906 \n    unstable \n    2 \n    1.930 \n  \n\n 907 \n    urged \n    2 \n    1.930 \n  \n\n 908 \n    urns \n    2 \n    1.930 \n  \n\n 909 \n    v-, \n    2 \n    1.930 \n  \n\n 910 \n    voices \n    2 \n    1.930 \n  \n\n 911 \n    vulgar \n    2 \n    1.930 \n  \n\n 912 \n    walcott \n    2 \n    1.930 \n  \n\n 913 \n    walk \n    2 \n    1.930 \n  \n\n 914 \n    walked \n    2 \n    1.930 \n  \n\n 915 \n    warming \n    2 \n    1.930 \n  \n\n 916 \n    watching \n    2 \n    1.930 \n  \n\n 917 \n    wear \n    2 \n    1.930 \n  \n\n 918 \n    while \n    2 \n    1.930 \n  \n\n 919 \n    whilst \n    2 \n    1.930 \n  \n\n 920 \n    whisper \n    2 \n    1.930 \n  \n\n 921 \n    whispered \n    2 \n    1.930 \n  \n\n 922 \n    wind \n    2 \n    1.930 \n  \n\n 923 \n    window \n    2 \n    1.930 \n  \n\n 924 \n    wisdom \n    2 \n    1.930 \n  \n\n 925 \n    wood \n    2 \n    1.930 \n  \n\n 926 \n    worms \n    2 \n    1.930 \n  \n\n 927 \n    wounds \n    2 \n    1.930 \n  \n\n 928 \n    y-, \n    2 \n    1.930 \n  \n\n 929 \n    you'll \n    2 \n    1.930 \n  \n\n 930 \n    you've \n    2 \n    1.930 \n  \n\n 931 \n    a-, \n    1 \n    0.965 \n  \n\n 932 \n    a-n-a-r-c-h \n    1 \n    0.965 \n  \n\n 933 \n    aboard \n    1 \n    0.965 \n  \n\n 934 \n    abolition \n    1 \n    0.965 \n  \n\n 935 \n    abor-, \n    1 \n    0.965 \n  \n\n 936 \n    aborigines \n    1 \n    0.965 \n  \n\n 937 \n    aborted \n    1 \n    0.965 \n  \n\n 938 \n    abscond \n    1 \n    0.965 \n  \n\n 939 \n    absent \n    1 \n    0.965 \n  \n\n 940 \n    absolutely \n    1 \n    0.965 \n  \n\n 941 \n    acceptable \n    1 \n    0.965 \n  \n\n 942 \n    accepted \n    1 \n    0.965 \n  \n\n 943 \n    accustomed \n    1 \n    0.965 \n  \n\n 944 \n    acha \n    1 \n    0.965 \n  \n\n 945 \n    aching \n    1 \n    0.965 \n  \n\n 946 \n    acknowledged \n    1 \n    0.965 \n  \n\n 947 \n    acquired \n    1 \n    0.965 \n  \n\n 948 \n    adjoining \n    1 \n    0.965 \n  \n\n 949 \n    admiration \n    1 \n    0.965 \n  \n\n 950 \n    admire \n    1 \n    0.965 \n  \n\n 951 \n    admired \n    1 \n    0.965 \n  \n\n 952 \n    admixture \n    1 \n    0.965 \n  \n\n 953 \n    adorned \n    1 \n    0.965 \n  \n\n 954 \n    adv-, \n    1 \n    0.965 \n  \n\n 955 \n    advance \n    1 \n    0.965 \n  \n\n 956 \n    affect \n    1 \n    0.965 \n  \n\n 957 \n    age \n    1 \n    0.965 \n  \n\n 958 \n    agitated \n    1 \n    0.965 \n  \n\n 959 \n    agreement \n    1 \n    0.965 \n  \n\n 960 \n    ahead \n    1 \n    0.965 \n  \n\n 961 \n    ain't \n    1 \n    0.965 \n  \n\n 962 \n    alarm \n    1 \n    0.965 \n  \n\n 963 \n    already \n    1 \n    0.965 \n  \n\n 964 \n    although \n    1 \n    0.965 \n  \n\n 965 \n    ambiguity \n    1 \n    0.965 \n  \n\n 966 \n    ambushes \n    1 \n    0.965 \n  \n\n 967 \n    amen \n    1 \n    0.965 \n  \n\n 968 \n    amerindian \n    1 \n    0.965 \n  \n\n 969 \n    amerindians \n    1 \n    0.965 \n  \n\n 970 \n    amusement \n    1 \n    0.965 \n  \n\n 971 \n    ancestral \n    1 \n    0.965 \n  \n\n 972 \n    anchored \n    1 \n    0.965 \n  \n\n 973 \n    ancient \n    1 \n    0.965 \n  \n\n 974 \n    anew \n    1 \n    0.965 \n  \n\n 975 \n    angel \n    1 \n    0.965 \n  \n\n 976 \n    angelic \n    1 \n    0.965 \n  \n\n 977 \n    anglican \n    1 \n    0.965 \n  \n\n 978 \n    anguish \n    1 \n    0.965 \n  \n\n 979 \n    anniversary \n    1 \n    0.965 \n  \n\n 980 \n    announces \n    1 \n    0.965 \n  \n\n 981 \n    answer \n    1 \n    0.965 \n  \n\n 982 \n    antique \n    1 \n    0.965 \n  \n\n 983 \n    antisocial \n    1 \n    0.965 \n  \n\n 984 \n    anything \n    1 \n    0.965 \n  \n\n 985 \n    anywhere \n    1 \n    0.965 \n  \n\n 986 \n    ap-, \n    1 \n    0.965 \n  \n\n 987 \n    apologies \n    1 \n    0.965 \n  \n\n 988 \n    apologist \n    1 \n    0.965 \n  \n\n 989 \n    apologize \n    1 \n    0.965 \n  \n\n 990 \n    appear \n    1 \n    0.965 \n  \n\n 991 \n    appearance \n    1 \n    0.965 \n  \n\n 992 \n    appease \n    1 \n    0.965 \n  \n\n 993 \n    arbitrary \n    1 \n    0.965 \n  \n\n 994 \n    arcades \n    1 \n    0.965 \n  \n\n 995 \n    aristotle \n    1 \n    0.965 \n  \n\n 996 \n    arose \n    1 \n    0.965 \n  \n\n 997 \n    arrange \n    1 \n    0.965 \n  \n\n 998 \n    arrived \n    1 \n    0.965 \n  \n\n 999 \n    arrogance \n    1 \n    0.965 \n  \n\n 1000 \n    arrow \n    1 \n    0.965 \n  \n\n 1001 \n    article \n    1 \n    0.965 \n  \n\n 1002 \n    artifice \n    1 \n    0.965 \n  \n\n 1003 \n    as-, \n    1 \n    0.965 \n  \n\n 1004 \n    aspects \n    1 \n    0.965 \n  \n\n 1005 \n    assault \n    1 \n    0.965 \n  \n\n 1006 \n    assigned \n    1 \n    0.965 \n  \n\n 1007 \n    assumed \n    1 \n    0.965 \n  \n\n 1008 \n    astonished \n    1 \n    0.965 \n  \n\n 1009 \n    await \n    1 \n    0.965 \n  \n\n 1010 \n    awaken \n    1 \n    0.965 \n  \n\n 1011 \n    awakens \n    1 \n    0.965 \n  \n\n 1012 \n    awed \n    1 \n    0.965 \n  \n\n 1013 \n    awhile \n    1 \n    0.965 \n  \n\n 1014 \n    awoke \n    1 \n    0.965 \n  \n\n 1015 \n    babbled \n    1 \n    0.965 \n  \n\n 1016 \n    babbling \n    1 \n    0.965 \n  \n\n 1017 \n    babies \n    1 \n    0.965 \n  \n\n 1018 \n    backdam \n    1 \n    0.965 \n  \n\n 1019 \n    backsides \n    1 \n    0.965 \n  \n\n 1020 \n    backwards \n    1 \n    0.965 \n  \n\n 1021 \n    bags \n    1 \n    0.965 \n  \n\n 1022 \n    baju's \n    1 \n    0.965 \n  \n\n 1023 \n    baked \n    1 \n    0.965 \n  \n\n 1024 \n    banged \n    1 \n    0.965 \n  \n\n 1025 \n    bank \n    1 \n    0.965 \n  \n\n 1026 \n    barely \n    1 \n    0.965 \n  \n\n 1027 \n    barred \n    1 \n    0.965 \n  \n\n 1028 \n    barrels \n    1 \n    0.965 \n  \n\n 1029 \n    barren \n    1 \n    0.965 \n  \n\n 1030 \n    base \n    1 \n    0.965 \n  \n\n 1031 \n    based \n    1 \n    0.965 \n  \n\n 1032 \n    basis \n    1 \n    0.965 \n  \n\n 1033 \n    bastards \n    1 \n    0.965 \n  \n\n 1034 \n    bathos \n    1 \n    0.965 \n  \n\n 1035 \n    battleground \n    1 \n    0.965 \n  \n\n 1036 \n    bawling \n    1 \n    0.965 \n  \n\n 1037 \n    bear \n    1 \n    0.965 \n  \n\n 1038 \n    beasts \n    1 \n    0.965 \n  \n\n 1039 \n    beaten \n    1 \n    0.965 \n  \n\n 1040 \n    beforehand \n    1 \n    0.965 \n  \n\n 1041 \n    beggar \n    1 \n    0.965 \n  \n\n 1042 \n    beggared \n    1 \n    0.965 \n  \n\n 1043 \n    begging \n    1 \n    0.965 \n  \n\n 1044 \n    beginning \n    1 \n    0.965 \n  \n\n 1045 \n    beguiled \n    1 \n    0.965 \n  \n\n 1046 \n    beholds \n    1 \n    0.965 \n  \n\n 1047 \n    bellies \n    1 \n    0.965 \n  \n\n 1048 \n    belonged \n    1 \n    0.965 \n  \n\n 1049 \n    beloved \n    1 \n    0.965 \n  \n\n 1050 \n    bench \n    1 \n    0.965 \n  \n\n 1051 \n    bends \n    1 \n    0.965 \n  \n\n 1052 \n    beneath \n    1 \n    0.965 \n  \n\n 1053 \n    bereft \n    1 \n    0.965 \n  \n\n 1054 \n    beseech \n    1 \n    0.965 \n  \n\n 1055 \n    bespeak \n    1 \n    0.965 \n  \n\n 1056 \n    best \n    1 \n    0.965 \n  \n\n 1057 \n    betoken \n    1 \n    0.965 \n  \n\n 1058 \n    betrayal \n    1 \n    0.965 \n  \n\n 1059 \n    betrayed \n    1 \n    0.965 \n  \n\n 1060 \n    betrayer \n    1 \n    0.965 \n  \n\n 1061 \n    better \n    1 \n    0.965 \n  \n\n 1062 \n    billboards \n    1 \n    0.965 \n  \n\n 1063 \n    bitch \n    1 \n    0.965 \n  \n\n 1064 \n    bites \n    1 \n    0.965 \n  \n\n 1065 \n    bits \n    1 \n    0.965 \n  \n\n 1066 \n    bizarre \n    1 \n    0.965 \n  \n\n 1067 \n    bla-, \n    1 \n    0.965 \n  \n\n 1068 \n    blacker \n    1 \n    0.965 \n  \n\n 1069 \n    blade \n    1 \n    0.965 \n  \n\n 1070 \n    blasted \n    1 \n    0.965 \n  \n\n 1071 \n    bleakly \n    1 \n    0.965 \n  \n\n 1072 \n    blessed \n    1 \n    0.965 \n  \n\n 1073 \n    blindfolds \n    1 \n    0.965 \n  \n\n 1074 \n    blinds \n    1 \n    0.965 \n  \n\n 1075 \n    blood-cloth \n    1 \n    0.965 \n  \n\n 1076 \n    blossoming \n    1 \n    0.965 \n  \n\n 1077 \n    blow \n    1 \n    0.965 \n  \n\n 1078 \n    blowing \n    1 \n    0.965 \n  \n\n 1079 \n    blurb \n    1 \n    0.965 \n  \n\n 1080 \n    boiled \n    1 \n    0.965 \n  \n\n 1081 \n    boils \n    1 \n    0.965 \n  \n\n 1082 \n    bore \n    1 \n    0.965 \n  \n\n 1083 \n    bosom \n    1 \n    0.965 \n  \n\n 1084 \n    both \n    1 \n    0.965 \n  \n\n 1085 \n    bourg-, \n    1 \n    0.965 \n  \n\n 1086 \n    bourgeoisie \n    1 \n    0.965 \n  \n\n 1087 \n    bowl \n    1 \n    0.965 \n  \n\n 1088 \n    box \n    1 \n    0.965 \n  \n\n 1089 \n    boys' \n    1 \n    0.965 \n  \n\n 1090 \n    braced \n    1 \n    0.965 \n  \n\n 1091 \n    breakfast \n    1 \n    0.965 \n  \n\n 1092 \n    breaks \n    1 \n    0.965 \n  \n\n 1093 \n    breast \n    1 \n    0.965 \n  \n\n 1094 \n    breasts \n    1 \n    0.965 \n  \n\n 1095 \n    breathless \n    1 \n    0.965 \n  \n\n 1096 \n    breeding \n    1 \n    0.965 \n  \n\n 1097 \n    brick \n    1 \n    0.965 \n  \n\n 1098 \n    brides \n    1 \n    0.965 \n  \n\n 1099 \n    briefly \n    1 \n    0.965 \n  \n\n 1100 \n    brings \n    1 \n    0.965 \n  \n\n 1101 \n    bro-, \n    1 \n    0.965 \n  \n\n 1102 \n    broad \n    1 \n    0.965 \n  \n\n 1103 \n    brought \n    1 \n    0.965 \n  \n\n 1104 \n    bruise \n    1 \n    0.965 \n  \n\n 1105 \n    bruised \n    1 \n    0.965 \n  \n\n 1106 \n    bruises \n    1 \n    0.965 \n  \n\n 1107 \n    bruising \n    1 \n    0.965 \n  \n\n 1108 \n    bubbling \n    1 \n    0.965 \n  \n\n 1109 \n    bugger \n    1 \n    0.965 \n  \n\n 1110 \n    buoying \n    1 \n    0.965 \n  \n\n 1111 \n    burden \n    1 \n    0.965 \n  \n\n 1112 \n    burial \n    1 \n    0.965 \n  \n\n 1113 \n    buries \n    1 \n    0.965 \n  \n\n 1114 \n    burning \n    1 \n    0.965 \n  \n\n 1115 \n    buy \n    1 \n    0.965 \n  \n\n 1116 \n    c-s \n    1 \n    0.965 \n  \n\n 1117 \n    cackled \n    1 \n    0.965 \n  \n\n 1118 \n    cakes \n    1 \n    0.965 \n  \n\n 1119 \n    calculating \n    1 \n    0.965 \n  \n\n 1120 \n    calling \n    1 \n    0.965 \n  \n\n 1121 \n    canals \n    1 \n    0.965 \n  \n\n 1122 \n    canefields \n    1 \n    0.965 \n  \n\n 1123 \n    cannon \n    1 \n    0.965 \n  \n\n 1124 \n    cannot \n    1 \n    0.965 \n  \n\n 1125 \n    canonical \n    1 \n    0.965 \n  \n\n 1126 \n    canyons \n    1 \n    0.965 \n  \n\n 1127 \n    capitalizing \n    1 \n    0.965 \n  \n\n 1128 \n    captain's \n    1 \n    0.965 \n  \n\n 1129 \n    careful \n    1 \n    0.965 \n  \n\n 1130 \n    caressed \n    1 \n    0.965 \n  \n\n 1131 \n    cargo \n    1 \n    0.965 \n  \n\n 1132 \n    carnival \n    1 \n    0.965 \n  \n\n 1133 \n    carve \n    1 \n    0.965 \n  \n\n 1134 \n    carved \n    1 \n    0.965 \n  \n\n 1135 \n    cast \n    1 \n    0.965 \n  \n\n 1136 \n    catching \n    1 \n    0.965 \n  \n\n 1137 \n    caused \n    1 \n    0.965 \n  \n\n 1138 \n    ceremonies \n    1 \n    0.965 \n  \n\n 1139 \n    chained \n    1 \n    0.965 \n  \n\n 1140 \n    chains \n    1 \n    0.965 \n  \n\n 1141 \n    chants \n    1 \n    0.965 \n  \n\n 1142 \n    chapter \n    1 \n    0.965 \n  \n\n 1143 \n    character \n    1 \n    0.965 \n  \n\n 1144 \n    chasing \n    1 \n    0.965 \n  \n\n 1145 \n    chaucer \n    1 \n    0.965 \n  \n\n 1146 \n    chaucer's \n    1 \n    0.965 \n  \n\n 1147 \n    cheap \n    1 \n    0.965 \n  \n\n 1148 \n    checked \n    1 \n    0.965 \n  \n\n 1149 \n    checks \n    1 \n    0.965 \n  \n\n 1150 \n    cheque \n    1 \n    0.965 \n  \n\n 1151 \n    cherubims \n    1 \n    0.965 \n  \n\n 1152 \n    chests \n    1 \n    0.965 \n  \n\n 1153 \n    chisel \n    1 \n    0.965 \n  \n\n 1154 \n    choice \n    1 \n    0.965 \n  \n\n 1155 \n    choose \n    1 \n    0.965 \n  \n\n 1156 \n    christian \n    1 \n    0.965 \n  \n\n 1157 \n    chronicles \n    1 \n    0.965 \n  \n\n 1158 \n    chuck \n    1 \n    0.965 \n  \n\n 1159 \n    church \n    1 \n    0.965 \n  \n\n 1160 \n    cinemas \n    1 \n    0.965 \n  \n\n 1161 \n    circumference \n    1 \n    0.965 \n  \n\n 1162 \n    civilization \n    1 \n    0.965 \n  \n\n 1163 \n    clasped \n    1 \n    0.965 \n  \n\n 1164 \n    clean \n    1 \n    0.965 \n  \n\n 1165 \n    clearly \n    1 \n    0.965 \n  \n\n 1166 \n    clears \n    1 \n    0.965 \n  \n\n 1167 \n    cleeps \n    1 \n    0.965 \n  \n\n 1168 \n    clenched \n    1 \n    0.965 \n  \n\n 1169 \n    climax \n    1 \n    0.965 \n  \n\n 1170 \n    climbing \n    1 \n    0.965 \n  \n\n 1171 \n    clogged \n    1 \n    0.965 \n  \n\n 1172 \n    cloth \n    1 \n    0.965 \n  \n\n 1173 \n    clothes \n    1 \n    0.965 \n  \n\n 1174 \n    cloudless \n    1 \n    0.965 \n  \n\n 1175 \n    clouds \n    1 \n    0.965 \n  \n\n 1176 \n    clowns \n    1 \n    0.965 \n  \n\n 1177 \n    coarse \n    1 \n    0.965 \n  \n\n 1178 \n    coarsely \n    1 \n    0.965 \n  \n\n 1179 \n    coast \n    1 \n    0.965 \n  \n\n 1180 \n    coastline \n    1 \n    0.965 \n  \n\n 1181 \n    coax \n    1 \n    0.965 \n  \n\n 1182 \n    cobs \n    1 \n    0.965 \n  \n\n 1183 \n    cocaine \n    1 \n    0.965 \n  \n\n 1184 \n    collapse \n    1 \n    0.965 \n  \n\n 1185 \n    collide \n    1 \n    0.965 \n  \n\n 1186 \n    colonies \n    1 \n    0.965 \n  \n\n 1187 \n    colonize \n    1 \n    0.965 \n  \n\n 1188 \n    columbus \n    1 \n    0.965 \n  \n\n 1189 \n    comedy \n    1 \n    0.965 \n  \n\n 1190 \n    comely \n    1 \n    0.965 \n  \n\n 1191 \n    command \n    1 \n    0.965 \n  \n\n 1192 \n    commer-, \n    1 \n    0.965 \n  \n\n 1193 \n    common \n    1 \n    0.965 \n  \n\n 1194 \n    compassion \n    1 \n    0.965 \n  \n\n 1195 \n    complete \n    1 \n    0.965 \n  \n\n 1196 \n    composed \n    1 \n    0.965 \n  \n\n 1197 \n    compound \n    1 \n    0.965 \n  \n\n 1198 \n    concealing \n    1 \n    0.965 \n  \n\n 1199 \n    concentration \n    1 \n    0.965 \n  \n\n 1200 \n    concerned \n    1 \n    0.965 \n  \n\n 1201 \n    conclusion \n    1 \n    0.965 \n  \n\n 1202 \n    conference \n    1 \n    0.965 \n  \n\n 1203 \n    confidently \n    1 \n    0.965 \n  \n\n 1204 \n    confront \n    1 \n    0.965 \n  \n\n 1205 \n    confronting \n    1 \n    0.965 \n  \n\n 1206 \n    confuses \n    1 \n    0.965 \n  \n\n 1207 \n    congealed \n    1 \n    0.965 \n  \n\n 1208 \n    conjures \n    1 \n    0.965 \n  \n\n 1209 \n    connect \n    1 \n    0.965 \n  \n\n 1210 \n    connection \n    1 \n    0.965 \n  \n\n 1211 \n    conquistador \n    1 \n    0.965 \n  \n\n 1212 \n    consciousness \n    1 \n    0.965 \n  \n\n 1213 \n    consideration \n    1 \n    0.965 \n  \n\n 1214 \n    considering \n    1 \n    0.965 \n  \n\n 1215 \n    conspiracy \n    1 \n    0.965 \n  \n\n 1216 \n    content \n    1 \n    0.965 \n  \n\n 1217 \n    context \n    1 \n    0.965 \n  \n\n 1218 \n    continues \n    1 \n    0.965 \n  \n\n 1219 \n    convinced \n    1 \n    0.965 \n  \n\n 1220 \n    coolies \n    1 \n    0.965 \n  \n\n 1221 \n    coral \n    1 \n    0.965 \n  \n\n 1222 \n    core \n    1 \n    0.965 \n  \n\n 1223 \n    cork \n    1 \n    0.965 \n  \n\n 1224 \n    corpse \n    1 \n    0.965 \n  \n\n 1225 \n    cosmos \n    1 \n    0.965 \n  \n\n 1226 \n    cougars \n    1 \n    0.965 \n  \n\n 1227 \n    count \n    1 \n    0.965 \n  \n\n 1228 \n    counted \n    1 \n    0.965 \n  \n\n 1229 \n    counterfeit \n    1 \n    0.965 \n  \n\n 1230 \n    country \n    1 \n    0.965 \n  \n\n 1231 \n    courtship \n    1 \n    0.965 \n  \n\n 1232 \n    cousin \n    1 \n    0.965 \n  \n\n 1233 \n    cover \n    1 \n    0.965 \n  \n\n 1234 \n    cowardice \n    1 \n    0.965 \n  \n\n 1235 \n    cowrie \n    1 \n    0.965 \n  \n\n 1236 \n    cows \n    1 \n    0.965 \n  \n\n 1237 \n    crab-back \n    1 \n    0.965 \n  \n\n 1238 \n    crazy \n    1 \n    0.965 \n  \n\n 1239 \n    creased \n    1 \n    0.965 \n  \n\n 1240 \n    created \n    1 \n    0.965 \n  \n\n 1241 \n    creates \n    1 \n    0.965 \n  \n\n 1242 \n    creating \n    1 \n    0.965 \n  \n\n 1243 \n    creature \n    1 \n    0.965 \n  \n\n 1244 \n    creolization \n    1 \n    0.965 \n  \n\n 1245 \n    crest \n    1 \n    0.965 \n  \n\n 1246 \n    crevices \n    1 \n    0.965 \n  \n\n 1247 \n    cried \n    1 \n    0.965 \n  \n\n 1248 \n    crimson \n    1 \n    0.965 \n  \n\n 1249 \n    criseyde's \n    1 \n    0.965 \n  \n\n 1250 \n    criteria \n    1 \n    0.965 \n  \n\n 1251 \n    critic \n    1 \n    0.965 \n  \n\n 1252 \n    critical \n    1 \n    0.965 \n  \n\n 1253 \n    crop \n    1 \n    0.965 \n  \n\n 1254 \n    cross-legged \n    1 \n    0.965 \n  \n\n 1255 \n    crossed \n    1 \n    0.965 \n  \n\n 1256 \n    crowd \n    1 \n    0.965 \n  \n\n 1257 \n    crucifixes \n    1 \n    0.965 \n  \n\n 1258 \n    crudest \n    1 \n    0.965 \n  \n\n 1259 \n    crumbling \n    1 \n    0.965 \n  \n\n 1260 \n    cubla \n    1 \n    0.965 \n  \n\n 1261 \n    cuff \n    1 \n    0.965 \n  \n\n 1262 \n    cultural \n    1 \n    0.965 \n  \n\n 1263 \n    cunning \n    1 \n    0.965 \n  \n\n 1264 \n    cunt \n    1 \n    0.965 \n  \n\n 1265 \n    cunt-doll \n    1 \n    0.965 \n  \n\n 1266 \n    cup \n    1 \n    0.965 \n  \n\n 1267 \n    curls \n    1 \n    0.965 \n  \n\n 1268 \n    currents \n    1 \n    0.965 \n  \n\n 1269 \n    curry \n    1 \n    0.965 \n  \n\n 1270 \n    curson \n    1 \n    0.965 \n  \n\n 1271 \n    curves \n    1 \n    0.965 \n  \n\n 1272 \n    cut \n    1 \n    0.965 \n  \n\n 1273 \n    d-phil \n    1 \n    0.965 \n  \n\n 1274 \n    dams \n    1 \n    0.965 \n  \n\n 1275 \n    daniel \n    1 \n    0.965 \n  \n\n 1276 \n    dark \n    1 \n    0.965 \n  \n\n 1277 \n    dart \n    1 \n    0.965 \n  \n\n 1278 \n    dawn \n    1 \n    0.965 \n  \n\n 1279 \n    day's \n    1 \n    0.965 \n  \n\n 1280 \n    dazzle \n    1 \n    0.965 \n  \n\n 1281 \n    debauchery \n    1 \n    0.965 \n  \n\n 1282 \n    debt \n    1 \n    0.965 \n  \n\n 1283 \n    decipher \n    1 \n    0.965 \n  \n\n 1284 \n    decorative \n    1 \n    0.965 \n  \n\n 1285 \n    deed \n    1 \n    0.965 \n  \n\n 1286 \n    deeds \n    1 \n    0.965 \n  \n\n 1287 \n    defensive \n    1 \n    0.965 \n  \n\n 1288 \n    defined \n    1 \n    0.965 \n  \n\n 1289 \n    defining \n    1 \n    0.965 \n  \n\n 1290 \n    definite \n    1 \n    0.965 \n  \n\n 1291 \n    defoe \n    1 \n    0.965 \n  \n\n 1292 \n    defoe's \n    1 \n    0.965 \n  \n\n 1293 \n    degradation \n    1 \n    0.965 \n  \n\n 1294 \n    degrees \n    1 \n    0.965 \n  \n\n 1295 \n    deliberately \n    1 \n    0.965 \n  \n\n 1296 \n    delivered \n    1 \n    0.965 \n  \n\n 1297 \n    demons \n    1 \n    0.965 \n  \n\n 1298 \n    deny \n    1 \n    0.965 \n  \n\n 1299 \n    depths \n    1 \n    0.965 \n  \n\n 1300 \n    descriptions \n    1 \n    0.965 \n  \n\n 1301 \n    deserve \n    1 \n    0.965 \n  \n\n 1302 \n    design \n    1 \n    0.965 \n  \n\n 1303 \n    destination \n    1 \n    0.965 \n  \n\n 1304 \n    destroy \n    1 \n    0.965 \n  \n\n 1305 \n    detects \n    1 \n    0.965 \n  \n\n 1306 \n    devices \n    1 \n    0.965 \n  \n\n 1307 \n    devoted \n    1 \n    0.965 \n  \n\n 1308 \n    devotion \n    1 \n    0.965 \n  \n\n 1309 \n    diamond \n    1 \n    0.965 \n  \n\n 1310 \n    diet \n    1 \n    0.965 \n  \n\n 1311 \n    digging \n    1 \n    0.965 \n  \n\n 1312 \n    diminished \n    1 \n    0.965 \n  \n\n 1313 \n    diomede's \n    1 \n    0.965 \n  \n\n 1314 \n    dips \n    1 \n    0.965 \n  \n\n 1315 \n    directly \n    1 \n    0.965 \n  \n\n 1316 \n    dirt \n    1 \n    0.965 \n  \n\n 1317 \n    dirty \n    1 \n    0.965 \n  \n\n 1318 \n    disappear \n    1 \n    0.965 \n  \n\n 1319 \n    disappeared \n    1 \n    0.965 \n  \n\n 1320 \n    disappears \n    1 \n    0.965 \n  \n\n 1321 \n    discernible \n    1 \n    0.965 \n  \n\n 1322 \n    dishevelled \n    1 \n    0.965 \n  \n\n 1323 \n    disperse \n    1 \n    0.965 \n  \n\n 1324 \n    display \n    1 \n    0.965 \n  \n\n 1325 \n    dissertation \n    1 \n    0.965 \n  \n\n 1326 \n    dissolve \n    1 \n    0.965 \n  \n\n 1327 \n    divided \n    1 \n    0.965 \n  \n\n 1328 \n    done \n    1 \n    0.965 \n  \n\n 1329 \n    door \n    1 \n    0.965 \n  \n\n 1330 \n    doorways \n    1 \n    0.965 \n  \n\n 1331 \n    dough \n    1 \n    0.965 \n  \n\n 1332 \n    douse \n    1 \n    0.965 \n  \n\n 1333 \n    dozen \n    1 \n    0.965 \n  \n\n 1334 \n    drank \n    1 \n    0.965 \n  \n\n 1335 \n    dread \n    1 \n    0.965 \n  \n\n 1336 \n    dreadful \n    1 \n    0.965 \n  \n\n 1337 \n    dream \n    1 \n    0.965 \n  \n\n 1338 \n    dreamed \n    1 \n    0.965 \n  \n\n 1339 \n    dregs \n    1 \n    0.965 \n  \n\n 1340 \n    dress \n    1 \n    0.965 \n  \n\n 1341 \n    drew \n    1 \n    0.965 \n  \n\n 1342 \n    drifting \n    1 \n    0.965 \n  \n\n 1343 \n    drink \n    1 \n    0.965 \n  \n\n 1344 \n    drives \n    1 \n    0.965 \n  \n\n 1345 \n    dropped \n    1 \n    0.965 \n  \n\n 1346 \n    droppings \n    1 \n    0.965 \n  \n\n 1347 \n    dug \n    1 \n    0.965 \n  \n\n 1348 \n    dull \n    1 \n    0.965 \n  \n\n 1349 \n    dumping \n    1 \n    0.965 \n  \n\n 1350 \n    dung \n    1 \n    0.965 \n  \n\n 1351 \n    during \n    1 \n    0.965 \n  \n\n 1352 \n    dutifully \n    1 \n    0.965 \n  \n\n 1353 \n    dying \n    1 \n    0.965 \n  \n\n 1354 \n    e-, \n    1 \n    0.965 \n  \n\n 1355 \n    earlier \n    1 \n    0.965 \n  \n\n 1356 \n    ease \n    1 \n    0.965 \n  \n\n 1357 \n    easier \n    1 \n    0.965 \n  \n\n 1358 \n    eaten \n    1 \n    0.965 \n  \n\n 1359 \n    ecstatic \n    1 \n    0.965 \n  \n\n 1360 \n    edit \n    1 \n    0.965 \n  \n\n 1361 \n    eggs \n    1 \n    0.965 \n  \n\n 1362 \n    eight \n    1 \n    0.965 \n  \n\n 1363 \n    eighteen \n    1 \n    0.965 \n  \n\n 1364 \n    eighteen- \n    1 \n    0.965 \n  \n\n 1365 \n    either \n    1 \n    0.965 \n  \n\n 1366 \n    elapsed \n    1 \n    0.965 \n  \n\n 1367 \n    electronic \n    1 \n    0.965 \n  \n\n 1368 \n    eloquently \n    1 \n    0.965 \n  \n\n 1369 \n    else \n    1 \n    0.965 \n  \n\n 1370 \n    emerges \n    1 \n    0.965 \n  \n\n 1371 \n    empire \n    1 \n    0.965 \n  \n\n 1372 \n    emptiness \n    1 \n    0.965 \n  \n\n 1373 \n    endless \n    1 \n    0.965 \n  \n\n 1374 \n    endow \n    1 \n    0.965 \n  \n\n 1375 \n    endowed \n    1 \n    0.965 \n  \n\n 1376 \n    enfolding \n    1 \n    0.965 \n  \n\n 1377 \n    engrossed \n    1 \n    0.965 \n  \n\n 1378 \n    enrich \n    1 \n    0.965 \n  \n\n 1379 \n    enslaved \n    1 \n    0.965 \n  \n\n 1380 \n    entrance \n    1 \n    0.965 \n  \n\n 1381 \n    entreaties \n    1 \n    0.965 \n  \n\n 1382 \n    epic \n    1 \n    0.965 \n  \n\n 1383 \n    epistemological \n    1 \n    0.965 \n  \n\n 1384 \n    equal \n    1 \n    0.965 \n  \n\n 1385 \n    eradication \n    1 \n    0.965 \n  \n\n 1386 \n    erases \n    1 \n    0.965 \n  \n\n 1387 \n    escarpments \n    1 \n    0.965 \n  \n\n 1388 \n    eskimos \n    1 \n    0.965 \n  \n\n 1389 \n    essay \n    1 \n    0.965 \n  \n\n 1390 \n    evening \n    1 \n    0.965 \n  \n\n 1391 \n    everybody \n    1 \n    0.965 \n  \n\n 1392 \n    evil \n    1 \n    0.965 \n  \n\n 1393 \n    evoke \n    1 \n    0.965 \n  \n\n 1394 \n    ex-student \n    1 \n    0.965 \n  \n\n 1395 \n    examination \n    1 \n    0.965 \n  \n\n 1396 \n    examine \n    1 \n    0.965 \n  \n\n 1397 \n    examined \n    1 \n    0.965 \n  \n\n 1398 \n    examining \n    1 \n    0.965 \n  \n\n 1399 \n    exchanges \n    1 \n    0.965 \n  \n\n 1400 \n    excitement \n    1 \n    0.965 \n  \n\n 1401 \n    exercised \n    1 \n    0.965 \n  \n\n 1402 \n    exhaustion \n    1 \n    0.965 \n  \n\n 1403 \n    exotic \n    1 \n    0.965 \n  \n\n 1404 \n    explores \n    1 \n    0.965 \n  \n\n 1405 \n    exploring \n    1 \n    0.965 \n  \n\n 1406 \n    exposed \n    1 \n    0.965 \n  \n\n 1407 \n    extravagant \n    1 \n    0.965 \n  \n\n 1408 \n    failed \n    1 \n    0.965 \n  \n\n 1409 \n    faithful \n    1 \n    0.965 \n  \n\n 1410 \n    falling \n    1 \n    0.965 \n  \n\n 1411 \n    familiar \n    1 \n    0.965 \n  \n\n 1412 \n    familiarity \n    1 \n    0.965 \n  \n\n 1413 \n    fanned \n    1 \n    0.965 \n  \n\n 1414 \n    fanning \n    1 \n    0.965 \n  \n\n 1415 \n    fantastic \n    1 \n    0.965 \n  \n\n 1416 \n    fare \n    1 \n    0.965 \n  \n\n 1417 \n    fascist \n    1 \n    0.965 \n  \n\n 1418 \n    fashion \n    1 \n    0.965 \n  \n\n 1419 \n    fate \n    1 \n    0.965 \n  \n\n 1420 \n    fatten \n    1 \n    0.965 \n  \n\n 1421 \n    fattening \n    1 \n    0.965 \n  \n\n 1422 \n    favour \n    1 \n    0.965 \n  \n\n 1423 \n    favourite \n    1 \n    0.965 \n  \n\n 1424 \n    favours \n    1 \n    0.965 \n  \n\n 1425 \n    fearful \n    1 \n    0.965 \n  \n\n 1426 \n    feel \n    1 \n    0.965 \n  \n\n 1427 \n    feign \n    1 \n    0.965 \n  \n\n 1428 \n    fell \n    1 \n    0.965 \n  \n\n 1429 \n    fence \n    1 \n    0.965 \n  \n\n 1430 \n    fidgeting \n    1 \n    0.965 \n  \n\n 1431 \n    field \n    1 \n    0.965 \n  \n\n 1432 \n    fiercely \n    1 \n    0.965 \n  \n\n 1433 \n    fifteen-thousand \n    1 \n    0.965 \n  \n\n 1434 \n    fifty-thousand-million \n    1 \n    0.965 \n  \n\n 1435 \n    file \n    1 \n    0.965 \n  \n\n 1436 \n    filled \n    1 \n    0.965 \n  \n\n 1437 \n    filling \n    1 \n    0.965 \n  \n\n 1438 \n    film \n    1 \n    0.965 \n  \n\n 1439 \n    filter \n    1 \n    0.965 \n  \n\n 1440 \n    fine \n    1 \n    0.965 \n  \n\n 1441 \n    fingers \n    1 \n    0.965 \n  \n\n 1442 \n    finish \n    1 \n    0.965 \n  \n\n 1443 \n    finished \n    1 \n    0.965 \n  \n\n 1444 \n    fireside \n    1 \n    0.965 \n  \n\n 1445 \n    fished \n    1 \n    0.965 \n  \n\n 1446 \n    fist \n    1 \n    0.965 \n  \n\n 1447 \n    fit \n    1 \n    0.965 \n  \n\n 1448 \n    five-hundredth \n    1 \n    0.965 \n  \n\n 1449 \n    flames \n    1 \n    0.965 \n  \n\n 1450 \n    flashed \n    1 \n    0.965 \n  \n\n 1451 \n    flat \n    1 \n    0.965 \n  \n\n 1452 \n    flayed \n    1 \n    0.965 \n  \n\n 1453 \n    flecks \n    1 \n    0.965 \n  \n\n 1454 \n    flicked \n    1 \n    0.965 \n  \n\n 1455 \n    flicking \n    1 \n    0.965 \n  \n\n 1456 \n    flies \n    1 \n    0.965 \n  \n\n 1457 \n    float \n    1 \n    0.965 \n  \n\n 1458 \n    flood \n    1 \n    0.965 \n  \n\n 1459 \n    flooding \n    1 \n    0.965 \n  \n\n 1460 \n    flowers \n    1 \n    0.965 \n  \n\n 1461 \n    flowers' \n    1 \n    0.965 \n  \n\n 1462 \n    flush \n    1 \n    0.965 \n  \n\n 1463 \n    fly \n    1 \n    0.965 \n  \n\n 1464 \n    foil \n    1 \n    0.965 \n  \n\n 1465 \n    foliage \n    1 \n    0.965 \n  \n\n 1466 \n    followed \n    1 \n    0.965 \n  \n\n 1467 \n    fondled \n    1 \n    0.965 \n  \n\n 1468 \n    food \n    1 \n    0.965 \n  \n\n 1469 \n    footstep \n    1 \n    0.965 \n  \n\n 1470 \n    forbidden \n    1 \n    0.965 \n  \n\n 1471 \n    force \n    1 \n    0.965 \n  \n\n 1472 \n    foreground \n    1 \n    0.965 \n  \n\n 1473 \n    forehead \n    1 \n    0.965 \n  \n\n 1474 \n    forget \n    1 \n    0.965 \n  \n\n 1475 \n    forgive \n    1 \n    0.965 \n  \n\n 1476 \n    forgot \n    1 \n    0.965 \n  \n\n 1477 \n    forgotten \n    1 \n    0.965 \n  \n\n 1478 \n    forlornly \n    1 \n    0.965 \n  \n\n 1479 \n    forms \n    1 \n    0.965 \n  \n\n 1480 \n    forth \n    1 \n    0.965 \n  \n\n 1481 \n    fortifications \n    1 \n    0.965 \n  \n\n 1482 \n    fountain \n    1 \n    0.965 \n  \n\n 1483 \n    fountaining \n    1 \n    0.965 \n  \n\n 1484 \n    fours \n    1 \n    0.965 \n  \n\n 1485 \n    frail \n    1 \n    0.965 \n  \n\n 1486 \n    frangipani \n    1 \n    0.965 \n  \n\n 1487 \n    freshly \n    1 \n    0.965 \n  \n\n 1488 \n    freshness \n    1 \n    0.965 \n  \n\n 1489 \n    fried \n    1 \n    0.965 \n  \n\n 1490 \n    friends \n    1 \n    0.965 \n  \n\n 1491 \n    frig-, \n    1 \n    0.965 \n  \n\n 1492 \n    frigates \n    1 \n    0.965 \n  \n\n 1493 \n    frigged \n    1 \n    0.965 \n  \n\n 1494 \n    front \n    1 \n    0.965 \n  \n\n 1495 \n    froze \n    1 \n    0.965 \n  \n\n 1496 \n    fuck-arse \n    1 \n    0.965 \n  \n\n 1497 \n    fumbled \n    1 \n    0.965 \n  \n\n 1498 \n    further \n    1 \n    0.965 \n  \n\n 1499 \n    gaiety \n    1 \n    0.965 \n  \n\n 1500 \n    game \n    1 \n    0.965 \n  \n\n 1501 \n    gang \n    1 \n    0.965 \n  \n\n 1502 \n    garden \n    1 \n    0.965 \n  \n\n 1503 \n    gardeners \n    1 \n    0.965 \n  \n\n 1504 \n    gasps \n    1 \n    0.965 \n  \n\n 1505 \n    gate \n    1 \n    0.965 \n  \n\n 1506 \n    gather \n    1 \n    0.965 \n  \n\n 1507 \n    gaudy \n    1 \n    0.965 \n  \n\n 1508 \n    generally \n    1 \n    0.965 \n  \n\n 1509 \n    generations \n    1 \n    0.965 \n  \n\n 1510 \n    gentle \n    1 \n    0.965 \n  \n\n 1511 \n    genuinely \n    1 \n    0.965 \n  \n\n 1512 \n    germans \n    1 \n    0.965 \n  \n\n 1513 \n    germany \n    1 \n    0.965 \n  \n\n 1514 \n    gestures \n    1 \n    0.965 \n  \n\n 1515 \n    gifts \n    1 \n    0.965 \n  \n\n 1516 \n    gilmore \n    1 \n    0.965 \n  \n\n 1517 \n    gilmore's \n    1 \n    0.965 \n  \n\n 1518 \n    girl's \n    1 \n    0.965 \n  \n\n 1519 \n    give \n    1 \n    0.965 \n  \n\n 1520 \n    gives \n    1 \n    0.965 \n  \n\n 1521 \n    glad \n    1 \n    0.965 \n  \n\n 1522 \n    glanced \n    1 \n    0.965 \n  \n\n 1523 \n    glances \n    1 \n    0.965 \n  \n\n 1524 \n    glared \n    1 \n    0.965 \n  \n\n 1525 \n    gleaming \n    1 \n    0.965 \n  \n\n 1526 \n    glimpses \n    1 \n    0.965 \n  \n\n 1527 \n    glittering \n    1 \n    0.965 \n  \n\n 1528 \n    globe \n    1 \n    0.965 \n  \n\n 1529 \n    gloomy \n    1 \n    0.965 \n  \n\n 1530 \n    glow \n    1 \n    0.965 \n  \n\n 1531 \n    gobble \n    1 \n    0.965 \n  \n\n 1532 \n    god's \n    1 \n    0.965 \n  \n\n 1533 \n    golding \n    1 \n    0.965 \n  \n\n 1534 \n    gone \n    1 \n    0.965 \n  \n\n 1535 \n    goods \n    1 \n    0.965 \n  \n\n 1536 \n    gorgeous \n    1 \n    0.965 \n  \n\n 1537 \n    gothic \n    1 \n    0.965 \n  \n\n 1538 \n    gouging \n    1 \n    0.965 \n  \n\n 1539 \n    grabbed \n    1 \n    0.965 \n  \n\n 1540 \n    gradual \n    1 \n    0.965 \n  \n\n 1541 \n    grandpa's \n    1 \n    0.965 \n  \n\n 1542 \n    grap-, \n    1 \n    0.965 \n  \n\n 1543 \n    grappled \n    1 \n    0.965 \n  \n\n 1544 \n    graves \n    1 \n    0.965 \n  \n\n 1545 \n    gravestone \n    1 \n    0.965 \n  \n\n 1546 \n    gravestones \n    1 \n    0.965 \n  \n\n 1547 \n    graveyard \n    1 \n    0.965 \n  \n\n 1548 \n    graze \n    1 \n    0.965 \n  \n\n 1549 \n    gri-, \n    1 \n    0.965 \n  \n\n 1550 \n    grief \n    1 \n    0.965 \n  \n\n 1551 \n    grievance \n    1 \n    0.965 \n  \n\n 1552 \n    grin \n    1 \n    0.965 \n  \n\n 1553 \n    gripping \n    1 \n    0.965 \n  \n\n 1554 \n    grope \n    1 \n    0.965 \n  \n\n 1555 \n    gropes \n    1 \n    0.965 \n  \n\n 1556 \n    grunted \n    1 \n    0.965 \n  \n\n 1557 \n    grunting \n    1 \n    0.965 \n  \n\n 1558 \n    guarded \n    1 \n    0.965 \n  \n\n 1559 \n    gulped \n    1 \n    0.965 \n  \n\n 1560 \n    guy-, \n    1 \n    0.965 \n  \n\n 1561 \n    ha-, \n    1 \n    0.965 \n  \n\n 1562 \n    habituated \n    1 \n    0.965 \n  \n\n 1563 \n    halfway \n    1 \n    0.965 \n  \n\n 1564 \n    handles \n    1 \n    0.965 \n  \n\n 1565 \n    hang \n    1 \n    0.965 \n  \n\n 1566 \n    happen \n    1 \n    0.965 \n  \n\n 1567 \n    hardening \n    1 \n    0.965 \n  \n\n 1568 \n    harvest \n    1 \n    0.965 \n  \n\n 1569 \n    harvests \n    1 \n    0.965 \n  \n\n 1570 \n    haughty \n    1 \n    0.965 \n  \n\n 1571 \n    haven \n    1 \n    0.965 \n  \n\n 1572 \n    hazarded \n    1 \n    0.965 \n  \n\n 1573 \n    headed \n    1 \n    0.965 \n  \n\n 1574 \n    headset \n    1 \n    0.965 \n  \n\n 1575 \n    headstone \n    1 \n    0.965 \n  \n\n 1576 \n    heal \n    1 \n    0.965 \n  \n\n 1577 \n    healed \n    1 \n    0.965 \n  \n\n 1578 \n    hearts \n    1 \n    0.965 \n  \n\n 1579 \n    heathen \n    1 \n    0.965 \n  \n\n 1580 \n    heavy \n    1 \n    0.965 \n  \n\n 1581 \n    heed \n    1 \n    0.965 \n  \n\n 1582 \n    hence \n    1 \n    0.965 \n  \n\n 1583 \n    hercules \n    1 \n    0.965 \n  \n\n 1584 \n    herd \n    1 \n    0.965 \n  \n\n 1585 \n    heritage \n    1 \n    0.965 \n  \n\n 1586 \n    hers \n    1 \n    0.965 \n  \n\n 1587 \n    hibiscus \n    1 \n    0.965 \n  \n\n 1588 \n    hides \n    1 \n    0.965 \n  \n\n 1589 \n    high \n    1 \n    0.965 \n  \n\n 1590 \n    higher \n    1 \n    0.965 \n  \n\n 1591 \n    hispaniola \n    1 \n    0.965 \n  \n\n 1592 \n    hitherto \n    1 \n    0.965 \n  \n\n 1593 \n    hoes \n    1 \n    0.965 \n  \n\n 1594 \n    hog \n    1 \n    0.965 \n  \n\n 1595 \n    hogarth's \n    1 \n    0.965 \n  \n\n 1596 \n    hold \n    1 \n    0.965 \n  \n\n 1597 \n    holding \n    1 \n    0.965 \n  \n\n 1598 \n    honour \n    1 \n    0.965 \n  \n\n 1599 \n    hooked \n    1 \n    0.965 \n  \n\n 1600 \n    hoping \n    1 \n    0.965 \n  \n\n 1601 \n    horrible \n    1 \n    0.965 \n  \n\n 1602 \n    horse \n    1 \n    0.965 \n  \n\n 1603 \n    hospital \n    1 \n    0.965 \n  \n\n 1604 \n    hot \n    1 \n    0.965 \n  \n\n 1605 \n    hue \n    1 \n    0.965 \n  \n\n 1606 \n    hugging \n    1 \n    0.965 \n  \n\n 1607 \n    huh \n    1 \n    0.965 \n  \n\n 1608 \n    humiliated \n    1 \n    0.965 \n  \n\n 1609 \n    hundred-and-fifty \n    1 \n    0.965 \n  \n\n 1610 \n    hungry \n    1 \n    0.965 \n  \n\n 1611 \n    hunter \n    1 \n    0.965 \n  \n\n 1612 \n    hunting \n    1 \n    0.965 \n  \n\n 1613 \n    husband \n    1 \n    0.965 \n  \n\n 1614 \n    i'd \n    1 \n    0.965 \n  \n\n 1615 \n    ice \n    1 \n    0.965 \n  \n\n 1616 \n    ideologically \n    1 \n    0.965 \n  \n\n 1617 \n    idle \n    1 \n    0.965 \n  \n\n 1618 \n    idly \n    1 \n    0.965 \n  \n\n 1619 \n    ignorantly \n    1 \n    0.965 \n  \n\n 1620 \n    illegal \n    1 \n    0.965 \n  \n\n 1621 \n    illusion \n    1 \n    0.965 \n  \n\n 1622 \n    imaginations \n    1 \n    0.965 \n  \n\n 1623 \n    imagined \n    1 \n    0.965 \n  \n\n 1624 \n    imagines \n    1 \n    0.965 \n  \n\n 1625 \n    immediately \n    1 \n    0.965 \n  \n\n 1626 \n    immensities \n    1 \n    0.965 \n  \n\n 1627 \n    immortalize \n    1 \n    0.965 \n  \n\n 1628 \n    imperiously \n    1 \n    0.965 \n  \n\n 1629 \n    implanted \n    1 \n    0.965 \n  \n\n 1630 \n    impregnated \n    1 \n    0.965 \n  \n\n 1631 \n    impressing \n    1 \n    0.965 \n  \n\n 1632 \n    imprint \n    1 \n    0.965 \n  \n\n 1633 \n    imprisoned \n    1 \n    0.965 \n  \n\n 1634 \n    including \n    1 \n    0.965 \n  \n\n 1635 \n    indelicately \n    1 \n    0.965 \n  \n\n 1636 \n    indented \n    1 \n    0.965 \n  \n\n 1637 \n    indians \n    1 \n    0.965 \n  \n\n 1638 \n    indies \n    1 \n    0.965 \n  \n\n 1639 \n    indifferently \n    1 \n    0.965 \n  \n\n 1640 \n    individual \n    1 \n    0.965 \n  \n\n 1641 \n    infested \n    1 \n    0.965 \n  \n\n 1642 \n    influenced \n    1 \n    0.965 \n  \n\n 1643 \n    ing-, \n    1 \n    0.965 \n  \n\n 1644 \n    injured \n    1 \n    0.965 \n  \n\n 1645 \n    innocent \n    1 \n    0.965 \n  \n\n 1646 \n    inscribed \n    1 \n    0.965 \n  \n\n 1647 \n    inside \n    1 \n    0.965 \n  \n\n 1648 \n    insisted \n    1 \n    0.965 \n  \n\n 1649 \n    instinct \n    1 \n    0.965 \n  \n\n 1650 \n    instruct \n    1 \n    0.965 \n  \n\n 1651 \n    instruction \n    1 \n    0.965 \n  \n\n 1652 \n    instrument \n    1 \n    0.965 \n  \n\n 1653 \n    insufficient \n    1 \n    0.965 \n  \n\n 1654 \n    insurance \n    1 \n    0.965 \n  \n\n 1655 \n    intellectually \n    1 \n    0.965 \n  \n\n 1656 \n    interior \n    1 \n    0.965 \n  \n\n 1657 \n    internal \n    1 \n    0.965 \n  \n\n 1658 \n    interrupt \n    1 \n    0.965 \n  \n\n 1659 \n    invent \n    1 \n    0.965 \n  \n\n 1660 \n    inventions \n    1 \n    0.965 \n  \n\n 1661 \n    invocations \n    1 \n    0.965 \n  \n\n 1662 \n    involuntarily \n    1 \n    0.965 \n  \n\n 1663 \n    iris \n    1 \n    0.965 \n  \n\n 1664 \n    iron \n    1 \n    0.965 \n  \n\n 1665 \n    isn't \n    1 \n    0.965 \n  \n\n 1666 \n    italian \n    1 \n    0.965 \n  \n\n 1667 \n    itch \n    1 \n    0.965 \n  \n\n 1668 \n    itself \n    1 \n    0.965 \n  \n\n 1669 \n    jacket \n    1 \n    0.965 \n  \n\n 1670 \n    jamaica \n    1 \n    0.965 \n  \n\n 1671 \n    jaws \n    1 \n    0.965 \n  \n\n 1672 \n    jellyfish \n    1 \n    0.965 \n  \n\n 1673 \n    jest \n    1 \n    0.965 \n  \n\n 1674 \n    jesus \n    1 \n    0.965 \n  \n\n 1675 \n    jhal \n    1 \n    0.965 \n  \n\n 1676 \n    jocularly \n    1 \n    0.965 \n  \n\n 1677 \n    john \n    1 \n    0.965 \n  \n\n 1678 \n    johnson \n    1 \n    0.965 \n  \n\n 1679 \n    joined \n    1 \n    0.965 \n  \n\n 1680 \n    journal \n    1 \n    0.965 \n  \n\n 1681 \n    jubilation \n    1 \n    0.965 \n  \n\n 1682 \n    jump \n    1 \n    0.965 \n  \n\n 1683 \n    jumps \n    1 \n    0.965 \n  \n\n 1684 \n    keeper \n    1 \n    0.965 \n  \n\n 1685 \n    kept \n    1 \n    0.965 \n  \n\n 1686 \n    kick \n    1 \n    0.965 \n  \n\n 1687 \n    kidnapping \n    1 \n    0.965 \n  \n\n 1688 \n    kill \n    1 \n    0.965 \n  \n\n 1689 \n    killing \n    1 \n    0.965 \n  \n\n 1690 \n    kingdom \n    1 \n    0.965 \n  \n\n 1691 \n    kisses \n    1 \n    0.965 \n  \n\n 1692 \n    knight \n    1 \n    0.965 \n  \n\n 1693 \n    knives \n    1 \n    0.965 \n  \n\n 1694 \n    knows \n    1 \n    0.965 \n  \n\n 1695 \n    kwesi \n    1 \n    0.965 \n  \n\n 1696 \n    labba \n    1 \n    0.965 \n  \n\n 1697 \n    labouring \n    1 \n    0.965 \n  \n\n 1698 \n    lacked \n    1 \n    0.965 \n  \n\n 1699 \n    lame \n    1 \n    0.965 \n  \n\n 1700 \n    lamentation \n    1 \n    0.965 \n  \n\n 1701 \n    lance \n    1 \n    0.965 \n  \n\n 1702 \n    lands \n    1 \n    0.965 \n  \n\n 1703 \n    landscape \n    1 \n    0.965 \n  \n\n 1704 \n    languages \n    1 \n    0.965 \n  \n\n 1705 \n    lap \n    1 \n    0.965 \n  \n\n 1706 \n    large \n    1 \n    0.965 \n  \n\n 1707 \n    largely \n    1 \n    0.965 \n  \n\n 1708 \n    lashes \n    1 \n    0.965 \n  \n\n 1709 \n    late \n    1 \n    0.965 \n  \n\n 1710 \n    laughed \n    1 \n    0.965 \n  \n\n 1711 \n    laughter \n    1 \n    0.965 \n  \n\n 1712 \n    laurels \n    1 \n    0.965 \n  \n\n 1713 \n    lawn \n    1 \n    0.965 \n  \n\n 1714 \n    lays \n    1 \n    0.965 \n  \n\n 1715 \n    lea-, \n    1 \n    0.965 \n  \n\n 1716 \n    leading \n    1 \n    0.965 \n  \n\n 1717 \n    leaf \n    1 \n    0.965 \n  \n\n 1718 \n    lean \n    1 \n    0.965 \n  \n\n 1719 \n    learned \n    1 \n    0.965 \n  \n\n 1720 \n    leaves \n    1 \n    0.965 \n  \n\n 1721 \n    led \n    1 \n    0.965 \n  \n\n 1722 \n    legendary \n    1 \n    0.965 \n  \n\n 1723 \n    lesser \n    1 \n    0.965 \n  \n\n 1724 \n    lest \n    1 \n    0.965 \n  \n\n 1725 \n    letter \n    1 \n    0.965 \n  \n\n 1726 \n    lettering \n    1 \n    0.965 \n  \n\n 1727 \n    letterings \n    1 \n    0.965 \n  \n\n 1728 \n    letters \n    1 \n    0.965 \n  \n\n 1729 \n    level \n    1 \n    0.965 \n  \n\n 1730 \n    lewis \n    1 \n    0.965 \n  \n\n 1731 \n    liberally \n    1 \n    0.965 \n  \n\n 1732 \n    library \n    1 \n    0.965 \n  \n\n 1733 \n    lifeless \n    1 \n    0.965 \n  \n\n 1734 \n    lift \n    1 \n    0.965 \n  \n\n 1735 \n    lilies \n    1 \n    0.965 \n  \n\n 1736 \n    lined \n    1 \n    0.965 \n  \n\n 1737 \n    lingered \n    1 \n    0.965 \n  \n\n 1738 \n    linton \n    1 \n    0.965 \n  \n\n 1739 \n    lip \n    1 \n    0.965 \n  \n\n 1740 \n    liquids \n    1 \n    0.965 \n  \n\n 1741 \n    listen \n    1 \n    0.965 \n  \n\n 1742 \n    literary \n    1 \n    0.965 \n  \n\n 1743 \n    littered \n    1 \n    0.965 \n  \n\n 1744 \n    lived \n    1 \n    0.965 \n  \n\n 1745 \n    livid \n    1 \n    0.965 \n  \n\n 1746 \n    living \n    1 \n    0.965 \n  \n\n 1747 \n    livingness \n    1 \n    0.965 \n  \n\n 1748 \n    local \n    1 \n    0.965 \n  \n\n 1749 \n    lock \n    1 \n    0.965 \n  \n\n 1750 \n    lofty \n    1 \n    0.965 \n  \n\n 1751 \n    loneliness \n    1 \n    0.965 \n  \n\n 1752 \n    lonely \n    1 \n    0.965 \n  \n\n 1753 \n    longing \n    1 \n    0.965 \n  \n\n 1754 \n    longs \n    1 \n    0.965 \n  \n\n 1755 \n    looks \n    1 \n    0.965 \n  \n\n 1756 \n    loop \n    1 \n    0.965 \n  \n\n 1757 \n    loose \n    1 \n    0.965 \n  \n\n 1758 \n    loosed \n    1 \n    0.965 \n  \n\n 1759 \n    loosen \n    1 \n    0.965 \n  \n\n 1760 \n    loosening \n    1 \n    0.965 \n  \n\n 1761 \n    lopsided \n    1 \n    0.965 \n  \n\n 1762 \n    losses \n    1 \n    0.965 \n  \n\n 1763 \n    lots \n    1 \n    0.965 \n  \n\n 1764 \n    loud \n    1 \n    0.965 \n  \n\n 1765 \n    louder \n    1 \n    0.965 \n  \n\n 1766 \n    loved \n    1 \n    0.965 \n  \n\n 1767 \n    lover's \n    1 \n    0.965 \n  \n\n 1768 \n    loving \n    1 \n    0.965 \n  \n\n 1769 \n    lowers \n    1 \n    0.965 \n  \n\n 1770 \n    lucri-, \n    1 \n    0.965 \n  \n\n 1771 \n    lullaby \n    1 \n    0.965 \n  \n\n 1772 \n    lungs \n    1 \n    0.965 \n  \n\n 1773 \n    lurked \n    1 \n    0.965 \n  \n\n 1774 \n    lying \n    1 \n    0.965 \n  \n\n 1775 \n    m-, \n    1 \n    0.965 \n  \n\n 1776 \n    m-a \n    1 \n    0.965 \n  \n\n 1777 \n    machines \n    1 \n    0.965 \n  \n\n 1778 \n    magical \n    1 \n    0.965 \n  \n\n 1779 \n    magician \n    1 \n    0.965 \n  \n\n 1780 \n    maju \n    1 \n    0.965 \n  \n\n 1781 \n    male \n    1 \n    0.965 \n  \n\n 1782 \n    malice \n    1 \n    0.965 \n  \n\n 1783 \n    man's \n    1 \n    0.965 \n  \n\n 1784 \n    manner \n    1 \n    0.965 \n  \n\n 1785 \n    margins \n    1 \n    0.965 \n  \n\n 1786 \n    mark \n    1 \n    0.965 \n  \n\n 1787 \n    marked \n    1 \n    0.965 \n  \n\n 1788 \n    marks \n    1 \n    0.965 \n  \n\n 1789 \n    married \n    1 \n    0.965 \n  \n\n 1790 \n    marvelled \n    1 \n    0.965 \n  \n\n 1791 \n    mashing \n    1 \n    0.965 \n  \n\n 1792 \n    mask \n    1 \n    0.965 \n  \n\n 1793 \n    massage \n    1 \n    0.965 \n  \n\n 1794 \n    massive \n    1 \n    0.965 \n  \n\n 1795 \n    master's \n    1 \n    0.965 \n  \n\n 1796 \n    masterpiece \n    1 \n    0.965 \n  \n\n 1797 \n    materialized \n    1 \n    0.965 \n  \n\n 1798 \n    may \n    1 \n    0.965 \n  \n\n 1799 \n    maybe \n    1 \n    0.965 \n  \n\n 1800 \n    meaning \n    1 \n    0.965 \n  \n\n 1801 \n    means \n    1 \n    0.965 \n  \n\n 1802 \n    meant \n    1 \n    0.965 \n  \n\n 1803 \n    meantime \n    1 \n    0.965 \n  \n\n 1804 \n    meats \n    1 \n    0.965 \n  \n\n 1805 \n    meet \n    1 \n    0.965 \n  \n\n 1806 \n    mentio-, \n    1 \n    0.965 \n  \n\n 1807 \n    mere \n    1 \n    0.965 \n  \n\n 1808 \n    merely \n    1 \n    0.965 \n  \n\n 1809 \n    mid-ground \n    1 \n    0.965 \n  \n\n 1810 \n    mile \n    1 \n    0.965 \n  \n\n 1811 \n    mingling \n    1 \n    0.965 \n  \n\n 1812 \n    mispronouncing \n    1 \n    0.965 \n  \n\n 1813 \n    misses \n    1 \n    0.965 \n  \n\n 1814 \n    mock \n    1 \n    0.965 \n  \n\n 1815 \n    mocked \n    1 \n    0.965 \n  \n\n 1816 \n    monkeys \n    1 \n    0.965 \n  \n\n 1817 \n    morally \n    1 \n    0.965 \n  \n\n 1818 \n    mother's \n    1 \n    0.965 \n  \n\n 1819 \n    mothers \n    1 \n    0.965 \n  \n\n 1820 \n    motives \n    1 \n    0.965 \n  \n\n 1821 \n    mouldy \n    1 \n    0.965 \n  \n\n 1822 \n    mound \n    1 \n    0.965 \n  \n\n 1823 \n    movement \n    1 \n    0.965 \n  \n\n 1824 \n    movements \n    1 \n    0.965 \n  \n\n 1825 \n    moves \n    1 \n    0.965 \n  \n\n 1826 \n    mr \n    1 \n    0.965 \n  \n\n 1827 \n    multiply \n    1 \n    0.965 \n  \n\n 1828 \n    multiplying \n    1 \n    0.965 \n  \n\n 1829 \n    munificence \n    1 \n    0.965 \n  \n\n 1830 \n    murdered \n    1 \n    0.965 \n  \n\n 1831 \n    mushrooms \n    1 \n    0.965 \n  \n\n 1832 \n    muttering \n    1 \n    0.965 \n  \n\n 1833 \n    mysterious \n    1 \n    0.965 \n  \n\n 1834 \n    mysteriously \n    1 \n    0.965 \n  \n\n 1835 \n    nag \n    1 \n    0.965 \n  \n\n 1836 \n    naked \n    1 \n    0.965 \n  \n\n 1837 \n    named \n    1 \n    0.965 \n  \n\n 1838 \n    names \n    1 \n    0.965 \n  \n\n 1839 \n    narrow \n    1 \n    0.965 \n  \n\n 1840 \n    nationalistic \n    1 \n    0.965 \n  \n\n 1841 \n    nauseous \n    1 \n    0.965 \n  \n\n 1842 \n    nearly \n    1 \n    0.965 \n  \n\n 1843 \n    necessary \n    1 \n    0.965 \n  \n\n 1844 \n    necklaces \n    1 \n    0.965 \n  \n\n 1845 \n    necks \n    1 \n    0.965 \n  \n\n 1846 \n    need \n    1 \n    0.965 \n  \n\n 1847 \n    needed \n    1 \n    0.965 \n  \n\n 1848 \n    negotiate \n    1 \n    0.965 \n  \n\n 1849 \n    neo- \n    1 \n    0.965 \n  \n\n 1850 \n    nero \n    1 \n    0.965 \n  \n\n 1851 \n    net \n    1 \n    0.965 \n  \n\n 1852 \n    newly \n    1 \n    0.965 \n  \n\n 1853 \n    news \n    1 \n    0.965 \n  \n\n 1854 \n    nibbling \n    1 \n    0.965 \n  \n\n 1855 \n    niggers \n    1 \n    0.965 \n  \n\n 1856 \n    nights \n    1 \n    0.965 \n  \n\n 1857 \n    nineteen-ninety \n    1 \n    0.965 \n  \n\n 1858 \n    nineteenth \n    1 \n    0.965 \n  \n\n 1859 \n    ninety-four \n    1 \n    0.965 \n  \n\n 1860 \n    ninety-three \n    1 \n    0.965 \n  \n\n 1861 \n    ninety-two \n    1 \n    0.965 \n  \n\n 1862 \n    nipple \n    1 \n    0.965 \n  \n\n 1863 \n    noah \n    1 \n    0.965 \n  \n\n 1864 \n    nobel \n    1 \n    0.965 \n  \n\n 1865 \n    noblest \n    1 \n    0.965 \n  \n\n 1866 \n    noblewoman \n    1 \n    0.965 \n  \n\n 1867 \n    nodded \n    1 \n    0.965 \n  \n\n 1868 \n    none \n    1 \n    0.965 \n  \n\n 1869 \n    nostalgia \n    1 \n    0.965 \n  \n\n 1870 \n    nostrils \n    1 \n    0.965 \n  \n\n 1871 \n    nothingness \n    1 \n    0.965 \n  \n\n 1872 \n    noticed \n    1 \n    0.965 \n  \n\n 1873 \n    numbers \n    1 \n    0.965 \n  \n\n 1874 \n    nurture \n    1 \n    0.965 \n  \n\n 1875 \n    obliterating \n    1 \n    0.965 \n  \n\n 1876 \n    obscene \n    1 \n    0.965 \n  \n\n 1877 \n    obscure \n    1 \n    0.965 \n  \n\n 1878 \n    obsessed \n    1 \n    0.965 \n  \n\n 1879 \n    obsessively \n    1 \n    0.965 \n  \n\n 1880 \n    obstinately \n    1 \n    0.965 \n  \n\n 1881 \n    occupied \n    1 \n    0.965 \n  \n\n 1882 \n    odyssey \n    1 \n    0.965 \n  \n\n 1883 \n    offering \n    1 \n    0.965 \n  \n\n 1884 \n    office \n    1 \n    0.965 \n  \n\n 1885 \n    omeros \n    1 \n    0.965 \n  \n\n 1886 \n    one's \n    1 \n    0.965 \n  \n\n 1887 \n    ones \n    1 \n    0.965 \n  \n\n 1888 \n    oozing \n    1 \n    0.965 \n  \n\n 1889 \n    opening \n    1 \n    0.965 \n  \n\n 1890 \n    ordinariness \n    1 \n    0.965 \n  \n\n 1891 \n    ordinary \n    1 \n    0.965 \n  \n\n 1892 \n    organized \n    1 \n    0.965 \n  \n\n 1893 \n    original \n    1 \n    0.965 \n  \n\n 1894 \n    ornaments \n    1 \n    0.965 \n  \n\n 1895 \n    otters \n    1 \n    0.965 \n  \n\n 1896 \n    ours \n    1 \n    0.965 \n  \n\n 1897 \n    outrageous \n    1 \n    0.965 \n  \n\n 1898 \n    outside \n    1 \n    0.965 \n  \n\n 1899 \n    outweighed \n    1 \n    0.965 \n  \n\n 1900 \n    overcome \n    1 \n    0.965 \n  \n\n 1901 \n    overpay \n    1 \n    0.965 \n  \n\n 1902 \n    overwhelm \n    1 \n    0.965 \n  \n\n 1903 \n    overwhelmed \n    1 \n    0.965 \n  \n\n 1904 \n    oxen \n    1 \n    0.965 \n  \n\n 1905 \n    packed \n    1 \n    0.965 \n  \n\n 1906 \n    paid \n    1 \n    0.965 \n  \n\n 1907 \n    paintings \n    1 \n    0.965 \n  \n\n 1908 \n    palette \n    1 \n    0.965 \n  \n\n 1909 \n    panic \n    1 \n    0.965 \n  \n\n 1910 \n    papa \n    1 \n    0.965 \n  \n\n 1911 \n    paper \n    1 \n    0.965 \n  \n\n 1912 \n    parameters \n    1 \n    0.965 \n  \n\n 1913 \n    parcelled \n    1 \n    0.965 \n  \n\n 1914 \n    parent \n    1 \n    0.965 \n  \n\n 1915 \n    parlours \n    1 \n    0.965 \n  \n\n 1916 \n    particles \n    1 \n    0.965 \n  \n\n 1917 \n    particular \n    1 \n    0.965 \n  \n\n 1918 \n    parts \n    1 \n    0.965 \n  \n\n 1919 \n    patiently \n    1 \n    0.965 \n  \n\n 1920 \n    pattern \n    1 \n    0.965 \n  \n\n 1921 \n    patterned \n    1 \n    0.965 \n  \n\n 1922 \n    pausing \n    1 \n    0.965 \n  \n\n 1923 \n    paying \n    1 \n    0.965 \n  \n\n 1924 \n    pe-, \n    1 \n    0.965 \n  \n\n 1925 \n    pearls \n    1 \n    0.965 \n  \n\n 1926 \n    pebbles \n    1 \n    0.965 \n  \n\n 1927 \n    peculiarly \n    1 \n    0.965 \n  \n\n 1928 \n    peeled \n    1 \n    0.965 \n  \n\n 1929 \n    peeping \n    1 \n    0.965 \n  \n\n 1930 \n    peoples \n    1 \n    0.965 \n  \n\n 1931 \n    percentage \n    1 \n    0.965 \n  \n\n 1932 \n    percentages \n    1 \n    0.965 \n  \n\n 1933 \n    performance \n    1 \n    0.965 \n  \n\n 1934 \n    permission \n    1 \n    0.965 \n  \n\n 1935 \n    permitted \n    1 \n    0.965 \n  \n\n 1936 \n    person \n    1 \n    0.965 \n  \n\n 1937 \n    persuaded \n    1 \n    0.965 \n  \n\n 1938 \n    perverse \n    1 \n    0.965 \n  \n\n 1939 \n    philosophically \n    1 \n    0.965 \n  \n\n 1940 \n    photograph \n    1 \n    0.965 \n  \n\n 1941 \n    phrase \n    1 \n    0.965 \n  \n\n 1942 \n    physically \n    1 \n    0.965 \n  \n\n 1943 \n    pick \n    1 \n    0.965 \n  \n\n 1944 \n    picnic \n    1 \n    0.965 \n  \n\n 1945 \n    piece \n    1 \n    0.965 \n  \n\n 1946 \n    piercing \n    1 \n    0.965 \n  \n\n 1947 \n    pillars \n    1 \n    0.965 \n  \n\n 1948 \n    pink \n    1 \n    0.965 \n  \n\n 1949 \n    pinned \n    1 \n    0.965 \n  \n\n 1950 \n    pipe \n    1 \n    0.965 \n  \n\n 1951 \n    piranha \n    1 \n    0.965 \n  \n\n 1952 \n    pirate \n    1 \n    0.965 \n  \n\n 1953 \n    pitched \n    1 \n    0.965 \n  \n\n 1954 \n    placed \n    1 \n    0.965 \n  \n\n 1955 \n    placing \n    1 \n    0.965 \n  \n\n 1956 \n    plaid \n    1 \n    0.965 \n  \n\n 1957 \n    plainest \n    1 \n    0.965 \n  \n\n 1958 \n    plantains \n    1 \n    0.965 \n  \n\n 1959 \n    plaque \n    1 \n    0.965 \n  \n\n 1960 \n    plastic \n    1 \n    0.965 \n  \n\n 1961 \n    plato \n    1 \n    0.965 \n  \n\n 1962 \n    playful \n    1 \n    0.965 \n  \n\n 1963 \n    playground \n    1 \n    0.965 \n  \n\n 1964 \n    playing \n    1 \n    0.965 \n  \n\n 1965 \n    pleased \n    1 \n    0.965 \n  \n\n 1966 \n    pleasing \n    1 \n    0.965 \n  \n\n 1967 \n    plough \n    1 \n    0.965 \n  \n\n 1968 \n    plucked \n    1 \n    0.965 \n  \n\n 1969 \n    plucks \n    1 \n    0.965 \n  \n\n 1970 \n    po-, \n    1 \n    0.965 \n  \n\n 1971 \n    pockets \n    1 \n    0.965 \n  \n\n 1972 \n    poetic \n    1 \n    0.965 \n  \n\n 1973 \n    poisoned \n    1 \n    0.965 \n  \n\n 1974 \n    poofs \n    1 \n    0.965 \n  \n\n 1975 \n    pope \n    1 \n    0.965 \n  \n\n 1976 \n    portent \n    1 \n    0.965 \n  \n\n 1977 \n    portions \n    1 \n    0.965 \n  \n\n 1978 \n    possess \n    1 \n    0.965 \n  \n\n 1979 \n    possessed \n    1 \n    0.965 \n  \n\n 1980 \n    possession \n    1 \n    0.965 \n  \n\n 1981 \n    possessions \n    1 \n    0.965 \n  \n\n 1982 \n    potions \n    1 \n    0.965 \n  \n\n 1983 \n    power \n    1 \n    0.965 \n  \n\n 1984 \n    practised \n    1 \n    0.965 \n  \n\n 1985 \n    praise \n    1 \n    0.965 \n  \n\n 1986 \n    preacher \n    1 \n    0.965 \n  \n\n 1987 \n    precarious \n    1 \n    0.965 \n  \n\n 1988 \n    precious \n    1 \n    0.965 \n  \n\n 1989 \n    prefer \n    1 \n    0.965 \n  \n\n 1990 \n    pregnancy \n    1 \n    0.965 \n  \n\n 1991 \n    preparations \n    1 \n    0.965 \n  \n\n 1992 \n    prepared \n    1 \n    0.965 \n  \n\n 1993 \n    press \n    1 \n    0.965 \n  \n\n 1994 \n    pretending \n    1 \n    0.965 \n  \n\n 1995 \n    prick \n    1 \n    0.965 \n  \n\n 1996 \n    priest \n    1 \n    0.965 \n  \n\n 1997 \n    prince \n    1 \n    0.965 \n  \n\n 1998 \n    print \n    1 \n    0.965 \n  \n\n 1999 \n    pristine \n    1 \n    0.965 \n  \n\n 2000 \n    privilege \n    1 \n    0.965 \n  \n\n 2001 \n    prize \n    1 \n    0.965 \n  \n\n 2002 \n    professor \n    1 \n    0.965 \n  \n\n 2003 \n    profitably \n    1 \n    0.965 \n  \n\n 2004 \n    profound \n    1 \n    0.965 \n  \n\n 2005 \n    profusion \n    1 \n    0.965 \n  \n\n 2006 \n    property \n    1 \n    0.965 \n  \n\n 2007 \n    prophecy \n    1 \n    0.965 \n  \n\n 2008 \n    prophesy \n    1 \n    0.965 \n  \n\n 2009 \n    proprietor \n    1 \n    0.965 \n  \n\n 2010 \n    proprietors \n    1 \n    0.965 \n  \n\n 2011 \n    protectively \n    1 \n    0.965 \n  \n\n 2012 \n    proudly \n    1 \n    0.965 \n  \n\n 2013 \n    provoked \n    1 \n    0.965 \n  \n\n 2014 \n    pseudo-jamaican \n    1 \n    0.965 \n  \n\n 2015 \n    published \n    1 \n    0.965 \n  \n\n 2016 \n    puffed \n    1 \n    0.965 \n  \n\n 2017 \n    punishment \n    1 \n    0.965 \n  \n\n 2018 \n    purchase \n    1 \n    0.965 \n  \n\n 2019 \n    purity \n    1 \n    0.965 \n  \n\n 2020 \n    purple \n    1 \n    0.965 \n  \n\n 2021 \n    purpose \n    1 \n    0.965 \n  \n\n 2022 \n    push \n    1 \n    0.965 \n  \n\n 2023 \n    pushing \n    1 \n    0.965 \n  \n\n 2024 \n    pussy \n    1 \n    0.965 \n  \n\n 2025 \n    puzzled \n    1 \n    0.965 \n  \n\n 2026 \n    qualification \n    1 \n    0.965 \n  \n\n 2027 \n    quality \n    1 \n    0.965 \n  \n\n 2028 \n    queers \n    1 \n    0.965 \n  \n\n 2029 \n    quell \n    1 \n    0.965 \n  \n\n 2030 \n    question \n    1 \n    0.965 \n  \n\n 2031 \n    queue \n    1 \n    0.965 \n  \n\n 2032 \n    quickened \n    1 \n    0.965 \n  \n\n 2033 \n    rab-, \n    1 \n    0.965 \n  \n\n 2034 \n    racks \n    1 \n    0.965 \n  \n\n 2035 \n    raggedly \n    1 \n    0.965 \n  \n\n 2036 \n    rags \n    1 \n    0.965 \n  \n\n 2037 \n    rainwater \n    1 \n    0.965 \n  \n\n 2038 \n    raised \n    1 \n    0.965 \n  \n\n 2039 \n    raises \n    1 \n    0.965 \n  \n\n 2040 \n    rank \n    1 \n    0.965 \n  \n\n 2041 \n    rape \n    1 \n    0.965 \n  \n\n 2042 \n    rapture \n    1 \n    0.965 \n  \n\n 2043 \n    rash \n    1 \n    0.965 \n  \n\n 2044 \n    rastafarian \n    1 \n    0.965 \n  \n\n 2045 \n    rather \n    1 \n    0.965 \n  \n\n 2046 \n    rats \n    1 \n    0.965 \n  \n\n 2047 \n    raven \n    1 \n    0.965 \n  \n\n 2048 \n    ravish \n    1 \n    0.965 \n  \n\n 2049 \n    reaches \n    1 \n    0.965 \n  \n\n 2050 \n    ready \n    1 \n    0.965 \n  \n\n 2051 \n    realization \n    1 \n    0.965 \n  \n\n 2052 \n    realize \n    1 \n    0.965 \n  \n\n 2053 \n    reaped \n    1 \n    0.965 \n  \n\n 2054 \n    rear \n    1 \n    0.965 \n  \n\n 2055 \n    reassurance \n    1 \n    0.965 \n  \n\n 2056 \n    received \n    1 \n    0.965 \n  \n\n 2057 \n    receives \n    1 \n    0.965 \n  \n\n 2058 \n    reckoning \n    1 \n    0.965 \n  \n\n 2059 \n    reclaimed \n    1 \n    0.965 \n  \n\n 2060 \n    reclimbing \n    1 \n    0.965 \n  \n\n 2061 \n    recognition \n    1 \n    0.965 \n  \n\n 2062 \n    reconceptualizing \n    1 \n    0.965 \n  \n\n 2063 \n    recover \n    1 \n    0.965 \n  \n\n 2064 \n    recreate \n    1 \n    0.965 \n  \n\n 2065 \n    redemptive \n    1 \n    0.965 \n  \n\n 2066 \n    refashioned \n    1 \n    0.965 \n  \n\n 2067 \n    reflect \n    1 \n    0.965 \n  \n\n 2068 \n    reflected \n    1 \n    0.965 \n  \n\n 2069 \n    reflection \n    1 \n    0.965 \n  \n\n 2070 \n    regain \n    1 \n    0.965 \n  \n\n 2071 \n    regular \n    1 \n    0.965 \n  \n\n 2072 \n    relief \n    1 \n    0.965 \n  \n\n 2073 \n    remaining \n    1 \n    0.965 \n  \n\n 2074 \n    remembered \n    1 \n    0.965 \n  \n\n 2075 \n    remove \n    1 \n    0.965 \n  \n\n 2076 \n    renewed \n    1 \n    0.965 \n  \n\n 2077 \n    reparation \n    1 \n    0.965 \n  \n\n 2078 \n    repeated \n    1 \n    0.965 \n  \n\n 2079 \n    report \n    1 \n    0.965 \n  \n\n 2080 \n    research \n    1 \n    0.965 \n  \n\n 2081 \n    resolve \n    1 \n    0.965 \n  \n\n 2082 \n    restaurants \n    1 \n    0.965 \n  \n\n 2083 \n    restoring \n    1 \n    0.965 \n  \n\n 2084 \n    restricted \n    1 \n    0.965 \n  \n\n 2085 \n    result \n    1 \n    0.965 \n  \n\n 2086 \n    resurrection \n    1 \n    0.965 \n  \n\n 2087 \n    retreat \n    1 \n    0.965 \n  \n\n 2088 \n    retreated \n    1 \n    0.965 \n  \n\n 2089 \n    return \n    1 \n    0.965 \n  \n\n 2090 \n    returns \n    1 \n    0.965 \n  \n\n 2091 \n    revelation \n    1 \n    0.965 \n  \n\n 2092 \n    revengefulness \n    1 \n    0.965 \n  \n\n 2093 \n    reverence \n    1 \n    0.965 \n  \n\n 2094 \n    revisualization \n    1 \n    0.965 \n  \n\n 2095 \n    revisualizing \n    1 \n    0.965 \n  \n\n 2096 \n    revive \n    1 \n    0.965 \n  \n\n 2097 \n    rhetoric \n    1 \n    0.965 \n  \n\n 2098 \n    riches \n    1 \n    0.965 \n  \n\n 2099 \n    richly \n    1 \n    0.965 \n  \n\n 2100 \n    richness \n    1 \n    0.965 \n  \n\n 2101 \n    rid-, \n    1 \n    0.965 \n  \n\n 2102 \n    rioting \n    1 \n    0.965 \n  \n\n 2103 \n    ripped \n    1 \n    0.965 \n  \n\n 2104 \n    rise \n    1 \n    0.965 \n  \n\n 2105 \n    ritual \n    1 \n    0.965 \n  \n\n 2106 \n    roam \n    1 \n    0.965 \n  \n\n 2107 \n    roamed \n    1 \n    0.965 \n  \n\n 2108 \n    roar \n    1 \n    0.965 \n  \n\n 2109 \n    rolling \n    1 \n    0.965 \n  \n\n 2110 \n    romantic \n    1 \n    0.965 \n  \n\n 2111 \n    romantics \n    1 \n    0.965 \n  \n\n 2112 \n    roni-, \n    1 \n    0.965 \n  \n\n 2113 \n    rooted \n    1 \n    0.965 \n  \n\n 2114 \n    roots \n    1 \n    0.965 \n  \n\n 2115 \n    rope \n    1 \n    0.965 \n  \n\n 2116 \n    ropes \n    1 \n    0.965 \n  \n\n 2117 \n    rose \n    1 \n    0.965 \n  \n\n 2118 \n    rouge \n    1 \n    0.965 \n  \n\n 2119 \n    roundness \n    1 \n    0.965 \n  \n\n 2120 \n    royalties \n    1 \n    0.965 \n  \n\n 2121 \n    royalty \n    1 \n    0.965 \n  \n\n 2122 \n    rub \n    1 \n    0.965 \n  \n\n 2123 \n    rubies \n    1 \n    0.965 \n  \n\n 2124 \n    runs \n    1 \n    0.965 \n  \n\n 2125 \n    rush \n    1 \n    0.965 \n  \n\n 2126 \n    rushed \n    1 \n    0.965 \n  \n\n 2127 \n    ruskin \n    1 \n    0.965 \n  \n\n 2128 \n    rusty \n    1 \n    0.965 \n  \n\n 2129 \n    saba \n    1 \n    0.965 \n  \n\n 2130 \n    sadu \n    1 \n    0.965 \n  \n\n 2131 \n    safety \n    1 \n    0.965 \n  \n\n 2132 \n    sailor's \n    1 \n    0.965 \n  \n\n 2133 \n    sale \n    1 \n    0.965 \n  \n\n 2134 \n    salty \n    1 \n    0.965 \n  \n\n 2135 \n    sat \n    1 \n    0.965 \n  \n\n 2136 \n    saturday \n    1 \n    0.965 \n  \n\n 2137 \n    savagely \n    1 \n    0.965 \n  \n\n 2138 \n    saving \n    1 \n    0.965 \n  \n\n 2139 \n    says \n    1 \n    0.965 \n  \n\n 2140 \n    scaling \n    1 \n    0.965 \n  \n\n 2141 \n    scarce \n    1 \n    0.965 \n  \n\n 2142 \n    scarify \n    1 \n    0.965 \n  \n\n 2143 \n    scatter \n    1 \n    0.965 \n  \n\n 2144 \n    scattered \n    1 \n    0.965 \n  \n\n 2145 \n    scavenged \n    1 \n    0.965 \n  \n\n 2146 \n    scented \n    1 \n    0.965 \n  \n\n 2147 \n    scholarship \n    1 \n    0.965 \n  \n\n 2148 \n    scorn \n    1 \n    0.965 \n  \n\n 2149 \n    screamed \n    1 \n    0.965 \n  \n\n 2150 \n    screaming \n    1 \n    0.965 \n  \n\n 2151 \n    screeching \n    1 \n    0.965 \n  \n\n 2152 \n    scrolls \n    1 \n    0.965 \n  \n\n 2153 \n    sea's \n    1 \n    0.965 \n  \n\n 2154 \n    seal's \n    1 \n    0.965 \n  \n\n 2155 \n    secondborn \n    1 \n    0.965 \n  \n\n 2156 \n    seconds \n    1 \n    0.965 \n  \n\n 2157 \n    secreted \n    1 \n    0.965 \n  \n\n 2158 \n    security \n    1 \n    0.965 \n  \n\n 2159 \n    seed \n    1 \n    0.965 \n  \n\n 2160 \n    seeing \n    1 \n    0.965 \n  \n\n 2161 \n    seeking \n    1 \n    0.965 \n  \n\n 2162 \n    seen \n    1 \n    0.965 \n  \n\n 2163 \n    seeped \n    1 \n    0.965 \n  \n\n 2164 \n    self- \n    1 \n    0.965 \n  \n\n 2165 \n    selflessly \n    1 \n    0.965 \n  \n\n 2166 \n    sell \n    1 \n    0.965 \n  \n\n 2167 \n    selling \n    1 \n    0.965 \n  \n\n 2168 \n    senior \n    1 \n    0.965 \n  \n\n 2169 \n    senses \n    1 \n    0.965 \n  \n\n 2170 \n    sensing \n    1 \n    0.965 \n  \n\n 2171 \n    serious \n    1 \n    0.965 \n  \n\n 2172 \n    seriously \n    1 \n    0.965 \n  \n\n 2173 \n    services \n    1 \n    0.965 \n  \n\n 2174 \n    seven \n    1 \n    0.965 \n  \n\n 2175 \n    seventeen \n    1 \n    0.965 \n  \n\n 2176 \n    seventeen-eighties \n    1 \n    0.965 \n  \n\n 2177 \n    sh-, \n    1 \n    0.965 \n  \n\n 2178 \n    shaped \n    1 \n    0.965 \n  \n\n 2179 \n    shaping \n    1 \n    0.965 \n  \n\n 2180 \n    shark \n    1 \n    0.965 \n  \n\n 2181 \n    sharp \n    1 \n    0.965 \n  \n\n 2182 \n    sharpening \n    1 \n    0.965 \n  \n\n 2183 \n    shave \n    1 \n    0.965 \n  \n\n 2184 \n    shelf \n    1 \n    0.965 \n  \n\n 2185 \n    shells \n    1 \n    0.965 \n  \n\n 2186 \n    shield \n    1 \n    0.965 \n  \n\n 2187 \n    shiny \n    1 \n    0.965 \n  \n\n 2188 \n    shipped \n    1 \n    0.965 \n  \n\n 2189 \n    ships \n    1 \n    0.965 \n  \n\n 2190 \n    shit \n    1 \n    0.965 \n  \n\n 2191 \n    shoals \n    1 \n    0.965 \n  \n\n 2192 \n    shocked \n    1 \n    0.965 \n  \n\n 2193 \n    shone \n    1 \n    0.965 \n  \n\n 2194 \n    shops \n    1 \n    0.965 \n  \n\n 2195 \n    short \n    1 \n    0.965 \n  \n\n 2196 \n    shorten \n    1 \n    0.965 \n  \n\n 2197 \n    shouted \n    1 \n    0.965 \n  \n\n 2198 \n    shove \n    1 \n    0.965 \n  \n\n 2199 \n    showed \n    1 \n    0.965 \n  \n\n 2200 \n    shows \n    1 \n    0.965 \n  \n\n 2201 \n    shrieks \n    1 \n    0.965 \n  \n\n 2202 \n    shrimps \n    1 \n    0.965 \n  \n\n 2203 \n    shuddered \n    1 \n    0.965 \n  \n\n 2204 \n    shut \n    1 \n    0.965 \n  \n\n 2205 \n    shy \n    1 \n    0.965 \n  \n\n 2206 \n    sickle \n    1 \n    0.965 \n  \n\n 2207 \n    sideways \n    1 \n    0.965 \n  \n\n 2208 \n    silent \n    1 \n    0.965 \n  \n\n 2209 \n    silently \n    1 \n    0.965 \n  \n\n 2210 \n    simpler \n    1 \n    0.965 \n  \n\n 2211 \n    sin \n    1 \n    0.965 \n  \n\n 2212 \n    sinking \n    1 \n    0.965 \n  \n\n 2213 \n    sinning \n    1 \n    0.965 \n  \n\n 2214 \n    sixty \n    1 \n    0.965 \n  \n\n 2215 \n    skels \n    1 \n    0.965 \n  \n\n 2216 \n    sketches \n    1 \n    0.965 \n  \n\n 2217 \n    skirt \n    1 \n    0.965 \n  \n\n 2218 \n    sky's \n    1 \n    0.965 \n  \n\n 2219 \n    slag \n    1 \n    0.965 \n  \n\n 2220 \n    slapped \n    1 \n    0.965 \n  \n\n 2221 \n    slaps \n    1 \n    0.965 \n  \n\n 2222 \n    sleeping \n    1 \n    0.965 \n  \n\n 2223 \n    slight \n    1 \n    0.965 \n  \n\n 2224 \n    slithers \n    1 \n    0.965 \n  \n\n 2225 \n    slob \n    1 \n    0.965 \n  \n\n 2226 \n    slow \n    1 \n    0.965 \n  \n\n 2227 \n    sluggishly \n    1 \n    0.965 \n  \n\n 2228 \n    smell \n    1 \n    0.965 \n  \n\n 2229 \n    smells \n    1 \n    0.965 \n  \n\n 2230 \n    smile \n    1 \n    0.965 \n  \n\n 2231 \n    smiling \n    1 \n    0.965 \n  \n\n 2232 \n    smoked \n    1 \n    0.965 \n  \n\n 2233 \n    smooched \n    1 \n    0.965 \n  \n\n 2234 \n    smouldered \n    1 \n    0.965 \n  \n\n 2235 \n    snakes \n    1 \n    0.965 \n  \n\n 2236 \n    snaps \n    1 \n    0.965 \n  \n\n 2237 \n    sniffs \n    1 \n    0.965 \n  \n\n 2238 \n    snorting \n    1 \n    0.965 \n  \n\n 2239 \n    so-, \n    1 \n    0.965 \n  \n\n 2240 \n    social \n    1 \n    0.965 \n  \n\n 2241 \n    sofa \n    1 \n    0.965 \n  \n\n 2242 \n    softening \n    1 \n    0.965 \n  \n\n 2243 \n    soothe \n    1 \n    0.965 \n  \n\n 2244 \n    soothes \n    1 \n    0.965 \n  \n\n 2245 \n    sort \n    1 \n    0.965 \n  \n\n 2246 \n    sounds \n    1 \n    0.965 \n  \n\n 2247 \n    south \n    1 \n    0.965 \n  \n\n 2248 \n    southernly \n    1 \n    0.965 \n  \n\n 2249 \n    spanking \n    1 \n    0.965 \n  \n\n 2250 \n    spears \n    1 \n    0.965 \n  \n\n 2251 \n    spectacle \n    1 \n    0.965 \n  \n\n 2252 \n    speed \n    1 \n    0.965 \n  \n\n 2253 \n    spheres \n    1 \n    0.965 \n  \n\n 2254 \n    spills \n    1 \n    0.965 \n  \n\n 2255 \n    spite \n    1 \n    0.965 \n  \n\n 2256 \n    sponges \n    1 \n    0.965 \n  \n\n 2257 \n    spontaneously \n    1 \n    0.965 \n  \n\n 2258 \n    sport \n    1 \n    0.965 \n  \n\n 2259 \n    spot \n    1 \n    0.965 \n  \n\n 2260 \n    sprawled \n    1 \n    0.965 \n  \n\n 2261 \n    sprawling \n    1 \n    0.965 \n  \n\n 2262 \n    sprightly \n    1 \n    0.965 \n  \n\n 2263 \n    spurting \n    1 \n    0.965 \n  \n\n 2264 \n    squirming \n    1 \n    0.965 \n  \n\n 2265 \n    stampeded \n    1 \n    0.965 \n  \n\n 2266 \n    standing \n    1 \n    0.965 \n  \n\n 2267 \n    stares \n    1 \n    0.965 \n  \n\n 2268 \n    start \n    1 \n    0.965 \n  \n\n 2269 \n    steadfast \n    1 \n    0.965 \n  \n\n 2270 \n    stepping \n    1 \n    0.965 \n  \n\n 2271 \n    steps \n    1 \n    0.965 \n  \n\n 2272 \n    sternly \n    1 \n    0.965 \n  \n\n 2273 \n    sticking \n    1 \n    0.965 \n  \n\n 2274 \n    stiff \n    1 \n    0.965 \n  \n\n 2275 \n    stifle \n    1 \n    0.965 \n  \n\n 2276 \n    stomach \n    1 \n    0.965 \n  \n\n 2277 \n    stomp \n    1 \n    0.965 \n  \n\n 2278 \n    stonemason \n    1 \n    0.965 \n  \n\n 2279 \n    stones \n    1 \n    0.965 \n  \n\n 2280 \n    stopped \n    1 \n    0.965 \n  \n\n 2281 \n    stopping \n    1 \n    0.965 \n  \n\n 2282 \n    stories \n    1 \n    0.965 \n  \n\n 2283 \n    storms \n    1 \n    0.965 \n  \n\n 2284 \n    stormy \n    1 \n    0.965 \n  \n\n 2285 \n    stout \n    1 \n    0.965 \n  \n\n 2286 \n    straight \n    1 \n    0.965 \n  \n\n 2287 \n    streaked \n    1 \n    0.965 \n  \n\n 2288 \n    streets \n    1 \n    0.965 \n  \n\n 2289 \n    strict \n    1 \n    0.965 \n  \n\n 2290 \n    stripes \n    1 \n    0.965 \n  \n\n 2291 \n    striptease \n    1 \n    0.965 \n  \n\n 2292 \n    strong \n    1 \n    0.965 \n  \n\n 2293 \n    stronger \n    1 \n    0.965 \n  \n\n 2294 \n    struck \n    1 \n    0.965 \n  \n\n 2295 \n    structures \n    1 \n    0.965 \n  \n\n 2296 \n    struggled \n    1 \n    0.965 \n  \n\n 2297 \n    stubble \n    1 \n    0.965 \n  \n\n 2298 \n    stubbornness \n    1 \n    0.965 \n  \n\n 2299 \n    stuck \n    1 \n    0.965 \n  \n\n 2300 \n    student \n    1 \n    0.965 \n  \n\n 2301 \n    study \n    1 \n    0.965 \n  \n\n 2302 \n    stuff \n    1 \n    0.965 \n  \n\n 2303 \n    stupefaction \n    1 \n    0.965 \n  \n\n 2304 \n    style \n    1 \n    0.965 \n  \n\n 2305 \n    subdue \n    1 \n    0.965 \n  \n\n 2306 \n    substantial \n    1 \n    0.965 \n  \n\n 2307 \n    subtracted \n    1 \n    0.965 \n  \n\n 2308 \n    sucks \n    1 \n    0.965 \n  \n\n 2309 \n    suffer \n    1 \n    0.965 \n  \n\n 2310 \n    suffered \n    1 \n    0.965 \n  \n\n 2311 \n    suffering \n    1 \n    0.965 \n  \n\n 2312 \n    suffolk \n    1 \n    0.965 \n  \n\n 2313 \n    sugared \n    1 \n    0.965 \n  \n\n 2314 \n    sugarloaves \n    1 \n    0.965 \n  \n\n 2315 \n    suit \n    1 \n    0.965 \n  \n\n 2316 \n    sulks \n    1 \n    0.965 \n  \n\n 2317 \n    sunken \n    1 \n    0.965 \n  \n\n 2318 \n    supervision \n    1 \n    0.965 \n  \n\n 2319 \n    suppose \n    1 \n    0.965 \n  \n\n 2320 \n    suppressing \n    1 \n    0.965 \n  \n\n 2321 \n    surmise \n    1 \n    0.965 \n  \n\n 2322 \n    surrounded \n    1 \n    0.965 \n  \n\n 2323 \n    surrounding \n    1 \n    0.965 \n  \n\n 2324 \n    survive \n    1 \n    0.965 \n  \n\n 2325 \n    sustaining \n    1 \n    0.965 \n  \n\n 2326 \n    swallow \n    1 \n    0.965 \n  \n\n 2327 \n    swallowed \n    1 \n    0.965 \n  \n\n 2328 \n    swallowing \n    1 \n    0.965 \n  \n\n 2329 \n    swarming \n    1 \n    0.965 \n  \n\n 2330 \n    swatter \n    1 \n    0.965 \n  \n\n 2331 \n    swearing \n    1 \n    0.965 \n  \n\n 2332 \n    sweat \n    1 \n    0.965 \n  \n\n 2333 \n    swelled \n    1 \n    0.965 \n  \n\n 2334 \n    swelling \n    1 \n    0.965 \n  \n\n 2335 \n    swiftly \n    1 \n    0.965 \n  \n\n 2336 \n    swim \n    1 \n    0.965 \n  \n\n 2337 \n    swimming \n    1 \n    0.965 \n  \n\n 2338 \n    switched \n    1 \n    0.965 \n  \n\n 2339 \n    sword \n    1 \n    0.965 \n  \n\n 2340 \n    swords \n    1 \n    0.965 \n  \n\n 2341 \n    t-, \n    1 \n    0.965 \n  \n\n 2342 \n    table \n    1 \n    0.965 \n  \n\n 2343 \n    takes \n    1 \n    0.965 \n  \n\n 2344 \n    taking \n    1 \n    0.965 \n  \n\n 2345 \n    tally \n    1 \n    0.965 \n  \n\n 2346 \n    tanda \n    1 \n    0.965 \n  \n\n 2347 \n    tastes \n    1 \n    0.965 \n  \n\n 2348 \n    taught \n    1 \n    0.965 \n  \n\n 2349 \n    taxes \n    1 \n    0.965 \n  \n\n 2350 \n    taxman \n    1 \n    0.965 \n  \n\n 2351 \n    tears \n    1 \n    0.965 \n  \n\n 2352 \n    temples \n    1 \n    0.965 \n  \n\n 2353 \n    tended \n    1 \n    0.965 \n  \n\n 2354 \n    terribly \n    1 \n    0.965 \n  \n\n 2355 \n    terror \n    1 \n    0.965 \n  \n\n 2356 \n    test \n    1 \n    0.965 \n  \n\n 2357 \n    tested \n    1 \n    0.965 \n  \n\n 2358 \n    texture \n    1 \n    0.965 \n  \n\n 2359 \n    th-, \n    1 \n    0.965 \n  \n\n 2360 \n    theme \n    1 \n    0.965 \n  \n\n 2361 \n    they'll \n    1 \n    0.965 \n  \n\n 2362 \n    thin \n    1 \n    0.965 \n  \n\n 2363 \n    thinking \n    1 \n    0.965 \n  \n\n 2364 \n    thinks \n    1 \n    0.965 \n  \n\n 2365 \n    thistlewood's \n    1 \n    0.965 \n  \n\n 2366 \n    thorns \n    1 \n    0.965 \n  \n\n 2367 \n    thousand-and-a-half \n    1 \n    0.965 \n  \n\n 2368 \n    threatening \n    1 \n    0.965 \n  \n\n 2369 \n    threatens \n    1 \n    0.965 \n  \n\n 2370 \n    three-hundred \n    1 \n    0.965 \n  \n\n 2371 \n    throat \n    1 \n    0.965 \n  \n\n 2372 \n    thumb \n    1 \n    0.965 \n  \n\n 2373 \n    thumbed \n    1 \n    0.965 \n  \n\n 2374 \n    tie \n    1 \n    0.965 \n  \n\n 2375 \n    tied \n    1 \n    0.965 \n  \n\n 2376 \n    tightly \n    1 \n    0.965 \n  \n\n 2377 \n    tiny \n    1 \n    0.965 \n  \n\n 2378 \n    tired \n    1 \n    0.965 \n  \n\n 2379 \n    tiring \n    1 \n    0.965 \n  \n\n 2380 \n    titanic \n    1 \n    0.965 \n  \n\n 2381 \n    tobacco \n    1 \n    0.965 \n  \n\n 2382 \n    toenail \n    1 \n    0.965 \n  \n\n 2383 \n    together \n    1 \n    0.965 \n  \n\n 2384 \n    told \n    1 \n    0.965 \n  \n\n 2385 \n    tolerance \n    1 \n    0.965 \n  \n\n 2386 \n    tomb \n    1 \n    0.965 \n  \n\n 2387 \n    tomb's \n    1 \n    0.965 \n  \n\n 2388 \n    tongueless \n    1 \n    0.965 \n  \n\n 2389 \n    tonight \n    1 \n    0.965 \n  \n\n 2390 \n    tools \n    1 \n    0.965 \n  \n\n 2391 \n    touch \n    1 \n    0.965 \n  \n\n 2392 \n    tour \n    1 \n    0.965 \n  \n\n 2393 \n    trace \n    1 \n    0.965 \n  \n\n 2394 \n    trade \n    1 \n    0.965 \n  \n\n 2395 \n    trader \n    1 \n    0.965 \n  \n\n 2396 \n    tragedy \n    1 \n    0.965 \n  \n\n 2397 \n    trailing \n    1 \n    0.965 \n  \n\n 2398 \n    train \n    1 \n    0.965 \n  \n\n 2399 \n    transformation \n    1 \n    0.965 \n  \n\n 2400 \n    traveller \n    1 \n    0.965 \n  \n\n 2401 \n    treasure \n    1 \n    0.965 \n  \n\n 2402 \n    tree \n    1 \n    0.965 \n  \n\n 2403 \n    trench \n    1 \n    0.965 \n  \n\n 2404 \n    trespass \n    1 \n    0.965 \n  \n\n 2405 \n    tribute \n    1 \n    0.965 \n  \n\n 2406 \n    triumph \n    1 \n    0.965 \n  \n\n 2407 \n    trojan \n    1 \n    0.965 \n  \n\n 2408 \n    trooped \n    1 \n    0.965 \n  \n\n 2409 \n    trouble \n    1 \n    0.965 \n  \n\n 2410 \n    troy \n    1 \n    0.965 \n  \n\n 2411 \n    trumpets \n    1 \n    0.965 \n  \n\n 2412 \n    trussed \n    1 \n    0.965 \n  \n\n 2413 \n    truth \n    1 \n    0.965 \n  \n\n 2414 \n    try \n    1 \n    0.965 \n  \n\n 2415 \n    tugged \n    1 \n    0.965 \n  \n\n 2416 \n    tumble \n    1 \n    0.965 \n  \n\n 2417 \n    tune \n    1 \n    0.965 \n  \n\n 2418 \n    twenty \n    1 \n    0.965 \n  \n\n 2419 \n    twenty-five \n    1 \n    0.965 \n  \n\n 2420 \n    twigs \n    1 \n    0.965 \n  \n\n 2421 \n    twinkling \n    1 \n    0.965 \n  \n\n 2422 \n    twirls \n    1 \n    0.965 \n  \n\n 2423 \n    twisted \n    1 \n    0.965 \n  \n\n 2424 \n    two-hundred \n    1 \n    0.965 \n  \n\n 2425 \n    type \n    1 \n    0.965 \n  \n\n 2426 \n    u-, \n    1 \n    0.965 \n  \n\n 2427 \n    unable \n    1 \n    0.965 \n  \n\n 2428 \n    unborn \n    1 \n    0.965 \n  \n\n 2429 \n    underpinnings \n    1 \n    0.965 \n  \n\n 2430 \n    understand \n    1 \n    0.965 \n  \n\n 2431 \n    undertaker's \n    1 \n    0.965 \n  \n\n 2432 \n    unending \n    1 \n    0.965 \n  \n\n 2433 \n    unfamiliar \n    1 \n    0.965 \n  \n\n 2434 \n    unfolding \n    1 \n    0.965 \n  \n\n 2435 \n    unfulfilled \n    1 \n    0.965 \n  \n\n 2436 \n    ungratefully \n    1 \n    0.965 \n  \n\n 2437 \n    unhappiness \n    1 \n    0.965 \n  \n\n 2438 \n    unhewn \n    1 \n    0.965 \n  \n\n 2439 \n    universal \n    1 \n    0.965 \n  \n\n 2440 \n    unlike \n    1 \n    0.965 \n  \n\n 2441 \n    unnamed \n    1 \n    0.965 \n  \n\n 2442 \n    unpastes \n    1 \n    0.965 \n  \n\n 2443 \n    unpublished \n    1 \n    0.965 \n  \n\n 2444 \n    unties \n    1 \n    0.965 \n  \n\n 2445 \n    unwrap \n    1 \n    0.965 \n  \n\n 2446 \n    urien \n    1 \n    0.965 \n  \n\n 2447 \n    using \n    1 \n    0.965 \n  \n\n 2448 \n    utter \n    1 \n    0.965 \n  \n\n 2449 \n    valour \n    1 \n    0.965 \n  \n\n 2450 \n    value \n    1 \n    0.965 \n  \n\n 2451 \n    vanquished \n    1 \n    0.965 \n  \n\n 2452 \n    vaster \n    1 \n    0.965 \n  \n\n 2453 \n    veil \n    1 \n    0.965 \n  \n\n 2454 \n    venture \n    1 \n    0.965 \n  \n\n 2455 \n    ventured \n    1 \n    0.965 \n  \n\n 2456 \n    ver-, \n    1 \n    0.965 \n  \n\n 2457 \n    videos \n    1 \n    0.965 \n  \n\n 2458 \n    view \n    1 \n    0.965 \n  \n\n 2459 \n    viewer's \n    1 \n    0.965 \n  \n\n 2460 \n    villagers \n    1 \n    0.965 \n  \n\n 2461 \n    villages \n    1 \n    0.965 \n  \n\n 2462 \n    violent \n    1 \n    0.965 \n  \n\n 2463 \n    wa-, \n    1 \n    0.965 \n  \n\n 2464 \n    wake \n    1 \n    0.965 \n  \n\n 2465 \n    walking \n    1 \n    0.965 \n  \n\n 2466 \n    walls \n    1 \n    0.965 \n  \n\n 2467 \n    wander \n    1 \n    0.965 \n  \n\n 2468 \n    wandered \n    1 \n    0.965 \n  \n\n 2469 \n    waning \n    1 \n    0.965 \n  \n\n 2470 \n    wanking \n    1 \n    0.965 \n  \n\n 2471 \n    war \n    1 \n    0.965 \n  \n\n 2472 \n    warehouse \n    1 \n    0.965 \n  \n\n 2473 \n    wares \n    1 \n    0.965 \n  \n\n 2474 \n    warriors \n    1 \n    0.965 \n  \n\n 2475 \n    wash \n    1 \n    0.965 \n  \n\n 2476 \n    washed \n    1 \n    0.965 \n  \n\n 2477 \n    washing \n    1 \n    0.965 \n  \n\n 2478 \n    wasn't \n    1 \n    0.965 \n  \n\n 2479 \n    watches \n    1 \n    0.965 \n  \n\n 2480 \n    waves \n    1 \n    0.965 \n  \n\n 2481 \n    wayward \n    1 \n    0.965 \n  \n\n 2482 \n    weapon \n    1 \n    0.965 \n  \n\n 2483 \n    wears \n    1 \n    0.965 \n  \n\n 2484 \n    weaves \n    1 \n    0.965 \n  \n\n 2485 \n    weaving \n    1 \n    0.965 \n  \n\n 2486 \n    week \n    1 \n    0.965 \n  \n\n 2487 \n    weeks \n    1 \n    0.965 \n  \n\n 2488 \n    weep \n    1 \n    0.965 \n  \n\n 2489 \n    weeping \n    1 \n    0.965 \n  \n\n 2490 \n    western \n    1 \n    0.965 \n  \n\n 2491 \n    wet \n    1 \n    0.965 \n  \n\n 2492 \n    what's \n    1 \n    0.965 \n  \n\n 2493 \n    wheeze \n    1 \n    0.965 \n  \n\n 2494 \n    whereby \n    1 \n    0.965 \n  \n\n 2495 \n    wherever \n    1 \n    0.965 \n  \n\n 2496 \n    whether \n    1 \n    0.965 \n  \n\n 2497 \n    whim \n    1 \n    0.965 \n  \n\n 2498 \n    whip \n    1 \n    0.965 \n  \n\n 2499 \n    whips \n    1 \n    0.965 \n  \n\n 2500 \n    whores \n    1 \n    0.965 \n  \n\n 2501 \n    wife \n    1 \n    0.965 \n  \n\n 2502 \n    wild \n    1 \n    0.965 \n  \n\n 2503 \n    wildest \n    1 \n    0.965 \n  \n\n 2504 \n    willed \n    1 \n    0.965 \n  \n\n 2505 \n    win \n    1 \n    0.965 \n  \n\n 2506 \n    windows \n    1 \n    0.965 \n  \n\n 2507 \n    wine \n    1 \n    0.965 \n  \n\n 2508 \n    wings \n    1 \n    0.965 \n  \n\n 2509 \n    winking \n    1 \n    0.965 \n  \n\n 2510 \n    winning \n    1 \n    0.965 \n  \n\n 2511 \n    wipe \n    1 \n    0.965 \n  \n\n 2512 \n    wish \n    1 \n    0.965 \n  \n\n 2513 \n    wished \n    1 \n    0.965 \n  \n\n 2514 \n    withdrawal \n    1 \n    0.965 \n  \n\n 2515 \n    witter \n    1 \n    0.965 \n  \n\n 2516 \n    woke \n    1 \n    0.965 \n  \n\n 2517 \n    won \n    1 \n    0.965 \n  \n\n 2518 \n    wonderland \n    1 \n    0.965 \n  \n\n 2519 \n    workers \n    1 \n    0.965 \n  \n\n 2520 \n    works \n    1 \n    0.965 \n  \n\n 2521 \n    wriggled \n    1 \n    0.965 \n  \n\n 2522 \n    wringing \n    1 \n    0.965 \n  \n\n 2523 \n    wrinkled \n    1 \n    0.965 \n  \n\n 2524 \n    writhes \n    1 \n    0.965 \n  \n\n 2525 \n    yard \n    1 \n    0.965 \n  \n\n 2526 \n    year \n    1 \n    0.965 \n  \n\n 2527 \n    yolk \n    1 \n    0.965 \n  \n\n 2528 \n    zong \n    1 \n    0.965"
  },
  {
    "objectID": "studies/base.html#association-scores",
    "href": "studies/base.html#association-scores",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n2.2 Association scores",
    "text": "2.2 Association scores\nNext, we calculate the association scores with a call to assoc_scores(), providing first the target frequency list flist_target and then the reference frequency list flist_ref. We’ll store the result in a variable called scores_kw. Once we have our scores_kw, we can sort them by PMI and by signed \\(G^2\\).\n\n# calculate scores\nscores_kw <- assoc_scores(flist_target, flist_ref)\n\n# print scores, sorted by PMI\nprint(scores_kw, sort_order = \"PMI\")\n\nAssociation scores (types in list: 575, sort order criterion: PMI)\n          type    a  PMI G_signed|    b   c       d dir exp_a DP_rows\n 1        manu 15.5 7.25    152.3|10346 0.5 1614253   1 0.102   0.001\n 2   gladstone 13.5 7.24    132.2|10348 0.5 1614253   1 0.089   0.001\n 3      rohini 10.5 7.23    102.1|10352 0.5 1614253   1 0.070   0.001\n 4     troilus  9.5 7.22     92.1|10352 0.5 1614253   1 0.064   0.001\n 5 thistlewood  7.5 7.20     72.1|10354 0.5 1614253   1 0.051   0.001\n 6    guyanese  6.5 7.19     62.1|10356 0.5 1614253   1 0.045   0.001\n 7   stillborn  6.5 7.19     62.1|10356 0.5 1614253   1 0.045   0.001\n 8       cabin  5.5 7.17     52.2|10356 0.5 1614253   1 0.038   0.001\n 9      kampta  5.5 7.17     52.2|10356 0.5 1614253   1 0.038   0.001\n10      guyana  9.0 7.14     84.5|10352 1.0 1614251   1 0.064   0.001\n11      anarch  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n12    aperture  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n13      coolie  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n14    criseyde  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n15       ellar  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n16     grandpa  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n17        kaka  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n18   lachrimae  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n19   overboard  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n20    awakened  7.0 7.10     64.8|10354 1.0 1614251   1 0.051   0.001\n...\n<number of extra columns to the right: 7>\n\n# print scores, sorted by G_signed\nprint(scores_kw, sort_order = \"G_signed\")\n\nAssociation scores (types in list: 575, sort order criterion: G_signed)\n        type     a  PMI G_signed|    b      c       d dir  exp_a\n 1       her 106.0 4.14    420.1|10255  838.0 1613414   1  6.020\n 2       she  81.0 3.55    256.0|10280 1005.0 1613247   1  6.926\n 3   slavery  24.0 6.88    206.8|10337    8.0 1614244   1  0.204\n 4       sea  28.0 5.69    176.1|10333   57.0 1614195   1  0.542\n 5      manu  15.5 7.25    152.3|10346    0.5 1614253   1  0.102\n 6    turner  17.0 6.79    143.0|10344    7.0 1614245   1  0.153\n 7 gladstone  13.5 7.24    132.2|10348    0.5 1614253   1  0.089\n 8        my  62.0 2.70    129.4|10299 1432.0 1612820   1  9.528\n 9        he 102.0 1.76    107.0|10259 4612.0 1609640   1 30.064\n10       his  64.0 2.30    103.9|10297 1968.0 1612284   1 12.959\n11    rohini  10.5 7.23    102.1|10352    0.5 1614253   1  0.070\n12    nigger  11.0 6.95     96.7|10350    3.0 1614249   1  0.089\n13   african  15.0 5.64     93.2|10346   32.0 1614220   1  0.300\n14   troilus   9.5 7.22     92.1|10352    0.5 1614253   1  0.064\n15      shah  10.0 7.03     90.3|10351    2.0 1614250   1  0.077\n16      dead  19.0 4.60     87.6|10342  104.0 1614148   1  0.784\n17      shop  15.0 5.39     87.1|10346   41.0 1614211   1  0.357\n18    guyana   9.0 7.14     84.5|10352    1.0 1614251   1  0.064\n19       him  36.0 2.87     82.2|10325  739.0 1613513   1  4.943\n20      fish  13.0 5.38     75.2|10348   36.0 1614216   1  0.312\n...\n<number of extra columns to the right: 8>\n\n\n\n\n\n\n\n\nHaldane-Anscombe correction\n\n\n\nIt might seem disconcerting that some frequencies (a column) have decimals. This is because some values in the contingency table were 0 and 0.5 was added to all the cells to avoid divisions by 0 (the Haldane-Anscombe correction). If you would rather use a different small number in these cases, you can set haldane = FALSE in your assoc_scores() call and set your desired small value in the small_pos argument."
  },
  {
    "objectID": "studies/base.html#filtering-of-keywords-by-pmi-and-signed-g2",
    "href": "studies/base.html#filtering-of-keywords-by-pmi-and-signed-g2",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n2.3 Filtering of keywords by PMI and signed \\(G^2\\)\n",
    "text": "2.3 Filtering of keywords by PMI and signed \\(G^2\\)\n\nWe can use filter() to filter the keywords (i.e. the rows of scores_kw) by PMI and signed \\(G^2\\). We’ll store the result in a variable called top_scores_kw and again print the result, first sorted by PMI, then by signed \\(G^2\\). This allows us to explore which words are ranked higher by each of the measures.\n\ntop_scores_kw <- scores_kw %>% \n  filter(PMI >= 2 & G_signed >= 2)\n\n# print top_scores_kw, sorted by PMI\ntop_scores_kw %>%\n  print(sort_order = \"PMI\")\n\nAssociation scores (types in list: 269, sort order criterion: PMI)\n          type    a  PMI G_signed|    b   c       d dir exp_a DP_rows\n 1        manu 15.5 7.25    152.3|10346 0.5 1614253   1 0.102   0.001\n 2   gladstone 13.5 7.24    132.2|10348 0.5 1614253   1 0.089   0.001\n 3      rohini 10.5 7.23    102.1|10352 0.5 1614253   1 0.070   0.001\n 4     troilus  9.5 7.22     92.1|10352 0.5 1614253   1 0.064   0.001\n 5 thistlewood  7.5 7.20     72.1|10354 0.5 1614253   1 0.051   0.001\n 6    guyanese  6.5 7.19     62.1|10356 0.5 1614253   1 0.045   0.001\n 7   stillborn  6.5 7.19     62.1|10356 0.5 1614253   1 0.045   0.001\n 8       cabin  5.5 7.17     52.2|10356 0.5 1614253   1 0.038   0.001\n 9      kampta  5.5 7.17     52.2|10356 0.5 1614253   1 0.038   0.001\n10      guyana  9.0 7.14     84.5|10352 1.0 1614251   1 0.064   0.001\n11      anarch  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n12    aperture  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n13      coolie  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n14    criseyde  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n15       ellar  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n16     grandpa  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n17        kaka  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n18   lachrimae  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n19   overboard  4.5 7.14     42.3|10358 0.5 1614253   1 0.032   0.000\n20    awakened  7.0 7.10     64.8|10354 1.0 1614251   1 0.051   0.001\n...\n<number of extra columns to the right: 7>\n\n# print top_scores_kw, sorted by G_signed\ntop_scores_kw %>%\n  print(sort_order = \"G_signed\")\n\nAssociation scores (types in list: 269, sort order criterion: G_signed)\n        type     a  PMI G_signed|    b      c       d dir  exp_a\n 1       her 106.0 4.14    420.1|10255  838.0 1613414   1  6.020\n 2       she  81.0 3.55    256.0|10280 1005.0 1613247   1  6.926\n 3   slavery  24.0 6.88    206.8|10337    8.0 1614244   1  0.204\n 4       sea  28.0 5.69    176.1|10333   57.0 1614195   1  0.542\n 5      manu  15.5 7.25    152.3|10346    0.5 1614253   1  0.102\n 6    turner  17.0 6.79    143.0|10344    7.0 1614245   1  0.153\n 7 gladstone  13.5 7.24    132.2|10348    0.5 1614253   1  0.089\n 8        my  62.0 2.70    129.4|10299 1432.0 1612820   1  9.528\n 9       his  64.0 2.30    103.9|10297 1968.0 1612284   1 12.959\n10    rohini  10.5 7.23    102.1|10352    0.5 1614253   1  0.070\n11    nigger  11.0 6.95     96.7|10350    3.0 1614249   1  0.089\n12   african  15.0 5.64     93.2|10346   32.0 1614220   1  0.300\n13   troilus   9.5 7.22     92.1|10352    0.5 1614253   1  0.064\n14      shah  10.0 7.03     90.3|10351    2.0 1614250   1  0.077\n15      dead  19.0 4.60     87.6|10342  104.0 1614148   1  0.784\n16      shop  15.0 5.39     87.1|10346   41.0 1614211   1  0.357\n17    guyana   9.0 7.14     84.5|10352    1.0 1614251   1  0.064\n18       him  36.0 2.87     82.2|10325  739.0 1613513   1  4.943\n19      fish  13.0 5.38     75.2|10348   36.0 1614216   1  0.312\n20      poem  15.0 4.81     73.7|10346   69.0 1614183   1  0.536\n...\n<number of extra columns to the right: 8>\n\n\nHere you can keep reading to see the steps for collocation analysis or skip to the Section 4 for tips on how to move forward."
  },
  {
    "objectID": "studies/base.html#association-scores-1",
    "href": "studies/base.html#association-scores-1",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n3.1 Association scores",
    "text": "3.1 Association scores\nNext, we calculate the association score with a call to assoc_scores(), providing the full object coocs instead of the separated frequency lists. We store the result in a variable called scores_colloc. Once we have our scores_colloc, we can sort them by PMI and by signed \\(G^2\\).\n\n# calculate scores\nscores_colloc <- assoc_scores(coocs)\n\n# print scores, sorted by PMI\nprint(scores_colloc, sort_order = \"PMI\")\n\nAssociation scores (types in list: 220, sort order criterion: PMI)\n            type  a  PMI G_signed|   b   c       d dir exp_a DP_rows\n 1         shiny  3 7.95     29.4|3946   2 1619980   1 0.012   0.001\n 2           toe  3 7.95     29.4|3946   2 1619980   1 0.012   0.001\n 3          bang  6 6.95     47.9|3943  14 1619968   1 0.049   0.002\n 4       premium  3 6.02     19.6|3946  16 1619966   1 0.046   0.001\n 5 organisations  5 5.88     31.6|3944  30 1619952   1 0.085   0.001\n 6  corporations  3 5.57     17.6|3946  23 1619959   1 0.063   0.001\n 7 multinational  3 5.10     15.6|3946  33 1619949   1 0.088   0.001\n 8       climate  4 4.83     19.3|3945  54 1619928   1 0.141   0.001\n 9         twice  4 4.83     19.3|3945  54 1619928   1 0.141   0.001\n10 philosophical  3 4.81     14.4|3946  41 1619941   1 0.107   0.001\n11           gap  5 4.58     22.4|3944  81 1619901   1 0.209   0.001\n12             l  3 4.46     13.0|3946  53 1619929   1 0.136   0.001\n13         tests  4 4.26     16.2|3945  82 1619900   1 0.209   0.001\n14       vessels  3 4.20     11.9|3946  64 1619918   1 0.163   0.001\n15    difference 19 4.17     74.9|3930 414 1619568   1 1.053   0.005\n16         river  3 4.16     11.8|3946  66 1619916   1 0.168   0.001\n17        debate  4 4.10     15.3|3945  92 1619890   1 0.233   0.001\n18           box  4 4.07     15.2|3945  94 1619888   1 0.238   0.001\n19    assumption  5 3.92     18.0|3944 131 1619851   1 0.331   0.001\n20    dictionary  3 3.78     10.2|3946  87 1619895   1 0.219   0.001\n...\n<number of extra columns to the right: 7>\n\n# print scores, sorted by G_signed\nprint(scores_colloc, sort_order = \"G_signed\")\n\nAssociation scores (types in list: 220, sort order criterion: G_signed)\n            type   a  PMI G_signed|   b     c       d dir  exp_a\n 1             a 314 1.83    358.7|3635 36117 1583865   1 88.591\n 2    difference  19 4.17     74.9|3930   414 1619568   1  1.053\n 3       problem  22 2.94     51.7|3927  1159 1618823   1  2.872\n 4          bang   6 6.95     47.9|3943    14 1619968   1  0.049\n 5           one  51 1.56     43.5|3898  7041 1612941   1 17.246\n 6        really  29 2.16     42.2|3920  2633 1617349   1  6.473\n 7         quite  25 2.32     40.7|3924  2033 1617949   1  5.005\n 8          very  42 1.67     40.0|3907  5387 1614595   1 13.202\n 9           how  34 1.89     39.8|3915  3734 1616248   1  9.163\n10 organisations   5 5.88     31.6|3944    30 1619952   1  0.085\n11         shiny   3 7.95     29.4|3946     2 1619980   1  0.012\n12           toe   3 7.95     29.4|3946     2 1619980   1  0.012\n13       there's  24 1.79     25.7|3925  2822 1617160   1  6.921\n14         thank   8 3.61     25.6|3941   261 1619721   1  0.654\n15           too  12 2.75     25.5|3937   722 1619260   1  1.785\n16           gap   5 4.58     22.4|3944    81 1619901   1  0.209\n17         great  10 2.80     21.8|3939   582 1619400   1  1.440\n18     companies   6 3.72     20.0|3943   181 1619801   1  0.455\n19       premium   3 6.02     19.6|3946    16 1619966   1  0.046\n20       climate   4 4.83     19.3|3945    54 1619928   1  0.141\n...\n<number of extra columns to the right: 8>"
  },
  {
    "objectID": "studies/base.html#filtering-of-collocates-by-pmi-and-signed-g2",
    "href": "studies/base.html#filtering-of-collocates-by-pmi-and-signed-g2",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n3.2 Filtering of collocates by PMI and signed \\(G^2\\)\n",
    "text": "3.2 Filtering of collocates by PMI and signed \\(G^2\\)\n\nWe’ll use filter() to filter the collocates (i.e. the object scores_colloc) by PMI and signed \\(G\\), store the result in a variable called top_scores_colloc and print the result, first sorted by PMI, then by signed \\(G^2\\). This allows us to explore which words are ranked higher by each of the measures.\n\ntop_scores_colloc <- scores_colloc %>% \n  filter(PMI >= 2 & G_signed >= 2)\n\n# print top_scores_colloc, sorted by PMI\ntop_scores_colloc %>%\n  print(sort_order = \"PMI\")\n\nAssociation scores (types in list: 61, sort order criterion: PMI)\n            type  a  PMI G_signed|   b   c       d dir exp_a DP_rows\n 1         shiny  3 7.95     29.4|3946   2 1619980   1 0.012   0.001\n 2           toe  3 7.95     29.4|3946   2 1619980   1 0.012   0.001\n 3          bang  6 6.95     47.9|3943  14 1619968   1 0.049   0.002\n 4       premium  3 6.02     19.6|3946  16 1619966   1 0.046   0.001\n 5 organisations  5 5.88     31.6|3944  30 1619952   1 0.085   0.001\n 6  corporations  3 5.57     17.6|3946  23 1619959   1 0.063   0.001\n 7 multinational  3 5.10     15.6|3946  33 1619949   1 0.088   0.001\n 8       climate  4 4.83     19.3|3945  54 1619928   1 0.141   0.001\n 9         twice  4 4.83     19.3|3945  54 1619928   1 0.141   0.001\n10 philosophical  3 4.81     14.4|3946  41 1619941   1 0.107   0.001\n11           gap  5 4.58     22.4|3944  81 1619901   1 0.209   0.001\n12             l  3 4.46     13.0|3946  53 1619929   1 0.136   0.001\n13         tests  4 4.26     16.2|3945  82 1619900   1 0.209   0.001\n14       vessels  3 4.20     11.9|3946  64 1619918   1 0.163   0.001\n15    difference 19 4.17     74.9|3930 414 1619568   1 1.053   0.005\n16         river  3 4.16     11.8|3946  66 1619916   1 0.168   0.001\n17        debate  4 4.10     15.3|3945  92 1619890   1 0.233   0.001\n18           box  4 4.07     15.2|3945  94 1619888   1 0.238   0.001\n19    assumption  5 3.92     18.0|3944 131 1619851   1 0.331   0.001\n20    dictionary  3 3.78     10.2|3946  87 1619895   1 0.219   0.001\n...\n<number of extra columns to the right: 7>\n\n# print top_scores_colloc, sorted by G_signed\ntop_scores_colloc %>%\n  print(sort_order = \"G_signed\")\n\nAssociation scores (types in list: 61, sort order criterion: G_signed)\n            type  a  PMI G_signed|   b    c       d dir exp_a DP_rows\n 1    difference 19 4.17     74.9|3930  414 1619568   1 1.053   0.005\n 2       problem 22 2.94     51.7|3927 1159 1618823   1 2.872   0.005\n 3          bang  6 6.95     47.9|3943   14 1619968   1 0.049   0.002\n 4        really 29 2.16     42.2|3920 2633 1617349   1 6.473   0.006\n 5         quite 25 2.32     40.7|3924 2033 1617949   1 5.005   0.005\n 6 organisations  5 5.88     31.6|3944   30 1619952   1 0.085   0.001\n 7         shiny  3 7.95     29.4|3946    2 1619980   1 0.012   0.001\n 8           toe  3 7.95     29.4|3946    2 1619980   1 0.012   0.001\n 9         thank  8 3.61     25.6|3941  261 1619721   1 0.654   0.002\n10           too 12 2.75     25.5|3937  722 1619260   1 1.785   0.003\n11           gap  5 4.58     22.4|3944   81 1619901   1 0.209   0.001\n12         great 10 2.80     21.8|3939  582 1619400   1 1.440   0.002\n13     companies  6 3.72     20.0|3943  181 1619801   1 0.455   0.001\n14       premium  3 6.02     19.6|3946   16 1619966   1 0.046   0.001\n15       climate  4 4.83     19.3|3945   54 1619928   1 0.141   0.001\n16         twice  4 4.83     19.3|3945   54 1619928   1 0.141   0.001\n17    assumption  5 3.92     18.0|3944  131 1619851   1 0.331   0.001\n18  corporations  3 5.57     17.6|3946   23 1619959   1 0.063   0.001\n19          area  8 2.78     17.2|3941  472 1619510   1 1.167   0.002\n20        market 10 2.39     17.0|3939  776 1619206   1 1.911   0.002\n...\n<number of extra columns to the right: 7>"
  },
  {
    "objectID": "studies/base.html#saving-the-results-to-file",
    "href": "studies/base.html#saving-the-results-to-file",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n4.1 Saving the results to file",
    "text": "4.1 Saving the results to file\nWe can use write_assoc() to write an assoc_scores object to a file. That file is a tab delimited text file. It can easily be imported in spreadsheet tools but also be read again in RStudio, in future sessions, with read_assoc().\n\ntop_scores_kw %>%\n  write_assoc(\"ahlct001_top_keywords.csv\") \n# top_scores_kw <- read_assoc(\"ahlct001_top_keywords.csv\")\n\ntop_scores_colloc %>%\n  write_assoc(\"big_top_collocates.csv\") \n# top_scores_colloc <- read_assoc(\"big_top_collocates.csv\")"
  },
  {
    "objectID": "studies/base.html#a-nicer-way-of-showing-the-scores-in-a-report",
    "href": "studies/base.html#a-nicer-way-of-showing-the-scores-in-a-report",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n4.2 A nicer way of showing the scores in a report",
    "text": "4.2 A nicer way of showing the scores in a report\nWe can turn the scores into a tibble with the function as_tibble(). This allows people familiar with the tidyverse to use the rich set of tidyverse functions that are applicable to tibbles.\nIn Table 2 we print the top thirty keywords (according to PMI, and sorted by descending PMI) the tidyverse way:\n\ntop_scores_kw %>% # also valid for top_scores_colloc\n  as_tibble() %>%\n  select(type, a, PMI, G_signed) %>% # select 4 columns\n  arrange(desc(PMI)) %>%             # sort by PMI (descending) \n  head(30) %>%                       # select top 30 rows\n  kbl(col.names = c(\"Type\", \"Frequency\", \"PMI\", r\"(Signed $G^2$)\")) %>% \n  kable_minimal() %>% \n  scroll_box(height = \"400px\")\n\n\n\n\nTable 2:  Top 30 key words of the ‘ahlct001’ file in the BASE corpus, sorted by PMI. \n \n Type \n    Frequency \n    PMI \n    Signed $G^2$ \n  \n\n\n manu \n    15.5 \n    7.25 \n    152.3 \n  \n\n gladstone \n    13.5 \n    7.24 \n    132.2 \n  \n\n rohini \n    10.5 \n    7.23 \n    102.1 \n  \n\n troilus \n    9.5 \n    7.22 \n    92.1 \n  \n\n thistlewood \n    7.5 \n    7.20 \n    72.1 \n  \n\n guyanese \n    6.5 \n    7.19 \n    62.1 \n  \n\n stillborn \n    6.5 \n    7.19 \n    62.1 \n  \n\n cabin \n    5.5 \n    7.17 \n    52.2 \n  \n\n kampta \n    5.5 \n    7.17 \n    52.2 \n  \n\n guyana \n    9.0 \n    7.14 \n    84.5 \n  \n\n anarch \n    4.5 \n    7.14 \n    42.3 \n  \n\n aperture \n    4.5 \n    7.14 \n    42.3 \n  \n\n coolie \n    4.5 \n    7.14 \n    42.3 \n  \n\n criseyde \n    4.5 \n    7.14 \n    42.3 \n  \n\n ellar \n    4.5 \n    7.14 \n    42.3 \n  \n\n grandpa \n    4.5 \n    7.14 \n    42.3 \n  \n\n kaka \n    4.5 \n    7.14 \n    42.3 \n  \n\n lachrimae \n    4.5 \n    7.14 \n    42.3 \n  \n\n overboard \n    4.5 \n    7.14 \n    42.3 \n  \n\n awakened \n    7.0 \n    7.10 \n    64.8 \n  \n\n beatings \n    3.5 \n    7.10 \n    32.4 \n  \n\n booths \n    3.5 \n    7.10 \n    32.4 \n  \n\n diomede \n    3.5 \n    7.10 \n    32.4 \n  \n\n ellar's \n    3.5 \n    7.10 \n    32.4 \n  \n\n gladstone's \n    3.5 \n    7.10 \n    32.4 \n  \n\n jamaican \n    3.5 \n    7.10 \n    32.4 \n  \n\n melody \n    3.5 \n    7.10 \n    32.4 \n  \n\n miriam's \n    3.5 \n    7.10 \n    32.4 \n  \n\n mist \n    3.5 \n    7.10 \n    32.4 \n  \n\n paki \n    3.5 \n    7.10 \n    32.4 \n  \n\n\n\n\n\n\nWe can do the same sorting by signed \\(G^2\\) instead, as shown in Table 3:\n\ntop_scores_colloc %>% # also valid for top_scores_kw\n  as_tibble() %>%\n  select(type, a, PMI, G_signed) %>% # select 4 columns\n  arrange(desc(G_signed)) %>%        # sort by G_signed (descending)  \n  head(30) %>%                       # select top 30 rows\n  kbl(col.names = c(\"Type\", \"Frequency\", \"PMI\", r\"(Signed $G^2$)\")) %>% \n  kable_minimal() %>% \n  scroll_box(height = \"400px\")\n\n\n\n\nTable 3:  Top 30 collocates of ‘big’ in the BASE corpus, sorted by G signed. \n \n Type \n    Frequency \n    PMI \n    Signed $G^2$ \n  \n\n\n difference \n    19 \n    4.17 \n    74.9 \n  \n\n problem \n    22 \n    2.94 \n    51.7 \n  \n\n bang \n    6 \n    6.95 \n    47.9 \n  \n\n really \n    29 \n    2.16 \n    42.2 \n  \n\n quite \n    25 \n    2.32 \n    40.7 \n  \n\n organisations \n    5 \n    5.88 \n    31.6 \n  \n\n shiny \n    3 \n    7.95 \n    29.4 \n  \n\n toe \n    3 \n    7.95 \n    29.4 \n  \n\n thank \n    8 \n    3.61 \n    25.6 \n  \n\n too \n    12 \n    2.75 \n    25.5 \n  \n\n gap \n    5 \n    4.58 \n    22.4 \n  \n\n great \n    10 \n    2.80 \n    21.8 \n  \n\n companies \n    6 \n    3.72 \n    20.0 \n  \n\n premium \n    3 \n    6.02 \n    19.6 \n  \n\n climate \n    4 \n    4.83 \n    19.3 \n  \n\n twice \n    4 \n    4.83 \n    19.3 \n  \n\n assumption \n    5 \n    3.92 \n    18.0 \n  \n\n corporations \n    3 \n    5.57 \n    17.6 \n  \n\n area \n    8 \n    2.78 \n    17.2 \n  \n\n market \n    10 \n    2.39 \n    17.0 \n  \n\n issue \n    7 \n    2.96 \n    16.6 \n  \n\n question \n    11 \n    2.19 \n    16.3 \n  \n\n tests \n    4 \n    4.26 \n    16.2 \n  \n\n multinational \n    3 \n    5.10 \n    15.6 \n  \n\n debate \n    4 \n    4.10 \n    15.3 \n  \n\n box \n    4 \n    4.07 \n    15.2 \n  \n\n such \n    9 \n    2.35 \n    15.0 \n  \n\n company \n    5 \n    3.38 \n    14.5 \n  \n\n philosophical \n    3 \n    4.81 \n    14.4 \n  \n\n enough \n    7 \n    2.66 \n    14.1"
  },
  {
    "objectID": "studies/base.html#plotting-the-association-scores",
    "href": "studies/base.html#plotting-the-association-scores",
    "title": "Keywords and collocation analysis on the BASE corpus",
    "section": "\n4.3 Plotting the association scores",
    "text": "4.3 Plotting the association scores\nIf we store the tibble version of the results in a variable, such as top_scores_df, we can reuse that object in several subsequent instructions without having to recreate it time and again. Let’s work with the keywords here.\n\ntop_scores_df <- as_tibble(top_scores_kw)\n\nTo illustrate the use of the tibble version of the results, we can, for instance, generate plots on the basis of the object top_scores_df. Our base plot will map the signed \\(G^2\\) values on the x-axis and the PMI scores on the y-axis. We’ll also set a common theme to all plots with theme_set().\n\ntheme_set(theme_minimal(base_size = 15))\ng <- top_scores_df %>%\n  ggplot(aes(x = PMI, y = G_signed)) +\n  labs(x = \"PMI\", y = \"Signed G\")\n\nIn Figure 1, we build a simple scatter plot with points representing the different words. The plot gives us an idea of to which extent both measures correlate.\n\ng + geom_point()\n\n\n\nFigure 1: Scatterplot of PMI by signed \\(G^2\\) in the keyword analysis.\n\n\n\n\nWe see that the measures do correlate a bit, but definitely not perfectly. In fact, should you build similar plots for other combinations of measures, you’ll find that some pair correlate much more clearly than what we see here.\nLet’s inspect to which extent absolute frequencies can explain the discrepancies between PMI and signed \\(G^2\\). In Figure 2 we have the frequencies in the target corpus (i.e. the values in the a cell of the contingency tables) mapped to the size of the symbols. We see that high frequencies tend to relatively boost signed G scores and that low frequencies appear to relatively increase the probability of obtaining a high PMI score.\n\ng + geom_point(aes(size = a))\n\n\n\nFigure 2: Scatterplot of PMI by signed \\(G^2\\) in the keyword analysis, with size reflecting frequency.\n\n\n\n\nWe can also plot the names of the words instead of using symbols, using the geom_text() function, as shown in Figure 3. Notice that in top_scores_df the names of the types are stored in a column called type.\n\ng + geom_text(aes(label = type))\n\n\n\nFigure 3: Scatterplot of PMI by signed \\(G^2\\) in the keyword analysis, with types instead of dots.\n\n\n\n\nFor a more sophisticated plot, the ggrepel package allows us to add text close to the position of their datapoints, avoiding overlap. To create Figure 4 we define a smaller dataframe with the subset of keywords for which \\(G^2\\) is larger than 100 and provide it as data for ggrepel::geom_text_repel(). The x and y aesthetics are inherited from the ggplot() call in g.\n\nhigh_G_signed <- top_scores_df %>% \n  filter(G_signed > 100) # extract types with high G_signed\n\ng + geom_point() +\n  ggrepel::geom_text_repel(data = high_G_signed, aes(label = type))\n\n\n\nFigure 4: Scatterplot of PMI by signed \\(G^2\\) in the keyword analysis, labeled with the highest \\(G^2\\) values."
  },
  {
    "objectID": "studies/ca-trump-clinton.html",
    "href": "studies/ca-trump-clinton.html",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "",
    "text": "This document illustrates how to use correspondence analysis (CA) to investigate the similarities between Clinton speeches and Trump speeches in a corpus that collects them. The approach adopts a bird’s-eye perspective, looking at the broad picture we get if we aggregate over a wide range of features of the speeches.\nA major advantage of the technique is that we can both compare the different documents based on the selected features, but also compare the features based on the documents they occur in and also see which features or groups of features are the ones that account for the most important dissimilarities between the documents.\nWe will illustrate four analyses with only minimal differences between each other. In all of them, the workflow consists of collecting frequency lists from each of the files in our corpus and creating a matrix with one row per document and one column per type that occurs in the corpus. The cells contain the occurrence frequency: a cell \\(ij\\) will contain the occurrence of type \\(j\\) in the document \\(i\\). Such matrix of counts will be given to ca::ca(), a function that runs correspondence analysis and returns, among other things, the coordinates of row items and of column items in the principal components as well as the variance covered by each dimension. Section 1.2 will show the full code to obtain the data and plot it before delving into the actual case studies.\nThe difference between the analyses is the definition of the types collected in the frequency lists. Section 2 will select only function words from a predefined list as features, since they have been shown to reliably discriminate between different authors and registers. Section 3, on the other hand, will rely on high frequency content words by first excluding the types in a stoplist and then only keeping the top 150 items. Finally, Section 4 and Section 5 will use bigrams and trigrams as types, respectively. Section 6 will round up showing how to condense the code of all studies in one shorter script."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#collect-the-corpus-filenames",
    "href": "studies/ca-trump-clinton.html#collect-the-corpus-filenames",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n1.1 Collect the corpus filenames",
    "text": "1.1 Collect the corpus filenames\nFirst, we collect the names of the corpus files and store them in an object called fnames. From this we also derive the short filenames (stored in short_fnames) and a character vector with, for each filename, the subcorpus (stored in sub_corp). The first variable, short_fnames, is also an fnames object and will provide more practical names for plotting. The second variable, sub_corp, is a character string and will help us assign colors in the plots based on whether the documents correspond to Clinton or Trump speeches. Table 1 illustrates a few of these values.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t forget to adapt the corpus_folder variable with the path to your copy of the corpus!\n\n\n\ncorpus_folder <- here(\"studies\", \"_corpora\", \"clinton_trump\")\nfnames <- get_fnames(corpus_folder)\nshort_fnames <- short_names(fnames)\nsub_corp <- fnames %>%\n  re_retrieve_first(\"/clinton_trump/([^/]+)\", requested_group = 1)\n\n\nCodeset.seed(8541)\ntibble(\n  fnames = gsub(corpus_folder, \"\", fnames),\n  short_fnames = short_fnames, sub_corp = sub_corp\n) %>%\n  group_by(sub_corp) %>% \n  slice_sample(n = 6) %>%\n  kbl() %>% \n  kable_paper()\n\n\n\n\nTable 1:  Five random filenames and their corresponding values in short_fnames and sub_corp. \n \n fnames \n    short_fnames \n    sub_corp \n  \n\n\n /Clinton/Clinton_2016-11-09.txt \n    Clinton_2016-11-09 \n    Clinton \n  \n\n /Clinton/Clinton_2016-09-29.txt \n    Clinton_2016-09-29 \n    Clinton \n  \n\n /Clinton/Clinton_2016-11-06-B.txt \n    Clinton_2016-11-06-B \n    Clinton \n  \n\n /Clinton/Clinton_2016-08-31.txt \n    Clinton_2016-08-31 \n    Clinton \n  \n\n /Clinton/Clinton_2016-09-30.txt \n    Clinton_2016-09-30 \n    Clinton \n  \n\n /Clinton/Clinton_2016-10-03.txt \n    Clinton_2016-10-03 \n    Clinton \n  \n\n /Trump/Trump_2016-09-13-B.txt \n    Trump_2016-09-13-B \n    Trump \n  \n\n /Trump/Trump_2016-08-19.txt \n    Trump_2016-08-19 \n    Trump \n  \n\n /Trump/Trump_2016-09-13.txt \n    Trump_2016-09-13 \n    Trump \n  \n\n /Trump/Trump_2016-10-18.txt \n    Trump_2016-10-18 \n    Trump \n  \n\n /Trump/Trump_2016-10-11.txt \n    Trump_2016-10-11 \n    Trump \n  \n\n /Trump/Trump_2016-10-12.txt \n    Trump_2016-10-12 \n    Trump"
  },
  {
    "objectID": "studies/ca-trump-clinton.html#sec-code",
    "href": "studies/ca-trump-clinton.html#sec-code",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n1.2 General code",
    "text": "1.2 General code\nThe code below will be implemented in each of the case studies below with the only difference in the freqlist() call in line 2 and the actual value of features (in lines 2 and 5).\n\n\nIn practice, if you were to run an analysis where this much code will be used over and over again with multiple modifications, it would be preferable to wrap it in custom functions. Section 6 will show you how, if you’re interested.\n\nd <- map(setNames(fnames, short_fnames), function(fname) {\n    freqlist(fname, ...)[features]\n  }) %>%\n  bind_cols() %>% \n  data.frame(row.names = features) %>% \n  as.matrix() %>% \n  t() %>% \n  drop_empty_rc()\n\nd_ca <- ca(d)\n\ntexts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"text\") %>% \n  mutate(Subcorpus = sub_corp)\n\nwords_df <- col_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"word\")\n\ndim_1 <- sprintf(\"Dimension 1 (%.2f %%)\", summary(d_ca)$scree[1,3])\ndim_2 <- sprintf(\"Dimension 2 (%.2f %%)\", summary(d_ca)$scree[2,3])\n\nggplot(words_df, aes(x = V1, y = V2)) +\n  geom_text(aes(label = word), color = \"gray60\") +\n  geom_point(data = texts_df, aes(color = Subcorpus)) +\n  scale_color_manual(values = c(\"#0000CD\",\"#DC143C\")) +\n  geom_hline(yintercept = 0, color = \"darkgray\") +\n  geom_vline(xintercept = 0, color = \"darkgray\") +\n  theme_bw(base_size = 12) +\n  labs(x = dim_1, y = dim_2) +\n  coord_fixed()\n\nThe first part of the code builds the matrix that we will apply the correspondence analysis to. This is an object of the data type matrix with as its rows the files and as its columns the features, and in the cells the absolute frequencies of the features in the files.\nThe code in lines 1-3 uses the map() function from the purrr package to apply the same process to each file in the corpus, returning a comparable output. This output is a frequency list computed with freqlist() and then adjusted to return values for all and only the elements in the features vector, which will be different in each study. The output of map() is a list, and we will combine its elements into a tibble with one column per file by means of bind_cols() in line 4. Because we named the files with the short names in line 1, via the setNames() call, the column names are the elements in short_fnames. Line 5 turns the tibble into a dataframe so we can define features as the row names.\nLines 6-8 turn this dataframe into a matrix, transpose it (so that the features become the columns and the files, the rows) and drop the rows and columns with only zero frequencies.\nIn line 10, we run correspondence analysis on this matrix, which we store in the variable d_ca. We could then inspect it by calling d_ca or summary(d_ca).\nThen, lines 12-17 extract the coordinates of the rows and columns for plotting and create tibbles with the labels, positions and, in the case of the rows, the color coding based on the subcorpora. mclm::row_pcoord() and mclm::col_pcoord() extract the coordinates of the rows and columns from the object d_ca; we are only interested in the first two columns, which are the coordinates of the two first principal components, therefore we add [,c(1,2)]. We then turn the matrix into a tibble assigning the column names “text” and “word” to the row names of the matrices. In the case of the documents, we also add the color coding column by creating a column “Subcorpus” and filling it with the vector sub_corp.\nLines 19 and 20 use the variance information from summary(d_ca) to enrich the labels of the axes in the plot. The rest of the lines create a biplot.\n\n\n\n\n\n\nParsing the ggplot() call\n\n\n\n\n\nWhat does each line from 22 to 30 do in the code chunk above?\n\nLine 22 calls ggplot() with the dataframe of features (the columns in our matrix) as dataset, assigning the variables “V1” and “V2” to the x and y coordiantes respectively. These are the automatic names given by as_tibble() in line 17 when we converted a matrix without column names1.\nLine 23 plots the items of the variable “word” (the types of our features) as text, in the color “gray60”.\nLine 24 plots the items of the dataframe of documents (the rows of the original matrix) as dots. The call inherits the x and y mappings from the ggplot() call, so that “V1” and “V2” will automatically be mapped into the axes. In addition, we map “Subcorpus” to the color aesthetics.\nLine 25 defines manually the colors to map to the “Subcorpus” variable. We could also use a variety of palettes, different colors, or remove the line and use the default color scheme of ggplot2.\nLines 26 and 27 add a horizontal and vertical line respectively, the former where \\(y = 0\\) and the latter where \\(x = 0\\), both in dark gray.\nLine 28 sets the theme and the base size for the font. If you create multiple plots, you can set this for all your plots with theme_set().\nLine 29 assigns the variables dim_1 and dim_2, defined in lines 19 and 20, as the labels of the x and y axes.\nLine 30 indicates that the units of both axes should be the same, which is not the default case for ggplot2 plots. The numbers themselves of the coordinates are not meaningful, but the distances between the points are, and they assume that the distance between the point in \\((0,1)\\) (where \\(x = 0\\) and \\(y = 1\\)) and the centroid \\((0, 0)\\) is the same as the distance between the point \\((1,0)\\) and the centroid \\((0,0)\\). coord_fixed() makes sure that’s the case."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#specifying-the-features",
    "href": "studies/ca-trump-clinton.html#specifying-the-features",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n2.1 Specifying the features",
    "text": "2.1 Specifying the features\nWe start by reading the names of the features for this analysis from the file function-words.txt and storing then into an object called features.\n\nfeatures <- read_types(here(\"studies\", \"assets\", \"ca-trump-clinton\", \"function-words.txt\"))\nprint(features, n = 10)\n\nType collection of length 149\n      type\n   -------\n 1   about\n 2  across\n 3 against\n 4   along\n 5  around\n 6      at\n 7  behind\n 8  beside\n 9 besides\n10      by\n...\n\n\nAs shown when printing it, the object features contains 149 function words.2"
  },
  {
    "objectID": "studies/ca-trump-clinton.html#building-the-file-by-feature-matrix",
    "href": "studies/ca-trump-clinton.html#building-the-file-by-feature-matrix",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n2.2 Building the file-by-feature matrix",
    "text": "2.2 Building the file-by-feature matrix\nOnce we have collected our features we can create the matrix with one row per document and one column per feature.\n\nd <- map(setNames(fnames, short_fnames), function(fname) {\n    freqlist(fname)[features]\n  }) %>%\n  bind_cols() %>% \n  data.frame(row.names = features) %>% \n  as.matrix() %>% \n  t() %>% \n  drop_empty_rc()\n\nThe top-left part of the resulting matrix (first ten rows, first ten columns) is shown in Table 2. Each row is a speech, each column is a function word, and the values are the absolute frequencies of a given function word in a igven document.\n\nCodekbl(d[1:10, 1:10]) %>% kable_paper()\n\n\n\n\nTable 2:  First rows and columns of a document-by-feature matrix where the rows are Clinton and Trump speeches and the columns are function words. \n \n   \n    about \n    across \n    against \n    along \n    around \n    at \n    behind \n    besides \n    by \n    despite \n  \n\n\n Clinton_2016.07.28 \n    11 \n    4 \n    3 \n    1 \n    3 \n    26 \n    1 \n    0 \n    20 \n    0 \n  \n\n Clinton_2016.07.29 \n    24 \n    4 \n    0 \n    0 \n    2 \n    8 \n    1 \n    0 \n    7 \n    0 \n  \n\n Clinton_2016.08.01 \n    7 \n    3 \n    0 \n    0 \n    1 \n    3 \n    0 \n    0 \n    2 \n    0 \n  \n\n Clinton_2016.08.05 \n    22 \n    5 \n    3 \n    1 \n    5 \n    30 \n    4 \n    0 \n    17 \n    0 \n  \n\n Clinton_2016.08.10 \n    19 \n    3 \n    0 \n    1 \n    2 \n    5 \n    0 \n    0 \n    9 \n    0 \n  \n\n Clinton_2016.08.11 \n    18 \n    10 \n    4 \n    1 \n    3 \n    23 \n    2 \n    0 \n    17 \n    0 \n  \n\n Clinton_2016.08.15 \n    41 \n    4 \n    6 \n    2 \n    5 \n    30 \n    0 \n    0 \n    18 \n    0 \n  \n\n Clinton_2016.08.16 \n    16 \n    5 \n    0 \n    1 \n    1 \n    7 \n    0 \n    0 \n    4 \n    0 \n  \n\n Clinton_2016.08.17 \n    34 \n    4 \n    1 \n    0 \n    0 \n    19 \n    0 \n    0 \n    7 \n    0 \n  \n\n Clinton_2016.08.25 \n    17 \n    2 \n    3 \n    0 \n    2 \n    9 \n    0 \n    0 \n    11 \n    0"
  },
  {
    "objectID": "studies/ca-trump-clinton.html#running-correspondence-analysis",
    "href": "studies/ca-trump-clinton.html#running-correspondence-analysis",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n2.3 Running correspondence analysis",
    "text": "2.3 Running correspondence analysis\nThen we run the actual correspondence analysis and store the result in an object called d_ca. For reasons of brevity, we will skip the inspection of the summary report of the analysis, which we would normally do with summary(d_ca).\n\nd_ca <- ca(d)\n# summary(d_ca)"
  },
  {
    "objectID": "studies/ca-trump-clinton.html#biplot",
    "href": "studies/ca-trump-clinton.html#biplot",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n2.4 Biplot",
    "text": "2.4 Biplot\nNext, we build the biplot, with color indicating the subcorpus that each file belongs to. First, we prepare the data needed for the plot. If you wanted to inspect the full output of row_pcoord() or col_pcoord(), you can run them separately first (e.g. row_pcoord(d_ca) %>% View()).\n\ntexts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"text\") %>% \n  mutate(Subcorpus = sub_corp)\nwords_df <- col_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"word\")\n\nTable 3 shows a random sample from texts_df and words_df, i.e. the rows and columns of the dataset with their principal component coordinates and the corpus they correspond to. The columns “V1” and “V2” contain the positions in the first and second dimension respectively. In Table 3 (a) we can already see that Clinton documents tend to be in the bottom half of the plot (“V2” is negative), whereas Trump documents tend to be in the top half (“V2” is positive).\n\nCodeset.seed(2022)\ntexts_df %>% group_by(Subcorpus) %>% \n  slice_sample(n = 5) %>% kbl() %>% \n  kable_paper(full_width = FALSE)\nwords_df %>%\n  slice_sample(n = 10) %>% kbl() %>% \n  kable_paper(full_width = FALSE)\n\n\nTable 3: Subset of speeches and function words with principal components.\n\n\n\n\n\n(a) Texts \n \n text \n    V1 \n    V2 \n    Subcorpus \n  \n\n\n Clinton_2016.11.09 \n    0.0472921 \n    -0.1750603 \n    Clinton \n  \n\n Clinton_2016.09.06 \n    -0.0619775 \n    -0.1397667 \n    Clinton \n  \n\n Clinton_2016.08.31 \n    0.0980152 \n    -0.2038008 \n    Clinton \n  \n\n Clinton_2016.08.05 \n    -0.1063086 \n    -0.1732206 \n    Clinton \n  \n\n Clinton_2016.08.11 \n    -0.0171809 \n    -0.2379479 \n    Clinton \n  \n\n Trump_2016.10.31 \n    0.1000379 \n    0.0894802 \n    Trump \n  \n\n Trump_2016.11.07.B \n    -0.0041106 \n    0.1529944 \n    Trump \n  \n\n Trump_2016.08.15 \n    0.3388391 \n    -0.1418274 \n    Trump \n  \n\n Trump_2016.08.02 \n    -0.1862853 \n    0.1850968 \n    Trump \n  \n\n Trump_2016.10.14.B \n    -0.0731448 \n    0.1318714 \n    Trump \n  \n\n\n\n\n\n\n(b) Features \n \n word \n    V1 \n    V2 \n  \n\n\n while \n    0.1871077 \n    0.0183116 \n  \n\n about \n    -0.1634588 \n    -0.1360846 \n  \n\n they \n    -0.1100616 \n    0.3573612 \n  \n\n he \n    -0.4679407 \n    -0.3371612 \n  \n\n after \n    0.2224265 \n    -0.1108768 \n  \n\n might \n    -0.3473552 \n    -0.2371209 \n  \n\n what \n    -0.1722558 \n    -0.0841692 \n  \n\n during \n    -0.0927357 \n    0.1178028 \n  \n\n themselves \n    -0.0473160 \n    -0.3076619 \n  \n\n therefore \n    -0.5136063 \n    -0.2700845 \n  \n\n\n\n\n\n\nThe plot is shown in Figure 1. We can see that 16.61% of the variation is covered by the first dimension and 12.99% by the second dimension. In addition, both subcorpora form very clear clusters, mostly divided by the second dimension: features higher on the y-axis are more characteristic of Trump’s speeches, and those lower on the y-axis are more characteristic of Clinton’s, as we could already see from Table 3 (a).\n\ndim_1 <- sprintf(\"Dimension 1 (%.2f%%)\", summary(d_ca)$scree[1,3])\ndim_2 <- sprintf(\"Dimension 2 (%.2f%%)\", summary(d_ca)$scree[2,3])\n\nggplot(words_df, aes(x = V1, y = V2)) +\n  geom_text(aes(label = word), color = \"gray60\") +\n  geom_point(data = texts_df, aes(color = Subcorpus)) +\n  scale_color_manual(values = c(\"#0000CD\",\"#DC143C\")) +\n  geom_hline(yintercept = 0, color = \"darkgray\") +\n  geom_vline(xintercept = 0, color = \"darkgray\") +\n  theme_bw(base_size = 12) +\n  labs(x = dim_1, y = dim_2) +\n  coord_fixed()\n\n\n\nFigure 1: Biplot of correspondence analysis of Clinton and Trump speeches based on function words."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#typical-clinton-features-vs.-typical-trump-features",
    "href": "studies/ca-trump-clinton.html#typical-clinton-features-vs.-typical-trump-features",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n2.5 Typical Clinton features vs. typical Trump features",
    "text": "2.5 Typical Clinton features vs. typical Trump features\nSince the dimension that sets apart the Clinton speeches (top) most clearly from the Trump speeches (bottom) is the second dimension (y-axis), we may want to inspect the features that occupy extreme positions on the y-axis (and that hence contribute importantly to the y-axis).\nThe code below sorts the dataframe with features information based on their coordinates in the second dimension (“V2”, i.e. the vertical axis) and extracts the first 20 elements. clinton_words and trump_words are thus vectors with the 20 words most characteristic of Clinton’s and Trump’s speeches respectively.\n\nclinton_words <- words_df %>%\n  arrange(desc(V2)) %>%\n  head(20) %>% pull(word)\ntrump_words <- words_df %>%\n  arrange(V2) %>%\n  head(20) %>% pull(word)\n\nThe markdown text below adds asterisks to each word to print them in italics and brings them together in an enumeration with the help of glue::glue_collapse(), resulting in the text right below it.\nThe words most characteristic of Clinton's speeches are\n`r glue::glue_collapse(paste0(\"*\", clinton_words, \"*\"), sep = \", \", last = \" and \")`.\nThose most characteristic of Trump's speeches, instead, are\n`r glue::glue_collapse(paste0(\"*\", trump_words, \"*\"), sep = \", \", last = \" and \")`.\n\nThe words most characteristic of Clinton’s speeches are anyway, little, mine, they, over, twice, myself, second, anything, though, otherwise, onto, along, never, such, them, her, two, but and ones. Those most characteristic of Trump’s speeches, instead, are someone, himself, toward, each, shall, everyone, ourselves, ours, his, anyone, often, several, own, across, both, sometimes, whose, may, should and my."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#specifying-the-features-1",
    "href": "studies/ca-trump-clinton.html#specifying-the-features-1",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n3.1 Specifying the features",
    "text": "3.1 Specifying the features\nFrom a technical perspective, the only difference with the case study in Section 2 is how we define the features. This time, we build a complete frequency list of the whole corpus, we then remove a number of stop words (mostly function words, but also some undesired types such as 000 and --), and finally we treat the top 150 (i.e. the 150 highest frequency items) of the remaining items as our features. The number 150 is an arbitrary choice but it makes this study more comparable to the one in Section 2.\n\nstop_list <- read_types(here(\"studies\", \"assets\", \"ca-trump-clinton\", \"stop_list.txt\"))\nprint(stop_list, n = 5)\n\nType collection of length 154\n     type\n  -------\n1     000\n2   about\n3  across\n4 against\n5   along\n...\n\nfeatures <- freqlist(fnames) %>%\n  drop_types(stop_list) %>%\n  keep_pos(1:150) %>%\n  as_types() %>%\n  print(n = 10)\n\nType collection of length 150\n        type\n   ---------\n 1     again\n 2       ago\n 3      also\n 4   america\n 5  american\n 6 americans\n 7  applause\n 8       are\n 9  audience\n10      back\n..."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#next-steps-and-plot",
    "href": "studies/ca-trump-clinton.html#next-steps-and-plot",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n3.2 Next steps and plot",
    "text": "3.2 Next steps and plot\nThe next steps, all the way up to the creation of the plot, are completely analogous to the previous analysis.\n\nd <- map(setNames(fnames, short_fnames), function(fname) {\n    freqlist(fname)[features]\n  }) %>%\n  bind_cols() %>% \n  data.frame(row.names = features) %>% \n  as.matrix() %>% \n  t() %>% \n  drop_empty_rc()\n\nd_ca <- ca(d)\n\ntexts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"text\") %>% \n  mutate(Subcorpus = sub_corp)\n\nwords_df <- col_pcoord(d_ca)[,c(1, 2)] %>% \n  as_tibble(rownames = \"word\")\n\nTable 4 shows again a random subset of documents with their new coordinates, as well as a random subset of features —this time content words— and their coordinates. Figure 2 shows the biplot.\n\nCodeset.seed(2022)\ntexts_df %>% group_by(Subcorpus) %>% \n  slice_sample(n = 5) %>% kbl() %>% \n  kable_paper(full_width = FALSE)\nwords_df %>%\n  slice_sample(n = 10) %>% kbl() %>% \n  kable_paper(full_width = FALSE)\n\n\nTable 4: Subset of speeches and content words with principal components.\n\n\n\n\n\n(a) Texts \n \n text \n    V1 \n    V2 \n    Subcorpus \n  \n\n\n Clinton_2016.11.09 \n    -0.4261368 \n    0.0843955 \n    Clinton \n  \n\n Clinton_2016.09.06 \n    -0.2692764 \n    0.1529539 \n    Clinton \n  \n\n Clinton_2016.08.31 \n    -0.4620391 \n    0.0589428 \n    Clinton \n  \n\n Clinton_2016.08.05 \n    -0.2577882 \n    0.3072020 \n    Clinton \n  \n\n Clinton_2016.08.11 \n    -0.4792801 \n    0.0970303 \n    Clinton \n  \n\n Trump_2016.10.31 \n    0.0593353 \n    -0.1727285 \n    Trump \n  \n\n Trump_2016.11.07.B \n    0.1602217 \n    -0.1026059 \n    Trump \n  \n\n Trump_2016.08.15 \n    -0.2888413 \n    -0.0287607 \n    Trump \n  \n\n Trump_2016.08.02 \n    0.3474765 \n    0.1105980 \n    Trump \n  \n\n Trump_2016.10.14.B \n    0.1594495 \n    -0.0278322 \n    Trump \n  \n\n\n\n\n\n\n(b) Features \n \n word \n    V1 \n    V2 \n  \n\n\n talk \n    -0.0728850 \n    0.2705457 \n  \n\n again \n    -0.0107773 \n    -0.3857531 \n  \n\n great \n    0.1749042 \n    -0.0561112 \n  \n\n doesn't \n    -0.0225670 \n    0.1318185 \n  \n\n put \n    -0.0909718 \n    -0.0302617 \n  \n\n think \n    0.0991768 \n    0.3251066 \n  \n\n say \n    0.1411930 \n    0.1319127 \n  \n\n been \n    0.0034804 \n    -0.0325135 \n  \n\n i've \n    -0.0535805 \n    0.3076036 \n  \n\n win \n    0.2972385 \n    -0.2643769 \n  \n\n\n\n\n\n\n\ndim_1 <- sprintf(\"Dimension 1 (%.2f %%)\", summary(d_ca)$scree[1,3])\ndim_2 <- sprintf(\"Dimension 2 (%.2f %%)\", summary(d_ca)$scree[2,3])\n\nggplot(words_df, aes(x = V1, y = V2)) +\n  geom_text(aes(label = word), color = \"gray60\") +\n  geom_point(data = texts_df, aes(color = Subcorpus)) +\n  scale_color_manual(values = c(\"#0000CD\",\"#DC143C\")) +\n  geom_hline(yintercept = 0, color = \"darkgray\") +\n  geom_vline(xintercept = 0, color = \"darkgray\") +\n  theme_bw(base_size = 12) +\n  labs(x = dim_1, y = dim_2) +\n  coord_fixed()\n\n\n\nFigure 2: Biplot of correspondence analysis of Clinton and Trump speeches based on content words.\n\n\n\n\nWe can see that 14.09% of the variation is covered by the first dimension and 10.69% by the second dimension. This time it is the combination of the two dimensions that sets apart the two groups of speeches. This makes it a bit more difficult to select the items on the basis of their coordinates. One approach is to select quadrants or regions, either to zoom in on the plot (Figure 3) or to obtain a selection of words.\n\nggplot(words_df, aes(x = V1, y = V2)) +\n  geom_text(aes(label = word), color = \"gray60\") +\n  geom_point(data = texts_df, aes(color = Subcorpus)) +\n  scale_color_manual(values = c(\"#0000CD\",\"#DC143C\")) +\n  geom_hline(yintercept = 0, color = \"darkgray\") +\n  geom_vline(xintercept = 0, color = \"darkgray\") +\n  theme_bw(base_size = 12) +\n  labs(x = dim_1, y = dim_2) +\n  coord_fixed(xlim = c(0, 0.5), ylim = c(-0.5, 0))\n\n\n\nFigure 3: Biplot of correspondence analysis of Clinton and Trump speeches based on content words, zoomed in for the bottom right quadrant.\n\n\n\n\nWe can also compute the euclidean distance from each point to the center of the plot to identify the items that are farthest from it (the dist_to_center variable). The code below selects all words in the bottom right quadrant (even beyond the zoomed-in region in Figure 3) and order thems by distance to the center; the markdown text below it lists some of them, resulting in the block text underneath.\n\nbottom_right <- words_df %>%\n  filter(V1 >= 0 & V2 <= 0) %>% \n  mutate(dist_to_center = sqrt(V1^2 + V2^2)) %>% \n  arrange(desc(dist_to_center)) %>% \n  pull(word)\n\nThere are `r length(bottom_right)` words in the bottom right quadrant of @fig-content;\nthe fifteen farthest from the center of the plot are\n`r glue::glue_collapse(paste0(\"*\", head(bottom_right, 15), \"*\"), sep = \", \", last = \" and \")`.\n\nThere are 42 words in the bottom right quadrant of Figure 2; the fifteen farthest from the center of the plot are booing, mexico, they’re, wall, trade, bad, percent, gonna, folks, win, she’s, stop, what’s, money and four."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#compile-frequencies",
    "href": "studies/ca-trump-clinton.html#compile-frequencies",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n6.1 Compile frequencies",
    "text": "6.1 Compile frequencies\nThe function below, compile_frequencies(), takes as arguments the features, the fnames, the short_fnames and the different arguments we want to give to freqlist(), since that will change in each case study. In line 1 we name the function, and in lines 1-4 we list the possible arguments. The first three are named, whereas ... allows us to add an undefined number or arguments. They will be given to freqlist() in line 6, e.g. ngram_size = 2.\n\ncompile_frequencies <- function(features,\n                                fnames,\n                                short_fnames,\n                                ...) {\n  d <- map(setNames(fnames, short_fnames), function(fname) {\n    freqlist(fname, ...)[features]\n  }) %>%\n  bind_cols() %>% \n  data.frame(row.names = features) %>% \n  as.matrix() %>% \n  t() %>% \n  drop_empty_rc()\n}"
  },
  {
    "objectID": "studies/ca-trump-clinton.html#obtain-coordinates",
    "href": "studies/ca-trump-clinton.html#obtain-coordinates",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n6.2 Obtain coordinates",
    "text": "6.2 Obtain coordinates\nBy giving the output of compile_frequencies() to ca(), we obtain a correspondence analysis object that we could call d_ca. Next, we might want to write one or two functions to create the small tibbles with the row and column coordinates and the variables for plotting. Here I will show how to create one function get_coords() that takes the object d_ca and the subcorpora vector and returns a named list with two elements. Alternatively, you could also write two separate functions: one for the rows and one for the columns (the latter wouldn’t need the subcorpora vector).\nHere, lines 2-4 replicate the texts_df definition and lines 6-7, that of words_df. Lines 9-12 create and return a named list with both elements.\n\nget_coords <- function(d_ca, sub_corp) {\n  texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% \n    as_tibble(rownames = \"text\") %>% \n    mutate(Subcorpus = sub_corp)\n  \n  words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% \n    as_tibble(rownames = \"word\")\n  \n  list(\n    rows = texts_df,\n    cols = words_df\n  )\n}\n\nIf you wrote two separate functions, you could call them to generate texts_df and words_df separately. Instead, the output of get_coords() will be a list, let’s say ca_coords, of which the element ca_coords$rows will correspond to texts_df and ca_coords$cols will correspond to words_df."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#plot",
    "href": "studies/ca-trump-clinton.html#plot",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n6.3 Plot",
    "text": "6.3 Plot\nFinally, we can write a plotting function that covers everything we need to generate our plot. Such functions are extremely useful when writing papers in which you need to print multiple different plots with similar aesthetic characteristics. Moreover, if you have one function to generate a lot of different plots, i.e. scatterplots for different datasets, and you decide to change the color palette, the background color or the size of the points, you can simply adjust the function, and all the plots generated with it will change in response.3\nThe function below, plot_ca(), takes the result of get_coords() and a list with variance data extracted from summary(d_ca)$scree[,3]. Lines 2 and 3 take the variance information to include it in the axis labels, later defined in line 12. In lines 5 and 7, what used to be texts_df and words_df are now ca_coords$rows and ca_coords$cols, i.e. the elements “rows” and “cols” from the output of get_coords().\n\nplot_ca <- function(ca_coords, variances) {\n  dim_1 <- sprintf(\"Dimension 1 (%.2f %%)\", variances[[1]])\n  dim_2 <- sprintf(\"Dimension 2 (%.2f %%)\", variances[[2]])\n  \n  ggplot(ca_coords$cols, aes(x = V1, y = V2)) +\n    geom_text(aes(label = word), color = \"gray60\") +\n    geom_point(data = ca_coords$rows, aes(color = Subcorpus)) +\n    scale_color_manual(values = c(\"#0000CD\",\"#DC143C\")) +\n    geom_hline(yintercept = 0, color = \"darkgray\") +\n    geom_vline(xintercept = 0, color = \"darkgray\") +\n    theme_bw(base_size = 12) +\n    labs(x = dim_1, y = dim_2) +\n    coord_fixed()\n}"
  },
  {
    "objectID": "studies/ca-trump-clinton.html#example-first-case-study",
    "href": "studies/ca-trump-clinton.html#example-first-case-study",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n6.4 Example: first case study",
    "text": "6.4 Example: first case study\nWith these functions, the code for the case study in Section 2 (after loading fnames, short_fnames and sub_corp, of course) becomes the one below. After each line, we have a significant unit that we might want to inspect or manipulate.\n\nfeatures <- read_types(here(\"studies\", \"assets\", \"ca-trump-clinton\", \"function-words.txt\"))\nd <- compile_frequencies(features, fnames, short_fnames)\nd_ca <- ca(d)\nca_coords <- get_coords(d_ca, sub_corp)\nplot_ca(ca_coords, summary(d_ca)$scree[,3])\n\n\n\n\nFigure 6: CA plot with function words as features.\n\n\n\n\nLine 1 reads the files with features; after it we can check whether it actually contains the items we wanted.\nLine 2 creates the matrix of documents by features, and we can inspect it afterwards with something like d[1:10,1:10] or dim(d).\nLine 3 runs the correspondence analysis and we can inspect the output with d_ca or summary(d_ca).\nLine 4 prepares the coordinates for plotting. We can then also check the most extreme features with head(arrange(ca_coords$cols, desc(V1))), for example.\nLine 5 plots the dots on the components (Figure 6)."
  },
  {
    "objectID": "studies/ca-trump-clinton.html#full-code",
    "href": "studies/ca-trump-clinton.html#full-code",
    "title": "CA applied to the Trump-Clinton speeches",
    "section": "\n6.5 Full code",
    "text": "6.5 Full code\nThe full code of this document could then be compressed as in the chunk below. Notice that here I have excluded all calls to print(), kbl() and kable_paper(). I have also used different variable names for the different studies instead of overwriting variables, since it can cause problems when you lose track of what is stored as what. Finally, I dedicated a line to define the path to the corpus and one to define the path to the lists of function words and the stoplist.\n\ncorpus_folder <- here(\"studies\", \"_corpora\", \"clinton_trump\")\nassets <- here(\"studies\", \"assets\", \"ca-trump-clinton\")\nfnames <- get_fnames(corpus_folder)\nshort_fnames <- short_names(fnames)\nsub_corp <- fnames %>%\n  re_retrieve_first(\"/clinton_trump/([^/]+)\", requested_group = 1)\n\n# Function words as features ----\nfeatures1 <- read_types(file.path(assets, \"function-words.txt\"))\nd1 <- compile_frequencies(features1, fnames, short_fnames)\n\nd_ca1 <- ca(d1)\nca_coords1 <- get_coords(d_ca1, sub_corp)\n\nplot_ca(ca_coords1, summary(d_ca1)$scree[,3])\n\n## Inspect extremes ----\nca_coords1$cols %>% \n  arrange(desc(V2)) %>% \n  head(20) %>% \n  pull(word)\nca_coords1$cols %>% \n  arrange(V2) %>% \n  head(20) %>% \n  pull(word)\n\n# Content words as features ----\nstop_list <- read_types(file.path(assets, \"stop_list.txt\"))\nfeatures2 <- freqlist(fnames) %>%\n  drop_types(stop_list) %>%\n  keep_bool(ranks(.) <= 150) %>%\n  as_types()\n\nd2 <- compile_frequencies(features2, fnames, short_fnames)\nd_ca2 <- ca(d2)\nca_coords2 <- get_coords(d_ca2, sub_corp)\nplot_ca(ca_coords2, summary(d_ca2)$scree[,3])\n\n## Zoomed-in plot ----\nplot_ca(ca_coords2, summary(d_ca2)$scree[,3]) +\n  coord_fixed(xlim = c(0, 0.5), ylim = c(-0.5, 0))\n\n## Select items ----\nca_coords2$cols %>%\n  filter(V1 >= 0 & V2 <= 0) %>% \n  mutate(dist_to_center = sqrt(V1^2 + V2^2)) %>% \n  arrange(desc(dist_to_center)) %>% \n  pull(word)\n  \n# Bigrams as features ----\nfeatures3 <- fnames %>%\n  freqlist(ngram_size = 2) %>%\n  drop_re(\"(applause|cheering|--)\") %>%\n  keep_bool(ranks(.) <= 150) %>%\n  as_types()\n\nd3 <- compile_frequencies(features3, fnames, short_fnames, ngram_size = 2)\nd_ca3 <- ca(d3)\nca_coords3 <- get_coords(d_ca3, sub_corp)\nplot_ca(ca_coords3, summary(d_ca3)$scree[,3])\n\n## Inspect extremes ----\nca_coords3$cols %>% \n  arrange(V1) %>%\n  head(20) %>% \n  pull(word)\nca_coords3$cols %>% \n  arrange(desc(V1)) %>%\n  head(20) %>% \n  pull(word)\n\n# Trigrams as features ----\nfeatures4 <- fnames %>%\n  freqlist(ngram_size = 3) %>%\n  drop_re(\"(applause|cheering|--)\") %>%\n  keep_bool(ranks(.) <= 150) %>%\n  as_types()\n\nd4 <- compile_frequencies(features4, fnames, short_fnames, ngram_size = 3)\nd_ca4 <- ca(d4)\nca_coords4 <- get_coords(d_ca4, sub_corp)\nplot_ca(ca_coords4, summary(d_ca4)$scree[,])\n\n## Inspect extremes ----\nca_coords4$cols %>% \n  arrange(V1) %>%\n  head(10) %>% \n  pull(word)\nca_coords4$cols %>% \n  arrange(desc(V1)) %>%\n  head(10) %>% \n  pull(word)"
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html",
    "href": "studies/genitive-alternation-analysis.html",
    "title": "Genitive Alternation II: Analysis",
    "section": "",
    "text": "This is the second part of a variation study focusing on the genitive alternation in English, using the Brown corpus. This part illustrates the analysis with conditional inference trees and mixed-effects logistic regression, whereas the first part focused on retrieving observations of the alternation."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#exploring-the-data",
    "href": "studies/genitive-alternation-analysis.html#exploring-the-data",
    "title": "Genitive Alternation II: Analysis",
    "section": "Exploring the data",
    "text": "Exploring the data\nIt is good practice to get familiar with your dataset before delving into the modelling. On the one hand, it may expose annotation errors, such as mistyped values in manually annotated variables. On the other hand, it gives you a foundation for the interpretation of the output and a notion of what you may encounter.\nIn the retrieval section we already saw the xtabs() function, which quickly returns a contingency table between two variables.\n\nxtabs(~ gen_type + possessor_type, data = cd)\n\n        possessor_type\ngen_type common proper\n      of   3151   1006\n      s    1050   2137\n\n\nWith tidyverse, particularly for a more tabular format, you can use count() combined with pivot_wider() to obtain a similar result, as shown in Table 1.\n\ncd %>% \n  count(gen_type, possessor_type) %>% \n  pivot_wider(names_from = possessor_type, values_from = n) %>% \n  kbl() %>%\n  kable_paper()\n\n\n\n\nTable 1:  Contingency table between the outcome gen_type and the predictor possessor_type. \n \n gen_type \n    common \n    proper \n  \n\n\n of \n    3151 \n    1006 \n  \n\n s \n    1050 \n    2137 \n  \n\n\n\n\n\nFor quantitative variables, we can extract the corresponding column from the dataset as a vector, e.g. pull(cd, size_possessor) or cd$size_possessor, and apply summary(). In order to correlate this with a categorical variable, we can first split the dataset based on the values of that variable with split() and then apply summary() on the columns with map().\n\n\n\npurrr functions such as map() apply the same function to each element of a list or vector. Here, we take each element of the split dataframe and apply the same code.\n\ncd %>% \n  split(.$gen_type) %>% \n  map(~ summary(.x$size_possessor))\n## $of\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    0.00    1.61    2.08    2.08    2.48    4.26 \n## \n## $s\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.693   1.609   1.946   1.939   2.197   3.611\n\ncd %>% ggplot(aes(x = gen_type, y = size_possessor)) +\n  geom_boxplot() + theme_minimal(base_size = 30)\n\n\n\n\nWe can already see here that the tiniest and the longest Possessors tend to go with the of-genitive. Remember that size_possessor doesn’t contain the length in characters but its logarithm. Therefore, the minimum size of 0 corresponds to a character length of 1. Table 2 collects these examples for deeper inspection.\n\ncd %>% \n  filter(size_possessor == 0) %>% \n  as_tibble() %>% \n  select(match, possessor) %>% \n  kbl() %>% kable_paper()\n\n\n\n\nTable 2:  Concordances where the Possessor constituent has only one character. \n \n match \n    possessor \n  \n\n\n the primary decomposition of T \n    t \n  \n\n The constancy of P \n    p \n  \n\n The choice of P \n    p \n  \n\n the graph of F \n    f \n  \n\n the graph of F \n    f \n  \n\n the multiplicity of F \n    f \n  \n\n the graph of F \n    f \n  \n\n the values of S \n    s \n  \n\n the values of S \n    s \n  \n\n the image of L \n    l \n  \n\n the plane of L \n    l \n  \n\n the image of L \n    l \n  \n\n\n\n\n\n\nFinally, we could also do some visualization. We could use barplots for the categorical variables, mosaic plots for the contingency table, histograms for the quantitative variables, scatterplots for the relationship between two quantitative variables or boxplots/violin plots to compare quantities and/or relate them to qualitative variables.\n\n\nBars\nMosaic\nHistograms\nPoints\nViolin\n\n\n\nBarplots can be created with the geom_bar() function. The position argument controls whether the bars of different colors are stacked or put next to each other.\n\nggplot(cd, aes(x = gen_type, fill = possessor_type)) +\n  geom_bar(position = position_dodge()) +\n  scale_fill_manual(values = c(\"orange\", \"darkgreen\"))\n\n\n\nFigure 1: Example of a barplot visualizing the number of observations with each outcome and Possessor type. If you already have the counts for the y-axis, use geom_col() instead.\n\n\n\n\n\n\nWhile the ggmosaic package offers a way of creating mosaic plots with ggplot2, graphics::mosaic() has the nice feature of shading the sections based on the residuals (with shade = TRUE). Red squares indicate that the frequency of a combination is significantly lower than expected (i.e. if the distribution was proportional), and blue squares, that the frequency is significantly higher than expected. In other words, you can already see if a pair of values of two categorical variables are “attracted” to each other. Here it is very clear that the Possessor types are not balanced across genitive variants: the common noun Possessor is much more attracted to the of-genitive, and the proper noun to the s-genitive.\n\nmosaicplot(gen_type ~ possessor_type, data = cd, shade = TRUE)\n\n\n\nFigure 2: Example of a mosaic plot visualizing the contingency table between the outcome gen_type and the predictor possessor_type.\n\n\n\n\n\n\nHistograms can be generated with geom_histogram(); they show the frequencies of different values along a continuous scale.\n\nggplot(cd, aes(x = size_possessor)) +\n  geom_histogram()\n\nggplot(cd, aes(x = size_possessed)) +\n  geom_histogram()\n\n\n\n\n\n(a) Size of the possessor slot\n\n\n\n\n\n\n(b) Size of the possessed slot\n\n\n\n\nFigure 3: Examples of histograms showing the frequencies of different Possessor and Possessed sizes (log of N of characters).\n\n\n\n\n\nA scatterplot of points can contrast two quantitative variables; in this case the log-transformed slot sizes (Figure 4 (a)) and the original sizes in number of characters, retrieved with exp() (Figure 4 (b)).\n\nggplot(cd, aes(x = size_possessor, y = size_possessed)) +\n  geom_point(size = 3, alpha = 0.5)\n\nggplot(cd, aes(x = exp(size_possessor), y = exp(size_possessed))) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = \"possessor\", y = \"possessed\")\n\n\n\n\n\n(a) Log-transformed character length\n\n\n\n\n\n\n(b) Original character length\n\n\n\n\nFigure 4: Example of a scatterplot contrasting the size of a Possessor and a Possessed slot in each observation.\n\n\n\n\n\nIn order to compare the ranges of size_possessor and size_possessed in a violin plot, we need to turn the dataset into a long-form version, so that the value of possessor/possessed is another variable. For that we use pivot_longer(), the complement of pivot_wider(), with the names of the columns that we want to change and a few useful arguments. First, for the selection of columns, we can also use dplyr selectors such as starts_with(), which selects all columns that start with a given string. Then, names_to and values_to indicate the new names for the columns to which the original names of the columns (here size_possessor and size_possessed) and their values will go. In this case, with names_to = \"slot\", values_to = \"size\", each row of the original dataset will become two rows, and the columns size_possessor and size_possessed will have disappeared. Instead, there will be a column slot with values size_possessor and size_possessed, and a column size with the original values of that row for size_possessor and size_possessed respectively. This allows us to use slot as a variable for the ggplot aesthetics. Finally, the names_transform argument allows us to transform the values that will go into the name-column, i.e. slot. In particular, this stringr::str_remove() call will remove the “size_” prefix from size_possessed and size_possessor.\nThe function to draw a violin plot is geom_violin().\n\n\n\n\n\n\nViolins and boxes\n\n\n\nIf you’re wondering why I use violin plots instead of boxplots, I invite you to read the article Same Stats, Different Graphs, by Justin Matejka and George Fitzmaurice.\nAnd if you are interested in more advanced graphs, combining half-vioilin plots and half-boxplots or scatterplots, check out the {gghalves} package.\n\n\n\ncd %>% as_tibble() %>% \n  select(starts_with(\"size_p\"), gen_type) %>% \n  pivot_longer(starts_with(\"size\"),\n               names_to = \"slot\", values_to = \"size\",\n               names_transform = ~ str_remove(.x, \"size_\")) %>% \n  ggplot(aes(x = slot, y = size, fill = gen_type)) +\n  geom_violin() +\n  scale_fill_manual(values = c(\"orange\", \"darkgreen\"))\n\n\n\nFigure 5: Example of a violin plot."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#conditional-trees-in-relation-to-other-techniques",
    "href": "studies/genitive-alternation-analysis.html#conditional-trees-in-relation-to-other-techniques",
    "title": "Genitive Alternation II: Analysis",
    "section": "Conditional trees in relation to other techniques",
    "text": "Conditional trees in relation to other techniques\nUnlike logistic regression analysis, conditional inference trees have the ability to deal with collinearity. This is the case where two variables basically do the same thing, or one variable has the same effect as a combination of two other variables. Therefore, we can use a conditional tree to see which predictors to use in the regression model. For example, in our dataset size_difference is redundant with size_possessor and size_possessed because it depends on them, and Figure 6 tells us that we can safely discard size_difference because it doesn’t have a significant effect.\nOn the other hand, conditional trees look at one variable at a time, and once a split is performed, the effect of the following variables is examined in relation to the split dataset. That is how the effect of size_possessor is different in the subset where possessor_type is common compared to the subset where possessor_type is proper, and how size_possessed is only a relevant predictor in certain splits. In contrast, regression models look at the effect of each variable on the whole dataset, while accounting for the effects of the rest of the variables.\nFinally, the conditional tree chooses the most relevant variable but does not tell us how close other variables were — it seems that size_possessor is not as effective as possessor_type when splitting the full dataset, but how less effective is it? In order to obtain a ranking of importance of the variables, we can run random forests. This technique generates a large number of trees with different subsets of the dataset (different subsets of rows and of columns) and checks which variable takes the first place in each of the trees. This is a handy technique when we have many different variables, but it is not very time-efficient and therefore it’s overkill for this dataset."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#maximal-model",
    "href": "studies/genitive-alternation-analysis.html#maximal-model",
    "title": "Genitive Alternation II: Analysis",
    "section": "Maximal model",
    "text": "Maximal model\nThe first model we fit is a maximal model, using all the predictors suggested by the conditional tree. Because complex mixed-effect logistic regression models often have a hard time trying to converge, we add a lme4::glmerControl() call to help it. Here we set the optimizer to “bobyqa”, but we could also increase the number of iteration with the argument optCtr = list(maxfun = 100000).\n\ncd_glmer <- glmer(\n    gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +\n               (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd,\n    control = glmerControl(optimizer = \"bobyqa\")\n  )\nsummary(cd_glmer) %>%\n  print(correlation = FALSE)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +  \n    (1 | comp) + (1 | source)\n   Data: cd\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n    7879     7941    -3931     7861     7335 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.630 -0.597 -0.266  0.622  5.265 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n source (Intercept) 0.793    0.891   \n comp   (Intercept) 0.304    0.552   \nNumber of obs: 7344, groups:  source, 499; comp, 15\n\nFixed effects:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                          -0.6953     0.5311   -1.31    0.190    \npossessor_typeproper                  2.5731     0.3299    7.80  6.2e-15 ***\nsize_possessor                       -0.4486     0.2510   -1.79    0.074 .  \nsize_possessed                        0.4472     0.2329    1.92    0.055 .  \npossessor_typeproper:size_possessor   0.0369     0.1240    0.30    0.766    \npossessor_typeproper:size_possessed  -0.2811     0.1130   -2.49    0.013 *  \nsize_possessor:size_possessed        -0.1228     0.1127   -1.09    0.276    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor the interpretation of the model, we want to look at:\n\nThe AIC value: the lower, the better. We can use it to compare models to each other.\nThe variance of the random effects: if it is not 0, it means we should keep them.\nThe p-values and estimates of the intercept and the effects. We do not care about non-significant effects, so we might want to try a new model without those predictors.\n\nAn useful way to check the significance of a predictor on the model is to run different models, each of which has removed one predictor, and compare them to the model with all the predictors. With the likelihood ratio test (LRT) we can see if removing anything from the model makes it significantly worse. We will do this with one handy function in the next section.\n\n\n\n\n\n\nSelection of predictors\n\n\n\nWhile p-values and LRT tests are useful statistical guides when selecting or discarding predictors, your selection may also be theoretically motivated. Regardless of these values, if a certain predictor was significant in previous research, you can decide to keep it in your models."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#removing-one-predictor",
    "href": "studies/genitive-alternation-analysis.html#removing-one-predictor",
    "title": "Genitive Alternation II: Analysis",
    "section": "Removing one predictor",
    "text": "Removing one predictor\nThe function afex::mixed() can take a model, run different iterations in which one predictor was removed, and compare if the model without a predictor is significantly worse than the model with all predictors. For that comparison we’ll use likelihood ratio (LRT).In linear models we would use an F-test instead.\n\ncd_glmer_lrt <- mixed(\n    gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +\n               (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd, \n    method = \"LRT\",\n    control = glmerControl(optimizer = \"bobyqa\")\n  )\n\nContrasts set to contr.sum for the following variables: gen_type, possessor_type, comp, source\n\n\nNumerical variables NOT centered on 0: size_possessor, size_possessed\nIf in interactions, interpretation of lower order (e.g., main) effects difficult.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe call to mixed() sends us a warning about the distribution of our numerical variables. This is useful but we don’t need to heed it as long as we don’t try to interpret the main effects of variables in interactions.\n\n\n\nprint(cd_glmer_lrt)\n\nMixed Model Anova Table (Type 3 tests, LRT-method)\n\nModel: gen_type ~ (possessor_type + size_possessor + size_possessed)^2 + \nModel:     (1 | comp) + (1 | source)\nData: cd\nDf full model: 9\n                         Effect df     Chisq p.value\n1                possessor_type  1 61.26 ***   <.001\n2                size_possessor  1    3.14 +    .076\n3                size_possessed  1      1.72    .190\n4 possessor_type:size_possessor  1      0.09    .765\n5 possessor_type:size_possessed  1    6.13 *    .013\n6 size_possessor:size_possessed  1      1.18    .278\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nThe output confirms that possessor_type and its interaction with size_possessed are significant. We can interpret as well that the other two interactions are not significant and must be dropped. However, we don’t know if the main effect of size_possessor is significant, because it’s being distorted by the interactions in the model. Therefore, we will try a new model with possessor_type, size_possessor, size_possessed and the interaction possessor_type:size_possessed.\n\n\n\n\n\n\nRandom effects and variable selection\n\n\n\nIf you model an interaction, you also have to keep the main effects of the predictors that participate in the interaction."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#second-model",
    "href": "studies/genitive-alternation-analysis.html#second-model",
    "title": "Genitive Alternation II: Analysis",
    "section": "Second model",
    "text": "Second model\nSince the new model is not as complex, we don’t need glmerControl() to help it converge.\n\ncd_glmer2 <- glmer(\n    gen_type ~ possessor_type + size_possessor + size_possessed +\n      possessor_type:size_possessed +\n      (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd\n  )\n\nsummary(cd_glmer2) %>%\n  print(correlation = FALSE)\n\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gen_type ~ possessor_type + size_possessor + size_possessed +  \n    possessor_type:size_possessed + (1 | comp) + (1 | source)\n   Data: cd\n\n     AIC      BIC   logLik deviance df.resid \n    7877     7925    -3931     7863     7337 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.632 -0.597 -0.267  0.621  5.332 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n source (Intercept) 0.794    0.891   \n comp   (Intercept) 0.306    0.553   \nNumber of obs: 7344, groups:  source, 499; comp, 15\n\nFixed effects:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                          -0.2284     0.2478   -0.92   0.3567    \npossessor_typeproper                  2.6650     0.2392   11.14   <2e-16 ***\nsize_possessor                       -0.6880     0.0625  -11.01   <2e-16 ***\nsize_possessed                        0.2055     0.0759    2.71   0.0068 ** \npossessor_typeproper:size_possessed  -0.2893     0.1120   -2.58   0.0098 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn this model, the AIC value is slightly lower (so, the model is better) and the variance of the random effects is larger than zero. In addition, all the current predictors and the interaction are significant.\nWe could still run afex::mixed() to check whether removing any predictor would make the model worse.\n\ncd_glmer2_lrt <- mixed(\n    gen_type ~ possessor_type + size_possessor + size_possessed +\n      possessor_type:size_possessed +\n      (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd,\n    method = \"LRT\")\nprint(cd_glmer2_lrt)\n\nMixed Model Anova Table (Type 3 tests, LRT-method)\n\nModel: gen_type ~ possessor_type + size_possessor + size_possessed + \nModel:     possessor_type:size_possessed + (1 | comp) + (1 | source)\nData: cd\nDf full model: 7\n                         Effect df      Chisq p.value\n1                possessor_type  1 126.10 ***   <.001\n2                size_possessor  1 125.08 ***   <.001\n3                size_possessed  1       1.11    .292\n4 possessor_type:size_possessed  1     6.62 *    .010\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n\n\nIndeed, the only predictor that we could “safely” remove is size_possessed, but we need it in order to keep the significant interaction in which it participates."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#concordance-index-c",
    "href": "studies/genitive-alternation-analysis.html#concordance-index-c",
    "title": "Genitive Alternation II: Analysis",
    "section": "Concordance index C",
    "text": "Concordance index C\nIn order to assess the strength of the predictions of the models, we can use the concordance index C, computed with Hmisc::somers2(). The first argument is the fitted values of the models (obtained with predict()), whereas the second argument is a numerical form of the observed outcomes.\n\n# maximal model\nHmisc::somers2(\n  predict(cd_glmer), as.numeric(cd$gen_type)-1\n)[\"C\"]\n\n     C \n0.8494 \n\n# minimal model\nHmisc::somers2(\n  predict(cd_glmer2), as.numeric(cd$gen_type)-1\n)[\"C\"]\n\n     C \n0.8494 \n\n# minimal model without random effects in the prediction\nHmisc::somers2(\n  predict(cd_glmer2, re.form = NA), as.numeric(cd$gen_type)-1\n)[\"C\"]\n\n     C \n0.7511 \n\n\n\n\n\n\n\n\nRandom effects and tidymodels\n\n\n\nWith basic predict() it’s possible to obtain the predictions both taking the random effects into account and excluding them. By default, they are taken into account.\nIf you use tidymodels for modelling, this will not be possible and you will have to take some differences into account:\n\nThe predicted value is the reference value.\nThere is no “concordance index C” measure in yardstick, the tidymodels package for evaluation; instead, there is a metric computing “the area under the ROC curve” (yardstick::roc_auc()), which is the same thing!\nPredictions are always made based on fixed effects only (since the values of your random effects are not supposed to be present in the data you’re predicting)."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#interpretation",
    "href": "studies/genitive-alternation-analysis.html#interpretation",
    "title": "Genitive Alternation II: Analysis",
    "section": "Interpretation",
    "text": "Interpretation\nOne of the first things we see in the output is that, when the Possessor is a proper noun and the Possesses slot has one character, the chances of the s-variant are significantly higher — about 14.3677 times more likely than the alternative, all other things being equal.\nMoreover the larger the Possessed constituent, the higher the chances of the s-variant. In contrast, all other things being equal, the larger the Possessor constituent, the lower the chances of the s-variant. This is consistent with the theoretical assumption that the longer constituent is placed at the end: the longer a constituent, the higher the chances of a construction that places it at the end.\nHowever, that effect of the size of the Possessed constituent is valid when the Possessor slot is a common noun, which in general favors the of construction (as indicated by the Intercept, although it is not significant). In contrast, when the Possessor is a proper noun (which favors the s-construction, all other things being equal), a larger Possessed constituent actually doesn’t favor the s construction as much.\n\n\n\n\n\n\nTip\n\n\n\nYou can always retrieve the coefficients with coefficients(summary(model)), which is a dataframe. For example coefficients(summary(cd_glmer2))[\"possessor_typeproper\",\"Estimate\"] returns the estimate coefficient of possessor_type. Adding exp(), you can retrieve the odds of the s-variant over the of-variant when the Possessor is a proper noun, all other things being equal."
  },
  {
    "objectID": "studies/genitive-alternation-analysis.html#plotting-effects",
    "href": "studies/genitive-alternation-analysis.html#plotting-effects",
    "title": "Genitive Alternation II: Analysis",
    "section": "Plotting effects",
    "text": "Plotting effects\nFigure 8 plots the effects in the models. Figure 8 (a) shows that the probabilities of the s-genitive decrease as the size of the Possessor constituent increases. These probabilities take the other predictors into account by assuming their typical cases.\nFigure 8 (b) illustrates the interaction between the type of the Possessor and the size of the Possessed constituent. As we saw in the printed output of the model, when the Possessor is a common noun, a larger size of the Possessed constituent increases the probabilities of the s-genitive, whereas the effect inverts when the Possessor is a proper noun. In addition, we see that the probability of the s-genitive is always higher for the proper noun, regardless of the size of the Possessed constituent.\n\nggpredict(cd_glmer2, \"size_possessor\") %>%\n  plot()\n\nggpredict(cd_glmer2, c(\"size_possessed\", \"possessor_type\")) %>%\n  plot()\n\n\n\n\n\n(a) Effect of the size of the possessor slot.\n\n\n\n\n\n\n(b) Interaction between size of the possessed slot and type of possessor.\n\n\n\n\nFigure 8: Effect plots predicting the probability of the s-genitive."
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html",
    "href": "studies/genitive-alternation-retrieval.html",
    "title": "Genitive Alternation I: Retrieval",
    "section": "",
    "text": "This is the first part of a variation study focusing on the genitive alternation in English, using the Brown corpus. This part focuses on retrieving observations of the alternation, whereas the second part will illustrate the analysis with conditional inference trees and mixed-effects logistic regression. One of the reasons to split the analysis in two is that, more often than not, between retrieval and analysis there is an additional step of manual cleaning or annotation of the dataset."
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html#pos-tags",
    "href": "studies/genitive-alternation-retrieval.html#pos-tags",
    "title": "Genitive Alternation I: Retrieval",
    "section": "POS tags",
    "text": "POS tags\nThe text in this corpus looks as follows, with word forms followed by a part-of-speech tag, with a slash in between:\n\n\nThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at\ninvestigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/\nvbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd\nplace/nn ./.\n\n\nSome of the POS tags of Brown are of particular importance for this case study.\n\nUseful POS-tags of the Brown corpus for the genitive alternation study.\n\n\n\n\n\n\nPOS tag\nDescription\nExample\n\n\n\nat\narticle\nthe/at\n\n\n\nin\npreposition\nof/in\n\n\n\njj\nadjective\nrecent/jj\n\n\n\nnn\ncommon noun\nplace/nn\n\n\n\nnp\nproper noun\nAtlanta/np\n\n\n\n-tl (suffix to a POS tag)\ntitle or part of title\nFulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl\n\n\n\n$ (suffix to a POS tag)\ngenitive marker1\n\nAtlanta’s/np$ recent/jj primary/nn"
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html#sec-query1",
    "href": "studies/genitive-alternation-retrieval.html#sec-query1",
    "title": "Genitive Alternation I: Retrieval",
    "section": "Of-genitive with common nouns",
    "text": "Of-genitive with common nouns\nIn the first query we will retrieve the attestations of the of-genitive with common nouns, e.g. “the idea of the student”.\nRegular expression\n\n\nIf you would like to practice regular expressions, the {mclmtutorials} package includes a tutorial on how to use them in {mclm}. Just install the package along with {learnr} and {gradethis} and run the tutorial:\n\n# install.packages(\"learnr\")\n# remotes::install_github(\"masterclm/mclmtutorials\")\n# remotes::install_github(\"rstudio/gradethis\")\nlearnr::run_tutorial(\"regex\", \"mclmtutorials\")\n\nAs a first step, we construct a regular expression that will match, in the non-tokenized corpus, the sequence we are interested in, and we’ll store it in a variable pattern. Notice that we use a raw string (preceded with r and, within the quotation marks, surrounded by two hyphens and a square bracket) and that we flag the regular expression with an x and an i. The x allows us to insert spaces and line breaks for more readability (free-spacing mode), and the i makes the search case insensitive.\n\npattern <- r\"--[(?xi)\n          \\b         the / at          \\s+\n          ( (?:  [^\\s/]+ / jj  [^\\s]*  \\s+ )*\n            (?:  [^\\s/]+ / nn  [^\\s]*  \\s+ )*\n                 [^\\s/]+ / nn  [^\\s]*\n          )                            \\s+\n                      of / in          \\s+\n                     the / at          \\s+\n          ( (?:  [^\\s/]+ / jj  [^\\s]*  \\s+ )*\n            (?:  [^\\s/]+ / nn  [^\\s]*  \\s+ )*\n                 [^\\s/]+ / nn  [^\\s$]* \n          )                            \\s    \n]--\"\n\nWith this pattern, we are asking for:\n\na word boundary,\nfollowed by the/at and whitespace,\noptionally followed by adjectives, each followed by whitespace, and then at least one common noun — this is the first capturing group, the Possessed,\nthen whitespace, followed by of/in, whitespace, the/at and whitespace,\noptionally followed by adjectives, each followed by whitespace, and then at least one common noun without the genitive tag — this is the second capturing group, the Possessor.\n\n\n\n\n\n\n\nExplanation of the regular expression\n\n\n\n\n\nIn line 2 we have a \\b, followed by the string the/at, followed by \\s+. This means that we want the literal string the/at preceded by a word boundary and followed by one or more whitespace characters. A word boundary at the beginning covers whitespace characters, punctuation marks and even the beginning of the text.\nLines 3-5 and 9-11 are surrounded by parentheses, turning them into capturing groups. Later, when we want to extract the Possessed and Possessor from the match, we can ask for the text matching each of the capturing groups.\nWithin the capturing groups we have non-capturing groups, marked by parentheses and ?: at the beginning. If we didn’t add ?: at the beginning, they would be included in the numbering of the capturing groups and we wouldn’t know for sure which number belongs to the full Possessed and Possessor groups. But we need the parentheses in order to apply the final asterisks (in lines 3, 4, 9 and 10) to the full match that they surround. But what are they matching, exactly?\nThe main capturing groups for Possessed and Possessor want to match a common noun optionally preceded by one or more adjectives and one or more nouns. The match for a common noun is [^\\s/]+ / nn [^\\s]*. [\\s/] matches either a whitespace character or a slash; adding the ^ inverts the match to anything but a whitespace character or a slash; + requires one or more of them and *, zero or more. Therefore, this regex asks for a sequence of characters that are neither a whitespace character or a slash, (e.g. idea or student), followed by a slash and then nn, followed by an optional sequence of characters that are not whitespace characters, in case the part-of-speech tag ends with an s (for plurar) or -tl. We use jj instead of nn when we ask for adjectives and we also reject $ at the end of the Possessor slot, because we don’t want an s-genitive to match.\nWithin each non-capturing group, therefore, we want to match either an adjective (lines 3 and 9) or a noun (lines 4 and 10), but we still get a match if neither of them occurs. They are also followed by \\s+ to capture the whitespace characters between the words.\nFinally, lines 7 and 8 match the core of the of-genitive construction, i.e. of/in and the/at, surrounded and separated by whitespace characters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nWe build the concordance with conc(), which takes a corpus or filenames, e.g. brown_fnames, and a regular expression to match, e.g. pattern. We can then explore the concordance with print_kwic(), explore() or View(). This really is advisable, in order to inspect how good the regex pattern is and see if you might want to refine it to capture patterns that were missed, or to exclude patterns that should be discarded. Table 1 prints the first 6 items in the concordance with kableExtra.\n\n\nWith practice, you’ll find the balance between refining the automatic process and performing manual cleaning. In some cases, it’s quicker to fix the automatic procedure; in other cases, manual adjustments are easier and more reliable.\n\ncd <- brown_fnames %>%\n  conc(pattern)\nprint_kwic(cd, n = 10)\n\nidx                      left|        match        |right                   \n  1 ... suit/nn to/to test/vb|the/at va...at act/nn|,/, and/cc then/rb th...\n  2 ...n/cd dollars/nns at/in|the/at en...j year/nn|next/ap Aug./np 31/cd...\n  3 ...e/ber teaching/vbg ./.|The/at re...rement/nn|would/md be/be in/in ...\n  4 ...ill/md retire/vb at/in|the/at cl...n term/nn|./. Dr./nn-tl Clark/n...\n  5 ... ``/`` Actually/rb ,/,|the/at ab...rocess/nn|may/md have/hv consti...\n  6 .../at 23d/od ward/nn ./.|The/at ca...udges/nns|in/in the/at 58th/od ...\n  7 ...O/nn line/nn ./. On/in|the/at ne... sheet/nn|must/md be/be set/vbn...\n  8 ...the/at alliance/nn ,/,|the/at us...ion/nn-tl|for/in-tl Economic/jj...\n  9 ...ns as/ql well/rb as/cs|the/at ma...errent/nn|./. This/dt increase/...\n 10 ...ey/ppss count/vb on/in|the/at ai...tries/nns|attending/vbg the/at ...\n...\n\n\n\nCodecd %>% \n  select(left, match, right) %>% \n  head(6) %>% \n  kbl(align = c(\"r\", \"c\", \"l\")) %>%\n  kable_paper(font_size = 15)\n\n\n\n\nTable 1:  Concordance with first results of first query. \n \n left \n    match \n    right \n  \n\n\n ns million/cd worth/nn of/in highway/nn reconstruction/nn bonds/nns ./. The/at bond/nn issue/nn will/md go/vb to/in the/at state/nn courts/nns for/in a/at friendly/jj test/nn suit/nn to/to test/vb \n    the/at validity/nn of/in the/at act/nn \n    ,/, and/cc then/rb the/at sales/nns will/md begin/vb and/cc contracts/nns let/vbn for/in repair/nn work/nn on/in some/dti of/in Georgia's/np$ most/ql heavily/rb traveled/vbn highways/nns ./. A/at H \n  \n\n servative/jj ''/'' his/pp$ estimate/nn that/cs it/pps would/md produce/vb 17/cd million/cd dollars/nns to/to help/vb erase/vb an/at anticipated/vbn deficit/nn of/in 63/cd million/cd dollars/nns at/in \n    the/at end/nn of/in the/at current/jj fiscal/jj year/nn \n    next/ap Aug./np 31/cd ./. He/pps told/vbd the/at committee/nn the/at measure/nn would/md merely/rb provide/vb means/nns of/in enforcing/vbg the/at escheat/nn law/nn which/wdt has/hvz been/ben on/in \n  \n\n over/np also/rb would/md require/vb junior-senior/jj high/nn teachers/nns to/to have/hv at/in least/ap 24/cd semester/nn hours/nns credit/vb in/in the/at subject/nn they/ppss are/ber teaching/vbg ./. \n    The/at remainder/nn of/in the/at 4-year/jj college/nn requirement/nn \n    would/md be/be in/in general/jj subjects/nns ./. ``/`` A/at person/nn with/in a/at master's/nn$ degree/nn in/in physics/nn ,/, chemistry/nn ,/, math/nn or/cc English/np ,/, yet/rb who/wps has/hvz n \n  \n\n /np Clark/np of/in Hays/np ,/, Kan./np as/cs the/at school's/nn$ new/jj president/nn ./. Dr./nn-tl Clark/np will/md succeed/vb Dr./nn-tl J./np R./np McLemore/np ,/, who/wps will/md retire/vb at/in \n    the/at close/nn of/in the/at present/jj school/nn term/nn \n    ./. Dr./nn-tl Clark/np holds/vbz an/at earned/vbn Doctor/nn-tl of/in-tl Education/nn-tl degree/nn from/in the/at University/nn-tl of/in-tl Oklahoma/np-tl ./. He/pps also/rb received/vbd a/at Master \n  \n\n d/jj jury/nn room/nn ''/'' ./. He/pps said/vbd this/dt constituted/vbd a/at ``/`` very/ql serious/jj misuse/nn ''/'' of/in the/at Criminal/jj-tl court/nn-tl processes/nns ./. ``/`` Actually/rb ,/, \n    the/at abuse/nn of/in the/at process/nn \n    may/md have/hv constituted/vbn a/at contempt/nn of/in the/at Criminal/jj-tl court/nn-tl of/in-tl Cook/np county/nn ,/, altho/cs vindication/nn of/in the/at authority/nn of/in that/dt court/nn is/bez n \n  \n\n at 21st/od and/cc 28th/od precincts/nns of/in the/at 29th/od ward/nn ,/, the/at 18th/od precinct/nn of/in the/at 4th/od ward/nn ,/, and/cc the/at 9th/od precinct/nn of/in the/at 23d/od ward/nn ./. \n    The/at case/nn of/in the/at judges/nns \n    in/in the/at 58th/od precinct/nn of/in the/at 23d/od ward/nn had/hvd been/ben heard/vbn previously/rb and/cc taken/vbn under/in advisement/nn by/in Karns/np ./. Two/cd other/ap cases/nns also/rb were/ \n  \n\n\n\n\n\nBecause cd is also a dataframe, of which match is a column, we can also inspect the elements in the match by extracting it.\n\nhead(cd$match)\n\n[1] \"the/at validity/nn of/in the/at act/nn \"                              \n[2] \"the/at end/nn of/in the/at current/jj fiscal/jj year/nn \"             \n[3] \"The/at remainder/nn of/in the/at 4-year/jj college/nn requirement/nn \"\n[4] \"the/at close/nn of/in the/at present/jj school/nn term/nn \"           \n[5] \"the/at abuse/nn of/in the/at process/nn \"                             \n[6] \"The/at case/nn of/in the/at judges/nns \"                              \n\n\nOnce we have a decent concordance, we can add variables that are characteristic of it. All of these observations will have the value of in the gen_type variable and the value common in the possessor_type variable. In addition, we can extract the constituents of the Possessed and Possessor slots with mclm::re_replace_first(). The first argument is a text to match (the elements in the match column); the second is a regular expression to match in the text (the pattern we use to retrieve the text), and the third is a replacement string. \"\\\\1\" and \"\\\\2\" correspond to the contents of the first and second capturing groups in pattern, respectively. In other words, we find the portion of text in each element of match that matches the pattern (which is all of it, since that was how it was constructed) and extract either the first capturing group, to fill the possessed column, or the second capturing group, to fill the possessor column.\n\ncd <- cd %>%\n  mutate(\n    gen_type = \"of\",\n    possessor_type = \"common\",\n    possessed = re_replace_first(match, pattern, \"\\\\1\"), \n    possessor = re_replace_first(match, pattern, \"\\\\2\") \n)\ncd %>% \n  as_tibble() %>% \n  select(match, possessed, possessor) %>% \n  head() %>% \n  kbl() %>% \n  kable_paper()\n\n\n\n\nTable 2:  Subset of annotated observations of the first dataset. \n \n match \n    possessed \n    possessor \n  \n\n\n the/at validity/nn of/in the/at act/nn \n    validity/nn \n    act/nn \n  \n\n the/at end/nn of/in the/at current/jj fiscal/jj year/nn \n    end/nn \n    current/jj fiscal/jj year/nn \n  \n\n The/at remainder/nn of/in the/at 4-year/jj college/nn requirement/nn \n    remainder/nn \n    4-year/jj college/nn requirement/nn \n  \n\n the/at close/nn of/in the/at present/jj school/nn term/nn \n    close/nn \n    present/jj school/nn term/nn \n  \n\n the/at abuse/nn of/in the/at process/nn \n    abuse/nn \n    process/nn \n  \n\n The/at case/nn of/in the/at judges/nns \n    case/nn \n    judges/nns"
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html#sec-query2",
    "href": "studies/genitive-alternation-retrieval.html#sec-query2",
    "title": "Genitive Alternation I: Retrieval",
    "section": "Of-genitive with proper nouns",
    "text": "Of-genitive with proper nouns\nThe second query also captures the of-genitive variant, but with proper nouns in the Possessor slot instead, e.g. “the rivers of Belgium”.\nRegular expression\nThe regular expression is very similar to the one for the first query, with a few differences:\n\nIt does not ask or even accept an article after of/in.\nThe noun(s) requested in lines 10 and 11 are proper nouns instead of common nouns.\nBetween the optional adjective(s) and the noun(s) of the second capturing group, i.e. the Possessor slot, we also accept optional items with any part-of-speech as long as they also end in -tl (line 9).\n\n\n\n\n\n\n\nRewriting variables\n\n\n\nNotice that we are rewriting the variable pattern with the regex for the second query, so, if you suddenly wanted to rerun the first query, you would need to change pattern again.\n\n\n\npattern <- r\"--[(?xi)\n       \\b         the / at                 \\s+\n       ( (?:  [^\\s/]+ / jj         [^\\s]*  \\s+ )*\n         (?:  [^\\s/]+ / nn         [^\\s]*  \\s+ )*\n              [^\\s/]+ / nn         [^\\s]*\n         )                                 \\s+\n                   of / in                 \\s+\n       ( (?:  [^\\s/]+ / jj         [^\\s]*  \\s+ )*\n         (?:  [^\\s/]+ / [^\\s]+ -tl [^\\s$]* \\s+ )*\n         (?:  [^\\s/]+ / np         [^\\s$]* \\s+ )*\n              [^\\s/]+ / np         [^\\s$]* \n       )                                   \\s\n]--\"\n\nCode\nThe steps are the same as for the first query. We store the object in a different variable, cd_new, and we assign the value proper to possessor_type instead of common. Afterwards, we rewrite cd to be the combination of cd (the first query) and cd_new (the second query) using merge_conc(), an mclm wrapper for bind_rows().\n\ncd_new <- brown_fnames %>%\n  conc(pattern) %>%\n  mutate(\n    gen_type = \"of\",\n    possessor_type = \"proper\",\n    possessed = re_replace_first(match, pattern, \"\\\\1\"), \n    possessor = re_replace_first(match, pattern, \"\\\\2\")  \n  )\n# print_kwic(cd_new, n = 10)\n\n\nCodecd_new %>% \n  select(left, match, right, possessed, possessor) %>% \n  head(6) %>% \n  kbl(align = c(\"r\", \"c\", \"l\", \"l\", \"l\")) %>%\n  kable_paper(font_size = 15)\n\n\n\n\nTable 3:  Concordance with first results of second query and first annotation. \n \n left \n    match \n    right \n    possessed \n    possessor \n  \n\n\n -tl Department/nn-tl ``/`` has/hvz seen/vbn fit/jj to/to distribute/vb these/dts funds/nns through/in the/at welfare/nn departments/nns of/in all/abn the/at counties/nns in/in the/at state/nn with/in \n    the/at exception/nn of/in Fulton/np-tl \n    County/nn-tl ,/, which/wdt receives/vbz none/pn of/in this/dt money/nn ./. The/at jurors/nns said/vbd they/ppss realize/vb ``/`` a/at proportionate/jj distribution/nn of/in these/dts funds/nns migh \n    exception/nn \n    Fulton/np-tl \n  \n\n Austin/np-hl ,/,-hl Texas/np-hl --/-- Committee/nn approval/nn of/in Gov./nn-tl Price/np Daniel's/np$ ``/`` abandoned/vbn property/nn ''/'' act/nn seemed/vbd certain/jj Thursday/nr despite/in \n    the/at adamant/jj protests/nns of/in Texas/np \n    bankers/nns ./. Daniel/np personally/rb led/vbd the/at fight/nn for/in the/at measure/nn ,/, which/wdt he/pps had/hvd watered/vbn down/rp considerably/rb since/in its/pp$ rejection/nn by/in two/cd \n    adamant/jj protests/nns \n    Texas/np \n  \n\n n on/in the/at hearing/nn ,/, since/cs the/at bill/nn was/bedz introduced/vbn only/rb last/ap Monday/nr ./. Austin/np-hl ,/,-hl Texas/np-hl --/-- Senators/nns unanimously/rb approved/vbd Thursday/nr \n    the/at bill/nn of/in Sen./nn-tl George/np Parkhouse/np \n    of/in Dallas/np authorizing/vbg establishment/nn of/in day/nn schools/nns for/in the/at deaf/jj in/in Dallas/np and/cc the/at four/cd other/ap largest/jjt counties/nns ./. The/at bill/nn is/bez des \n    bill/nn \n    Sen./nn-tl George/np Parkhouse/np \n  \n\n --/-- Principals/nns of/in the/at 13/cd schools/nns in/in the/at Denton/np-tl Independent/jj-tl School/nn-tl District/nn-tl have/hv been/ben re-elected/vbn for/in the/at 1961-62/cd session/nn upon/in \n    the/at recommendation/nn of/in Supt./nn-tl Chester/np O./np Strickland/np \n    ./. State/nn and/cc federal/jj legislation/nn against/in racial/jj discrimination/nn in/in employment/nn was/bedz called/vbn for/in yesterday/nr in/in a/at report/nn of/in a/at ``/`` blue/jj ribbon \n    recommendation/nn \n    Supt./nn-tl Chester/np O./np Strickland/np \n  \n\n b of/in the/at circumstances/nns that/wps have/hv brought/vbn these/dts troubles/nns about/rp ,/, has/hvz been/ben conspicuous/jj by/in its/pp$ absence/nn ./. Explosion/nn-hl avoided/vbn-hl In/in \n    the/at case/nn of/in Portugal/np \n    ,/, which/wdt a/at few/ap weeks/nns ago/rb was/bedz rumored/vbn ready/jj to/to walk/vb out/rp of/in the/at NATO/nn Council/nn-tl should/md critics/nns of/in its/pp$ Angola/np policy/nn prove/vb harsh/ \n    case/nn \n    Portugal/np \n  \n\n some/dti disappointment/nn that/cs the/at United/vbn-tl States/nns-tl leadership/nn has/hvz not/* been/ben as/ql much/rb in/in evidence/nn as/cs hoped/vbn for/in ./. One/cd diplomat/nn described/vbd \n    the/at tenor/nn of/in Secretary/nn-tl of/in-tl State/nn-tl Dean/np \n    Rusk's/np$ speeches/nns as/cs ``/`` inconclusive/jj ''/'' ./. But/cc he/pps hastened/vbd to/to add/vb that/cs ,/, if/cs United/vbn-tl States/nns-tl policies/nns were/bed not/* always/rb clear/jj ,/, d \n    tenor/nn \n    Secretary/nn-tl of/in-tl State/nn-tl Dean/np \n  \n\n\n\n\n\n\ncd <- merge_conc(cd, cd_new)\nnrow(cd)\n\n[1] 4157"
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html#sec-query3",
    "href": "studies/genitive-alternation-retrieval.html#sec-query3",
    "title": "Genitive Alternation I: Retrieval",
    "section": "S-genitive with common nouns",
    "text": "S-genitive with common nouns\nThe third query retrieves attestations of the s-genitive variant with common nouns in the Possessor slot, e.g. “the student’s idea”.\nRegular expression\nThe main regular expression symbols are the same used in the previous queries. The main differences with the first regular expression are twofold. First, we don’t have a word between the two capturing groups; instead, we only have whitespace, and the noun in line 5 must include $ somewhere in its part-of-speech tag. Second, the first capturing group now represents the Possessor, and the second capturing group, the Possessed. This doesn’t affect the writing of the regular expression itself, other than the requirement of $ at the end of the first component and that it should not be present in the second component. However, it will affect the code below when extracting the Possessed and Possessor variables.\nAgain, here we overwrite the pattern variable with the new regular expression.\n\npattern <- r\"--[(?xi)\n      \\b         the / at                  \\s+\n      (  (?: [^\\s/]+ / jj  [^\\s]*          \\s+ )*\n         (?: [^\\s/]+ / nn  [^\\s]*          \\s+ )*\n             [^\\s/]+ / nn  [^\\s]* [$] [^\\s]*\n       )                                   \\s+\n      (  (?: [^\\s/]+ / jj  [^\\s]*          \\s+ )*\n         (?: [^\\s/]+ / nn  [^\\s]*          \\s+ )*\n             [^\\s/]+ / nn  [^\\s$]*            \n       )                                   \\s \n]--\"\n\nCode\nNow that we have merged the first two queries into cd, we don’t need cd_new anymore, so we can overwrite it with the output of the third query. Again, we call conc() with brown_fnames and the new pattern and assign the values that correspond to this concordance: s for gen_type and common for possessor_type, and the appropriate capturing groups of the pattern for possessed and possessor. Since the word order is inverted in relation to the of-genitive, now the first capturing group corresponds to the Possessor and the second one to the Possessed.\nFinally, we use merge_conc() to merge the output of the first two queries, cd, with the output of the third query, cd_new, overwriting cd.\n\ncd_new <- brown_fnames %>%\n  conc(pattern) %>%\n  mutate(\n    gen_type = \"s\",\n    possessor_type = \"common\",\n    possessed = re_replace_first(match, pattern, \"\\\\2\"), \n    possessor = re_replace_first(match, pattern, \"\\\\1\")  \n  )\n\n# print_kwic(cd_new, n = 10)\n\n\nCodecd_new %>% \n  select(left, match, right, possessed, possessor) %>% \n  head(6) %>% \n  kbl(align = c(\"r\", \"c\", \"l\", \"l\", \"l\")) %>%\n  kable_paper(font_size = 15)\n\n\n\n\nTable 4:  Concordance with first results of third query and first annotation. \n \n left \n    match \n    right \n    possessed \n    possessor \n  \n\n\n re-set/vb the/at effective/jj date/nn so/cs that/cs an/at orderly/jj implementation/nn of/in the/at law/nn may/md be/be effected/vbn ''/'' ./. The/at grand/jj jury/nn took/vbd a/at swipe/nn at/in \n    the/at State/nn-tl Welfare/nn-tl Department's/nn$-tl handling/nn \n    of/in federal/jj funds/nns granted/vbn for/in child/nn welfare/nn services/nns in/in foster/jj homes/nns ./. ``/`` This/dt is/bez one/cd of/in the/at major/jj items/nns in/in the/at Fulton/np-tl Co \n    handling/nn \n    State/nn-tl Welfare/nn-tl Department's/nn$-tl \n  \n\n Atlanta/np-tl Bar/nn-tl Association/nn-tl and/cc an/at interim/nn citizens/nns committee/nn ./. ``/`` These/dts actions/nns should/md serve/vb to/to protect/vb in/in fact/nn and/cc in/in effect/nn \n    the/at court's/nn$ wards/nns \n    from/in undue/jj costs/nns and/cc its/pp$ appointed/vbn and/cc elected/vbn servants/nns from/in unmeritorious/jj criticisms/nns ''/'' ,/, the/at jury/nn said/vbd ./. Regarding/in Atlanta's/np$ new/ \n    wards/nns \n    court's/nn$ \n  \n\n e/np of/in Griffin/np ./. Attorneys/nns for/in the/at mayor/nn said/vbd that/cs an/at amicable/jj property/nn settlement/nn has/hvz been/ben agreed/vbn upon/rb ./. The/at petition/nn listed/vbd \n    the/at mayor's/nn$ occupation/nn \n    as/cs ``/`` attorney/nn ''/'' and/cc his/pp$ age/nn as/cs 71/cd ./. It/pps listed/vbd his/pp$ wife's/nn$ age/nn as/cs 74/cd and/cc place/nn of/in birth/nn as/cs Opelika/np ,/, Ala./np ./. The/at pe \n    occupation/nn \n    mayor's/nn$ \n  \n\n more/ap than/in a/at year/nn ./. The/at Hartsfield/np home/nr is/bez at/in 637/cd E./np Pelham/np Rd./nn-tl Aj/nn ./. Henry/np L./np Bowden/np was/bedz listed/vbn on/in the/at petition/nn as/cs \n    the/at mayor's/nn$ attorney/nn \n    ./. Hartsfield/np has/hvz been/ben mayor/nn of/in Atlanta/np ,/, with/in exception/nn of/in one/cd brief/jj interlude/nn ,/, since/in 1937/cd ./. His/pp$ political/jj career/nn goes/vbz back/rb to/ \n    attorney/nn \n    mayor's/nn$ \n  \n\n ith/in exception/nn of/in one/cd brief/jj interlude/nn ,/, since/in 1937/cd ./. His/pp$ political/jj career/nn goes/vbz back/rb to/in his/pp$ election/nn to/in city/nn council/nn in/in 1923/cd ./. \n    The/at mayor's/nn$ present/jj term/nn \n    of/in office/nn expires/vbz Jan./np 1/cd ./. He/pps will/md be/be succeeded/vbn by/in Ivan/np Allen/np Jr./np ,/, who/wps became/vbd a/at candidate/nn in/in the/at Sept./np 13/cd primary/nn after/cs M \n    present/jj term/nn \n    mayor's/nn$ \n  \n\n ue/vb the/at new/jj rural/jj roads/nns bonds/nns ./. Schley/np County/nn-tl Rep./nn-tl B./np D./np Pelham/np will/md offer/vb a/at resolution/nn Monday/nr in/in the/at House/nn-tl to/to rescind/vb \n    the/at body's/nn$ action/nn \n    of/in Friday/nr in/in voting/vbg itself/ppl a/at $10/nns per/in day/nn increase/nn in/in expense/nn allowances/nns ./. Pelham/np said/vbd Sunday/nr night/nn there/ex was/bedz research/nn being/beg \n    action/nn \n    body's/nn$ \n  \n\n\n\n\n\n\ncd <- merge_conc(cd, cd_new)\nnrow(cd)\n\n[1] 5207"
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html#sec-query4",
    "href": "studies/genitive-alternation-retrieval.html#sec-query4",
    "title": "Genitive Alternation I: Retrieval",
    "section": "S-genitive with proper nouns",
    "text": "S-genitive with proper nouns\nThe fourth query captures the s-genitive variant with proper nouns in the Possessor slot, e.g. Belgium’s rivers.\nRegular expression\nAgain we overwrite pattern with the regular expression for the last query, which is very similar to the third query. The difference, like the difference between the second and the first, is that it asks of proper nouns instead of common nouns in the Possessor slot (lines 4 and 5; but also it excludes articles at the beginning) and that it allows for any item with -tl in its part-of-speech tag preceding the Possessor noun (line 3).\n\npattern <- r\"--[(?xi)\n        \\b  ( (?: [^\\s/]+ / jj         [^\\s]*          \\s+ )*\n              (?: [^\\s/]+ / [^\\s]+ -tl [^\\s]*          \\s+ )*\n              (?: [^\\s/]+ / np         [^\\s$]*         \\s+ )*\n                  [^\\s/]+ / np         [^\\s]* [$] [^\\s]*        \n            )                                          \\s+\n            ( (?: [^\\s/]+ / jj         [^\\s]*          \\s+ )*\n              (?: [^\\s/]+ / nn         [^\\s]*          \\s+ )*\n                  [^\\s/]+ / nn         [^\\s$]*           \n            )                                          \\s\n]--\"\n\nCode\nAgain we overwrite the now useless cd_new with the output of the fourth query and assign the appropriate values for the common variables. The possessor_type is now proper, but the rest of the variables take the same values as in the previous query. At the end, we overwrite cd by merging the old cd, which contains the output of the first three queries, and cd_new.\n\ncd_new <- brown_fnames %>%\n  conc(pattern) %>%\n  mutate(\n    gen_type = \"s\",\n    possessor_type = \"proper\",\n    possessed = re_replace_first(match, pattern, \"\\\\2\"), \n    possessor = re_replace_first(match, pattern, \"\\\\1\")  \n  )\n\n# print_kwic(cd_new, n = 10)\n\n\nCodecd_new %>% \n  select(left, match, right, possessed, possessor) %>% \n  head(6) %>% \n  kbl(align = c(\"r\", \"c\", \"l\", \"l\", \"l\")) %>%\n  kable_paper(font_size = 15)\n\n\n\n\nTable 5:  Concordance with first results of fourth query and first annotation. \n \n left \n    match \n    right \n    possessed \n    possessor \n  \n\n\n The/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in \n    Atlanta's/np$ recent/jj primary/nn election/nn \n    produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./. The/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl \n    recent/jj primary/nn election/nn \n    Atlanta's/np$ \n  \n\n nterest/nn in/in the/at election/nn ,/, the/at number/nn of/in voters/nns and/cc the/at size/nn of/in this/dt city/nn ''/'' ./. The/at jury/nn said/vbd it/pps did/dod find/vb that/cs many/ap of/in \n    Georgia's/np$ registration/nn \n    and/cc election/nn laws/nns ``/`` are/ber outmoded/jj or/cc inadequate/jj and/cc often/rb ambiguous/jj ''/'' ./. It/pps recommended/vbd that/cs Fulton/np legislators/nns act/vb ``/`` to/to have/hv \n    registration/nn \n    Georgia's/np$ \n  \n\n at result/nn of/in city/nn personnel/nns policies/nns ''/'' ./. It/pps urged/vbd that/cs the/at city/nn ``/`` take/vb steps/nns to/to remedy/vb ''/'' this/dt problem/nn ./. Implementation/nn of/in \n    Georgia's/np$ automobile/nn title/nn law/nn \n    was/bedz also/rb recommended/vbn by/in the/at outgoing/jj jury/nn ./. It/pps urged/vbd that/cs the/at next/ap Legislature/nn-tl ``/`` provide/vb enabling/vbg funds/nns and/cc re-set/vb the/at effec \n    automobile/nn title/nn law/nn \n    Georgia's/np$ \n  \n\n t's/nn$ wards/nns from/in undue/jj costs/nns and/cc its/pp$ appointed/vbn and/cc elected/vbn servants/nns from/in unmeritorious/jj criticisms/nns ''/'' ,/, the/at jury/nn said/vbd ./. Regarding/in \n    Atlanta's/np$ new/jj multi-million-dollar/jj airport/nn \n    ,/, the/at jury/nn recommended/vbd ``/`` that/cs when/wrb the/at new/jj management/nn takes/vbz charge/nn Jan./np 1/cd the/at airport/nn be/be operated/vbn in/in a/at manner/nn that/wps will/md elimin \n    new/jj multi-million-dollar/jj airport/nn \n    Atlanta's/np$ \n  \n\n /nn opposes/vbz in/in its/pp$ platform/nn ./. Sam/np Caldwell/np ,/, State/nn-tl Highway/nn-tl Department/nn-tl public/jj relations/nns director/nn ,/, resigned/vbd Tuesday/nr to/to work/vb for/in \n    Lt./nn-tl Gov./nn-tl Garland/np Byrd's/np$ campaign/nn \n    ./. Caldwell's/np$ resignation/nn had/hvd been/ben expected/vbn for/in some/dti time/nn ./. He/pps will/md be/be succeeded/vbn by/in Rob/np Ledford/np of/in Gainesville/np ,/, who/wps has/hvz been/ \n    campaign/nn \n    Lt./nn-tl Gov./nn-tl Garland/np Byrd's/np$ \n  \n\n ll/np ,/, State/nn-tl Highway/nn-tl Department/nn-tl public/jj relations/nns director/nn ,/, resigned/vbd Tuesday/nr to/to work/vb for/in Lt./nn-tl Gov./nn-tl Garland/np Byrd's/np$ campaign/nn ./. \n    Caldwell's/np$ resignation/nn \n    had/hvd been/ben expected/vbn for/in some/dti time/nn ./. He/pps will/md be/be succeeded/vbn by/in Rob/np Ledford/np of/in Gainesville/np ,/, who/wps has/hvz been/ben an/at assistant/nn more/ap than/i \n    resignation/nn \n    Caldwell's/np$ \n  \n\n\n\n\n\n\ncd <- merge_conc(cd, cd_new)\nnrow(cd)\n\n[1] 7344"
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html#citing-examples-in-markdown",
    "href": "studies/genitive-alternation-retrieval.html#citing-examples-in-markdown",
    "title": "Genitive Alternation I: Retrieval",
    "section": "Citing examples in markdown",
    "text": "Citing examples in markdown\nWith this new version of the concordance, which you can explore with print_kwic(cd), View(cd) or explore(cd), you can also extract full examples. If you use R Markdown or Quarto to write a report, rather than copy-pasting an example to describe, you can join the new left, match and right columns and call a specific example to illustrate.\nIn the following code:\n\nThe call to mutate() and short_names() shortens the names of the files.\nThe calls to unite() create a column with the name of the first argument (conc_line or id) by joining the rest of the columns mentioned (left, match and right, or source and id). By default they are separated with an underscore, but the sep argument allows you to use a different term.\nThe call to select() extracts only the id and conc_line columns. Afterwards, deframe() turns the two-column tibble into a named character vector: the values are the contexts and their names, the ids.\n\n\nexamples <- cd %>% \n  as_tibble() %>% \n  mutate(source = short_names(source)) %>% \n  unite(conc_line, left, match, right, sep = \" \") %>% \n  unite(id, source, id) %>% \n  select(id, conc_line) %>%\n  deframe()\n\nhead(examples)\n\n                                                                                                                                                                                                                                                                                                         ca01_1 \n                         \"ns million worth of highway reconstruction bonds . The bond issue will go to the state courts for a friendly test suit to test  the validity of the act  , and then the sales will begin and contracts let for repair work on some of Georgia's most heavily traveled highways . A H\" \n                                                                                                                                                                                                                                                                                                         ca02_2 \n             \"servative '' his estimate that it would produce 17 million dollars to help erase an anticipated deficit of 63 million dollars at  the end of the current fiscal year  next Aug. 31 . He told the committee the measure would merely provide means of enforcing the escheat law which has been on\" \n                                                                                                                                                                                                                                                                                                         ca02_3 \n\"over also would require junior-senior high teachers to have at least 24 semester hours credit in the subject they are teaching .  The remainder of the 4-year college requirement  would be in general subjects . `` A person with a master's degree in physics , chemistry , math or English , yet who has n\" \n                                                                                                                                                                                                                                                                                                         ca02_4 \n                            \"/np Clark of Hays , Kan. as the school's new president . Dr. Clark will succeed Dr. J. R. McLemore , who will retire at  the close of the present school term  . Dr. Clark holds an earned Doctor of Education degree from the University of Oklahoma . He also received a Master\" \n                                                                                                                                                                                                                                                                                                         ca03_5 \n                                \"d jury room '' . He said this constituted a `` very serious misuse '' of the Criminal court processes . `` Actually ,  the abuse of the process  may have constituted a contempt of the Criminal court of Cook county , altho vindication of the authority of that court is n\" \n                                                                                                                                                                                                                                                                                                         ca03_6 \n                            \"at 21st and 28th precincts of the 29th ward , the 18th precinct of the 4th ward , and the 9th precinct of the 23d ward .  The case of the judges  in the 58th precinct of the 23d ward had been heard previously and taken under advisement by Karns . Two other cases also were/\" \n\nexamples[[\"ca01_1\"]]\n\n[1] \"ns million worth of highway reconstruction bonds . The bond issue will go to the state courts for a friendly test suit to test  the validity of the act  , and then the sales will begin and contracts let for repair work on some of Georgia's most heavily traveled highways . A H\"\n\n\nThen you could write, in an empty line of your report, a numbered example with R inline code; this lets you easily cross-reference your example with (@label), which returns (1).\n(@label) `r examples[[\"ca01_1\"]]` ``\n\nns million worth of highway reconstruction bonds . The bond issue will go to the state courts for a friendly test suit to test the validity of the act , and then the sales will begin and contracts let for repair work on some of Georgia’s most heavily traveled highways . A H\n\nIn any case, you probably want to edit your example a bit, removing unnecessary text from the extremes, editing sections to include italics or bold or even, if your corpus is in a language other than English, add glosses and translations to print with glossr."
  },
  {
    "objectID": "studies/genitive-alternation-retrieval.html#save",
    "href": "studies/genitive-alternation-retrieval.html#save",
    "title": "Genitive Alternation I: Retrieval",
    "section": "Save your progress and be free!",
    "text": "Save your progress and be free!\nWe are DONE. Congratulations! We can now store the output in a tab-separated file with write_conc(). Then you can open the file with any spreadsheet software for manual cleaning and/or annotation and then read it again from R with read_conc(filename), as we will do in the second part of this study.\n\nwrite_conc(cd, file.path(data_folder, \"genitive-alternation.tab\"))\n\n\n\n\n\n\n\nCSV\n\n\n\nSpreadsheet programs may call this format a “csv”, comma-separated values file. What matters is that it’s a plain text file. A proper csv file will have commas to separate the different columns, or semicolons in some places where commas are used to indicate decimals. However, this is meant for numbers: texts, such as concordances, will inevitably have commas and probably also semicolons. Therefore, we use tabs to separate the columns instead.\nThe tidyverse functions to read and write tab-separated files are read_tsv() and write_tsv() respectively."
  },
  {
    "objectID": "studies/register-analysis.html",
    "href": "studies/register-analysis.html",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "",
    "text": "This document shows how to perform a simple register analysis with Factor Analysis on the Brown corpus. For that purpose we will use the TEI-compliant XML version of the corpus, which will allow us to extract a bit more information from the corpus than other formats. Section 3 will show what the XML file looks like and how we can read and parse it with xml2 and mclm. Section 4 will build on that knowledge to compute a number of frequencies as variables for the Factor Analysis. As we do so, we will illustrate how to take advantage of the possibility of having any type of data in a tibble column. Finally Section 5 will show the Factor Analysis itself with factanal() and illustrate some plotting options."
  },
  {
    "objectID": "studies/register-analysis.html#sec-features",
    "href": "studies/register-analysis.html#sec-features",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n1.1 Features of interest",
    "text": "1.1 Features of interest\nFor this simple analysis we will annotate 13 numerical variables, which is quite modest compared to Biber (1988):\n\n\n\n\n\n\nName\nValue\n\n\n\nttr\nType token ratio (number of types divided by number of tokens)1\n\n\n\nword_len\nAverage word length, in characters\n\n\np_mw\nProportion of combined tags; typically clitics as in she’s\n\n\n\np_c\nProportion of punctuation characters\n\n\np_ppss\nProportion of personal pronouns nominative, besides third person singular\n\n\np_noun\nProportion of nouns\n\n\np_bigr\nNumber of unique word bigrams, relative to document size\n\n\np_nomin\nProportion of nominalisations (nouns ending in -tion, -ment, -ness, or -ity)\n\n\np_pobi\nNumber of unique pos tag bigrams, relative to document size\n\n\np_adj\nProportion of adjectives\n\n\np_neg\nNumber of negations, relative to document size\n\n\np_adv\nProportion of adverbs\n\n\np_qual\nNumber of qualifiers, relative to document size\n\n\n\nThe idea is that these variables are relatively straightforward to annotate — with some programming, they can be easily extracted automatically. The theoretically relevant dimensions, instead, are much harder to operationalize: Factor Analysis is meant to extract these dimensions from the operationalizable linguistic features."
  },
  {
    "objectID": "studies/register-analysis.html#read-from-xml",
    "href": "studies/register-analysis.html#read-from-xml",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.1 Read from XML",
    "text": "4.1 Read from XML\nFirst, we will read the files by applying read_xml() to each element of the filename column in d via map(). This results in a column that is a list of xml_document objects. d$xml[[1]] is the same as read_xml(d$filename[[1]]).\n\n\n\n\n\n\nExample\n\n\n\nmap(some_list, tolower) returns a list in which the first element is tolower(some_list[[1]]), the second is tolower(some_list[[2]]), etc.\n\n\n\nd <- d %>% mutate(xml = map(filename, read_xml))\nd %>% mutate(filename = short_names(filename)) # just hiding the path when printing\n\n# A tibble: 500 × 2\n   filename xml       \n   <fnames> <list>    \n 1 a01      <xml_dcmn>\n 2 a02      <xml_dcmn>\n 3 a03      <xml_dcmn>\n 4 a04      <xml_dcmn>\n 5 a05      <xml_dcmn>\n 6 a06      <xml_dcmn>\n 7 a07      <xml_dcmn>\n 8 a08      <xml_dcmn>\n 9 a09      <xml_dcmn>\n10 a10      <xml_dcmn>\n# … with 490 more rows\n\nd$xml[[1]]\n\n{xml_document}\n<TEI xmlns=\"http://www.tei-c.org/ns/1.0\">\n[1] <teiHeader>\\n  <fileDesc>\\n    <titleStmt>\\n      <title>Sample A01 from  ...\n[2] <text xml:id=\"A01\" decls=\"A\">\\n  <body>\\n    <p>\\n      <s n=\"1\">\\n       ..."
  },
  {
    "objectID": "studies/register-analysis.html#extract-tags-with-find_xpath",
    "href": "studies/register-analysis.html#extract-tags-with-find_xpath",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.2 Extract tags with find_xpath()\n",
    "text": "4.2 Extract tags with find_xpath()\n\nIn order to extract lists of tags from each of the file, we will use mclm::find_xpath(). We need to provide a namespace definition, so we will create a column ns to store the output of xml_ns() on each file. Then we will create four columns with the output of find_xpath() and different XPath patterns: one to collect the word tags w, one to collect the multiword (or “combined tags”) mw, one to collect the punctuation tags c and one to collect all of them, i.e. all the tokens. For this purpose we need to combine both the elements in the xml column and the elements in the newly made ns column. The function to achieve this is map2, which applies a function to each element of two paired lists of the same length.\n\n\n\n\n\n\nExample\n\n\n\nmap2(list1, list2, mean) returns a list whose first element is mean(list1[[1]], list2[[1]]), the second element is mean(list1[[2]], list2[[2]]), etc.\n\n\nThe call to find_xpath() takes two positional arguments (the xml_document and an XPath pattern) and a few named arguments, among which namespaces. In our call inside map2(), we can use a formula, which allows us to call the function and replace the argument corresponding to the first list with .x and the one corresponding to the second list .y.\n\n\n\n\n\n\nExample\n\n\n\nmap2(list1, list2, mean) can also be written as map2(list1, list2, ~ mean(.x, .y)).\nThis notation is useful when the elements in the lists do not correspond to the positional arguments of the function.\n\n\n\nd <- d %>% \n  select(-filename) %>%\n  mutate(\n    ns = map(xml, xml_ns),\n    token_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w | //d1:mw | //d1:c\", namespaces = .y)),\n    word_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w\", namespaces = .y)),\n    mw_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:mw\", namespaces = .y)),\n    punctuation_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:c\", namespaces = .y))\n    )\nd\n\n# A tibble: 500 × 6\n   xml        ns             token_tags word_tags  mw_tags    punctuation_tags\n   <list>     <list>         <list>     <list>     <list>     <list>          \n 1 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 2 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 3 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 4 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 5 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 6 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 7 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 8 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 9 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n10 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n# … with 490 more rows\n\n\n\n\nAfter this, we don’t need the filename column anymore — in this stepwise illustration of the process, I will be removing obsolete columns, but when running everything at once, as shown in Section 4.10, we won’t be doing that; we’ll just select the useful columns at the end.\nEach new column is a list with elements of different kinds. The elements in the token_tags and mw_tags columns, for example, are xml_nodeset objects. Below we can see the first element of each of these columns, i.e. the tokens and the combined tags of the first file in the corpus.\n\nprint(d$token_tags[[1]], max_n = 10) # left side\nd$mw_tags[[1]] # right side\n\n\n\n{xml_nodeset (2242)}\n [1] <w type=\"AT\">The</w>\n [2] <w type=\"NP\" subtype=\"TL\">Fulton</w>\n [3] <w type=\"NN\" subtype=\"TL\">County</w>\n [4] <w type=\"JJ\" subtype=\"TL\">Grand</w>\n [5] <w type=\"NN\" subtype=\"TL\">Jury</w>\n [6] <w type=\"VBD\">said</w>\n [7] <w type=\"NR\">Friday</w>\n [8] <w type=\"AT\">an</w>\n [9] <w type=\"NN\">investigation</w>\n[10] <w type=\"IN\">of</w>\n...\n\n\n{xml_nodeset (3)}\n[1] <mw pos=\"DOD *\">didn't </mw>\n[2] <mw pos=\"DOD *\">didn't </mw>\n[3] <mw pos=\"BEDZ *\">wasn't </mw>"
  },
  {
    "objectID": "studies/register-analysis.html#retrieve-text-with-xml_text",
    "href": "studies/register-analysis.html#retrieve-text-with-xml_text",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.3 Retrieve text with xml_text()\n",
    "text": "4.3 Retrieve text with xml_text()\n\nWith the new columns we can create tokens objects by retrieving the contents of the tags (for word form tokens) or the POS-tags. The columns all_tokens and words are created by applying a chain of functions to the elements of the token_tags and word_tags columns: xml_text() to obtain the content, tolower() to turn it to lower case, cleanup_spaces() to remove any spaces that might be accidentally included in the token, and as_tokens() to turn the resulting character vector into a tokens object. Therefore, the columns all_tokens and words will be lists of tokens objects. Below is an example of the chain of functions as applied to the first element of d$token_tags.\n\nd$token_tags[[1]] %>% xml_text() %>% head()\n\n[1] \"The\"    \"Fulton\" \"County\" \"Grand\"  \"Jury\"   \"said\"  \n\nd$token_tags[[1]] %>% xml_text() %>% tolower() %>% head()\n\n[1] \"the\"    \"fulton\" \"county\" \"grand\"  \"jury\"   \"said\"  \n\nd$token_tags[[1]] %>% xml_text() %>% tolower() %>% cleanup_spaces() %>% head()\n\n[1] \"the\"    \"fulton\" \"county\" \"grand\"  \"jury\"   \"said\"  \n\nd$token_tags[[1]] %>% xml_text() %>% tolower() %>% as_tokens() %>% print(10)\n\nToken sequence of length 2242\nidx         token\n--- -------------\n  1           the\n  2        fulton\n  3        county\n  4         grand\n  5          jury\n  6          said\n  7        friday\n  8            an\n  9 investigation\n 10            of\n...\n\n\n\nd <- d %>%\n  select(-xml, -ns) %>% \n  mutate(\n    all_tokens = map(token_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens()),\n    words = map(word_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens())\n  )\nd\n\n# A tibble: 500 × 6\n   token_tags word_tags  mw_tags    punctuation_tags all_tokens       words   \n   <list>     <list>     <list>     <list>           <list>           <list>  \n 1 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,242]> <tokens>\n 2 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,277]> <tokens>\n 3 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,275]> <tokens>\n 4 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,216]> <tokens>\n 5 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,244]> <tokens>\n 6 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,263]> <tokens>\n 7 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,270]> <tokens>\n 8 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,187]> <tokens>\n 9 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,234]> <tokens>\n10 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,282]> <tokens>\n# … with 490 more rows\n\n\n\n\nNow that we have extracted the terms that we are interested in, we don’t really need the xml and ns columns anymore,\nThe lists below don’t look very different because there are no punctuation marks in the first 10 tokens.\n\nprint(d$all_tokens[[1]], 10) # left\nprint(d$words[[1]], 10) # right\n\n\n\nToken sequence of length 2242\nidx         token\n--- -------------\n  1           the\n  2        fulton\n  3        county\n  4         grand\n  5          jury\n  6          said\n  7        friday\n  8            an\n  9 investigation\n 10            of\n...\n\n\nToken sequence of length 1985\nidx         token\n--- -------------\n  1           the\n  2        fulton\n  3        county\n  4         grand\n  5          jury\n  6          said\n  7        friday\n  8            an\n  9 investigation\n 10            of\n..."
  },
  {
    "objectID": "studies/register-analysis.html#retrieve-pos-tags-with-xml_attr",
    "href": "studies/register-analysis.html#retrieve-pos-tags-with-xml_attr",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.4 Retrieve POS-tags with xml_attr()\n",
    "text": "4.4 Retrieve POS-tags with xml_attr()\n\nIn order to collect the POS-tags, we could simply apply xml_attr() in the same way that we were calling xml_text(), but the situation is a bit more complicated. As it turns out, the attribute indicating the part-of-speech for w and c elements is “type”, but in a mw element it’s called “pos”.\n\nxml_attr(d$token_tags[[1]], \"type\") %>% head()\n\n[1] \"AT\"  \"NP\"  \"NN\"  \"JJ\"  \"NN\"  \"VBD\"\n\nxml_attr(d$mw_tags[[1]], \"type\")\n\n[1] NA NA NA\n\nxml_attr(d$mw_tags[[1]], \"pos\")\n\n[1] \"DOD *\"  \"DOD *\"  \"BEDZ *\"\n\n\nAs a consequence, we need to apply a more complicated workflow to create the pos_codes column, i.e. an equivalent of the all_tokens column but with POS-tags instead of word forms. For each element of token_tags (lets call them toks), we will do the following:\n\nExtract the “type” attribute, which will contain the POS-tag for the w and celements but NA for mw elements, and store it in a most_pos vector: most_pos <- xml_attr(toks, \"type\").\nExtract the “pos” attribute, which will contain the POS-tag for mw elements but NA for the rest, and store it in a mw_pos element: mw_pos <- xml_attr(toks, \"pos\").\nReplace the NA values in most_pos (most_pos[is.na(most_pos)]) with the non-NA values in mw_pos (mw_pos[!is.na(mw_pos)]).\nTurn the resulting, fixed most_pos into a tokens object with as_tokens(most_pos).\n\nIn order to provide such a workflow with map(), we will do it with an anonymous function defined as function(argument) { the code... }.\n\n\n\n\n\n\nExample\n\n\n\nmap(some_list, tolower) is the same as map(some_list, ~ tolower(.x)) and the same as map(some_list, function(list_element) { tolower(list_element) }).\n\n\n\nd <- d %>%\n  mutate(\n    pos_codes = map(token_tags, function(toks) {\n      most_pos <- xml_attr(toks, \"type\")\n      mw_pos <- xml_attr(toks, \"pos\")\n      most_pos[is.na(most_pos)] <- mw_pos[!is.na(mw_pos)]\n      as_tokens(most_pos) # return value\n      })\n    )\nd\n\n# A tibble: 500 × 7\n   token_tags word_tags  mw_tags    punctuation_tags all_tok…¹ words    pos_co…²\n   <list>     <list>     <list>     <list>           <list>    <list>   <list>  \n 1 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 2 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 3 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 4 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 5 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 6 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 7 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 8 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 9 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n10 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n# … with 490 more rows, and abbreviated variable names ¹​all_tokens, ²​pos_codes\n\nprint(d$pos_codes[[1]], 10)\n\nToken sequence of length 2242\nidx token\n--- -----\n  1    AT\n  2    NP\n  3    NN\n  4    JJ\n  5    NN\n  6   VBD\n  7    NR\n  8    AT\n  9    NN\n 10    IN\n..."
  },
  {
    "objectID": "studies/register-analysis.html#build-bigrams",
    "href": "studies/register-analysis.html#build-bigrams",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.5 Build bigrams",
    "text": "4.5 Build bigrams\nThe following step, before we can compute the frequencies we want, is to obtain bigrams. To each element x of all_tokens (for word form bigrams) and pos_codes (for POS-tags bigrams) we will implement the following code: paste(x, c(x[-1], \"EOF\"), sep = \"|\"). As shown below, x[-1] returns the vector x minus its first element, c(x[-1], \"EOF\") appends “EOF” (which stands for End Of File) to the vector, and paste() glues both vectors. paste() is a vectorized function, so it will automatically “paste” the first element of the first vector with the first element of the second vector, the second element of each vector, the third element of each vector, etc. The sep argument lets us decide what character will be used when joining those elements.4\n\nx <- head(letters, 10)\nx\n\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\nx[-1]\n\n[1] \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n\nc(x[-1], \"EOF\")\n\n [1] \"b\"   \"c\"   \"d\"   \"e\"   \"f\"   \"g\"   \"h\"   \"i\"   \"j\"   \"EOF\"\n\npaste(x, c(x[-1], \"EOF\"))\n\n [1] \"a b\"   \"b c\"   \"c d\"   \"d e\"   \"e f\"   \"f g\"   \"g h\"   \"h i\"   \"i j\"  \n[10] \"j EOF\"\n\npaste(x, c(x[-1], \"EOF\"), sep = \"|\")\n\n [1] \"a|b\"   \"b|c\"   \"c|d\"   \"d|e\"   \"e|f\"   \"f|g\"   \"g|h\"   \"h|i\"   \"i|j\"  \n[10] \"j|EOF\"\n\npaste(x, c(x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()\n\nToken sequence of length 10\nidx token\n--- -----\n  1   a|b\n  2   b|c\n  3   c|d\n  4   d|e\n  5   e|f\n  6   f|g\n  7   g|h\n  8   h|i\n  9   i|j\n 10 j|EOF\n\n\nThe result of this code is a vector of bigrams, i.e. each element corresponds to one element of x along with its following element.\n\nd <- d %>% \n  select(-token_tags, -word_tags) %>% \n  mutate(\n    bigrams = map(all_tokens, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()),\n    pos_bigrams = map(pos_codes, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens())\n  )\nd\n\n# A tibble: 500 × 7\n   mw_tags    punctuation_tags all_tokens words    pos_codes bigrams  pos_bigr…¹\n   <list>     <list>           <list>     <list>   <list>    <list>   <list>    \n 1 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 2 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 3 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 4 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 5 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 6 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 7 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 8 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 9 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n10 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n# … with 490 more rows, and abbreviated variable name ¹​pos_bigrams\n\n\n\n\nAt this point we don’t need token_tags or word_tags anymore either.\nBelow we see the first token and POS-tag bigrams found in the first file of the corpus.\n\nprint(d$bigrams[[1]], 10) # left\nprint(d$pos_bigrams[[1]], 10) # right\n\n\n\nToken sequence of length 2242\nidx            token\n--- ----------------\n  1       the|fulton\n  2    fulton|county\n  3     county|grand\n  4       grand|jury\n  5        jury|said\n  6      said|friday\n  7        friday|an\n  8 an|investigation\n  9 investigation|of\n 10     of|atlanta's\n...\n\n\nToken sequence of length 2242\nidx  token\n--- ------\n  1  AT|NP\n  2  NP|NN\n  3  NN|JJ\n  4  JJ|NN\n  5 NN|VBD\n  6 VBD|NR\n  7  NR|AT\n  8  AT|NN\n  9  NN|IN\n 10 IN|NPg\n..."
  },
  {
    "objectID": "studies/register-analysis.html#get-counts-with-n_tokens",
    "href": "studies/register-analysis.html#get-counts-with-n_tokens",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.6 Get counts with n_tokens()\n",
    "text": "4.6 Get counts with n_tokens()\n\nNow we have everything we need to compute the frequencies we were interested in. The new columns won’t be lists anymore but simple numeric vectors, so we will use map_dbl() instead of map(). This function can be used when the output of the function applied to each element is a single number. For example, n_tokens() returns the number of tokens in a tokens, types or freqlist object, so applying it to each element of the all_tokens column, which is a list of tokens objects, will return a series of numbers.\n\n\n\n\n\n\nLists and vectors\n\n\n\nIn contrast to map(), map_dbl() returns a numeric vector rather than a list of numbers, which is a different kind of object in R. For example, if we have a vector c(1, 2, 3), we can divide each element of it by the same number with a vectorized function such as /.\n\nnumeric_vector <- c(1, 2, 3)\nnumeric_vector\n\n[1] 1 2 3\n\nnumeric_vector/2\n\n[1] 0.5 1.0 1.5\n\n\nHowever, if it is a list, that won’t work; we would need a function such as map().\n\na_list <- list(1, 2, 3)\na_list\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\na_list/2\n\nError in a_list/2: non-numeric argument to binary operator\n\n\n\n\nOnce we have the n_tok column, we can create other numeric columns with map_dbl(), applying different functions that compute quantities, and then divide them by the contents of n_tok to obtain proportions. The number of types can be obtained by calling n_types() on the elements of the all_tokens, bigrams and pos_bigrams columns; the number of mw and c tags can be obtained by applying length to the elements of the mw_tags and punctuation_tags columns. For mw elements, which are very infrequent, we will multiply the resulting proportion by 10 000. The mean word length can be be obtained by applying sum(nchar(x))/n_tokens(x) to each element of the words columns: we count the number of characters of each element, sum them and then divide them by the number of elements.\n\nd <- d %>% \n  mutate(\n    n_tok = map_dbl(all_tokens, n_tokens),\n    ttr = map_dbl(all_tokens, n_types)/n_tok,\n    p_bigr = map_dbl(bigrams, n_types) / n_tok,\n    p_pobi = map_dbl(pos_bigrams, n_types) / n_tok,\n    p_mw = map_dbl(mw_tags, length) / n_tok * 10000,\n    p_c = map_dbl(punctuation_tags, length) / n_tok,\n    word_len = map_dbl(words, ~ sum(nchar(.x))/n_tokens(.x))\n  )\nd %>% select(all_tokens, n_tok, ttr, starts_with(\"p_\"), word_len)\n\n# A tibble: 500 × 8\n   all_tokens       n_tok   ttr p_bigr p_pobi  p_mw    p_c word_len\n   <list>           <dbl> <dbl>  <dbl>  <dbl> <dbl>  <dbl>    <dbl>\n 1 <tokens [2,242]>  2242 0.357  0.824  0.207 13.4  0.113      4.99\n 2 <tokens [2,277]>  2277 0.381  0.836  0.185 17.6  0.118      4.95\n 3 <tokens [2,275]>  2275 0.340  0.808  0.187 17.6  0.110      4.88\n 4 <tokens [2,216]>  2216 0.383  0.840  0.226  4.51 0.0884     5.19\n 5 <tokens [2,244]>  2244 0.320  0.765  0.206  0    0.0936     4.77\n 6 <tokens [2,263]>  2263 0.350  0.813  0.202 53.0  0.114      4.89\n 7 <tokens [2,270]>  2270 0.370  0.828  0.202 17.6  0.105      5.13\n 8 <tokens [2,187]>  2187 0.356  0.810  0.212  4.57 0.0841     4.91\n 9 <tokens [2,234]>  2234 0.379  0.823  0.179 17.9  0.0971     4.94\n10 <tokens [2,282]>  2282 0.368  0.832  0.216  4.38 0.120      4.82\n# … with 490 more rows\n\n\nAs the output above shows, the new columns are numeric vectors rather than lists."
  },
  {
    "objectID": "studies/register-analysis.html#filter-tokens-with-re",
    "href": "studies/register-analysis.html#filter-tokens-with-re",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.7 Filter tokens with re()\n",
    "text": "4.7 Filter tokens with re()\n\nFinally, we want to obtain the number of nominalizations and of different grammatical categories. We can obtain the nominalizations by extracting the word forms in the all_tokens column with a regular expression:\n\nfirst_tokens <- d$all_tokens[[1]]\nprint(first_tokens[re(\"..+(tion|ment|ness|ity)$\")], 10)\n\nToken sequence of length 80\nidx          token\n--- --------------\n  1  investigation\n  2       election\n  3       election\n  4       election\n  5       election\n  6   registration\n  7       election\n  8 administration\n  9     department\n 10 implementation\n...\n\nn_tokens(first_tokens[re(\"..+(tion|ment|ness|ity)$\")])\n\n[1] 80\n\nn_tokens(first_tokens[re(\"..+(tion|ment|ness|ity)$\")]) / d$n_tok[[1]]\n\n[1] 0.036\n\nn_tokens(first_tokens[re(\"..+(tion|ment|ness|ity)$\")]) / d$n_tok[[1]] * 10000\n\n[1] 357\n\n\nThe code below applies such workflow to the all_tokens column, obtaining the proportion of tokens that are nominalizations in each of the files, and similar instructions to pos_codes, extracting the proportion of tokens that are nouns in each file.\n\nd <- d %>% \n  mutate(\n    p_nomin = map_dbl(all_tokens, ~n_tokens(.x[re(\"..+(tion|ment|ness|ity)$\")])) / n_tok * 10000,\n    p_noun = map_dbl(pos_codes, ~ n_tokens(.x[re(\"NN\")])) / n_tok\n  )\nd %>% select(all_tokens, pos_codes, p_nomin, p_noun)\n\n# A tibble: 500 × 4\n   all_tokens       pos_codes        p_nomin p_noun\n   <list>           <list>             <dbl>  <dbl>\n 1 <tokens [2,242]> <tokens [2,242]>    357.  0.244\n 2 <tokens [2,277]> <tokens [2,277]>    211.  0.250\n 3 <tokens [2,275]> <tokens [2,275]>    224.  0.256\n 4 <tokens [2,216]> <tokens [2,216]>    370.  0.225\n 5 <tokens [2,244]> <tokens [2,244]>    241.  0.223\n 6 <tokens [2,263]> <tokens [2,263]>    305.  0.226\n 7 <tokens [2,270]> <tokens [2,270]>    304.  0.224\n 8 <tokens [2,187]> <tokens [2,187]>    251.  0.220\n 9 <tokens [2,234]> <tokens [2,234]>    255.  0.269\n10 <tokens [2,282]> <tokens [2,282]>    153.  0.208\n# … with 490 more rows"
  },
  {
    "objectID": "studies/register-analysis.html#build-many-columns-at-once-and-unnest",
    "href": "studies/register-analysis.html#build-many-columns-at-once-and-unnest",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.8 Build many columns at once and unnest()\n",
    "text": "4.8 Build many columns at once and unnest()\n\nFor the other POS-tags, we can do the same we did for the nouns. In the case of personal pronouns (“PPSS”), adjectives (“JJ”), negations (“*“), adverbs (”RB”) and past tenses (“QL”), which are very infrequent, we’ll multiply the proportions by 10 000. Therefore, given a tokens object with POS-tags pos and a number of tokens nt, for each of these regular expressions regex we will run the exact same code: n_tokens(pos[re(regex)])/n_tok * 10000. In order to avoid redundancy, we can do this for each POS-tag with map_dbl().\n\npos <- d$pos_codes[[1]]\nnt <- d$n_tok[[1]]\npos_mapping <- c(p_ppss = \"PPSS\", p_adj = \"JJ\",\n                 p_neg = \"[*]\", p_adv = \"RB\", p_qual = \"QL\")\nmap_dbl(pos_mapping, ~n_tokens(pos[re(.x)])/nt * 10000)\n\np_ppss  p_adj  p_neg  p_adv p_qual \n    36    464     13    205     27 \n\nmap_dbl(pos_mapping, ~n_tokens(pos[re(.x)])/nt * 10000) %>% as_tibble_row()\n\n# A tibble: 1 × 5\n  p_ppss p_adj p_neg p_adv p_qual\n   <dbl> <dbl> <dbl> <dbl>  <dbl>\n1   35.7  464.  13.4  205.   26.8\n\n\nIf we wrap this map_dbl() call inside a function pos_proportions(), we can easily apply ti to any pos and nt values.\n\npos_proportions <- function(pos, nt) {\n  pos_mapping <- c(p_ppss = \"PPSS\", p_adj = \"JJ\", p_neg = \"[*]\",\n                   p_adv = \"RB\", p_qual = \"QL\")\n  map_dbl(pos_mapping, ~n_tokens(pos[re(.x)])/nt * 10000) %>% \n    as_tibble_row()\n}\npos_proportions(pos, nt)\n\n# A tibble: 1 × 5\n  p_ppss p_adj p_neg p_adv p_qual\n   <dbl> <dbl> <dbl> <dbl>  <dbl>\n1   35.7  464.  13.4  205.   26.8\n\n\nWe can then apply our custom function pos_proportions to each element pair made from an item of pos_codes and one of n_tok using map2(), which will create a column of tibbles. Afterwards, unnest() turns the columns of those mini tibbles into columns of our dataframe.\n\nd <- d %>% mutate(pos_prop = map2(pos_codes, n_tok, pos_proportions))\nd %>% select(pos_codes, n_tok, pos_prop)\n\n# A tibble: 500 × 3\n   pos_codes        n_tok pos_prop        \n   <list>           <dbl> <list>          \n 1 <tokens [2,242]>  2242 <tibble [1 × 5]>\n 2 <tokens [2,277]>  2277 <tibble [1 × 5]>\n 3 <tokens [2,275]>  2275 <tibble [1 × 5]>\n 4 <tokens [2,216]>  2216 <tibble [1 × 5]>\n 5 <tokens [2,244]>  2244 <tibble [1 × 5]>\n 6 <tokens [2,263]>  2263 <tibble [1 × 5]>\n 7 <tokens [2,270]>  2270 <tibble [1 × 5]>\n 8 <tokens [2,187]>  2187 <tibble [1 × 5]>\n 9 <tokens [2,234]>  2234 <tibble [1 × 5]>\n10 <tokens [2,282]>  2282 <tibble [1 × 5]>\n# … with 490 more rows\n\nd %>% select(pos_codes, n_tok, pos_prop) %>% unnest(pos_prop)\n\n# A tibble: 500 × 7\n   pos_codes        n_tok p_ppss p_adj p_neg p_adv p_qual\n   <list>           <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1 <tokens [2,242]>  2242   35.7  464. 13.4   205.   26.8\n 2 <tokens [2,277]>  2277   39.5  457.  8.78  193.   13.2\n 3 <tokens [2,275]>  2275   30.8  541. 13.2   163.   39.6\n 4 <tokens [2,216]>  2216   18.1  731.  4.51  311.   99.3\n 5 <tokens [2,244]>  2244   62.4  584.  0     196.   26.7\n 6 <tokens [2,263]>  2263  128.   535.  8.84  163.   13.3\n 7 <tokens [2,270]>  2270   48.5  604.  8.81  163.   26.4\n 8 <tokens [2,187]>  2187   45.7  590.  4.57  306.   77.7\n 9 <tokens [2,234]>  2234   44.8  389.  8.95  179.   22.4\n10 <tokens [2,282]>  2282  101.   482.  4.38  184.   48.2\n# … with 490 more rows\n\nd <- d %>% unnest(pos_prop) # assign to d"
  },
  {
    "objectID": "studies/register-analysis.html#sec-mat",
    "href": "studies/register-analysis.html#sec-mat",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.9 Obtain matrix",
    "text": "4.9 Obtain matrix\nThe last we have to do is select the columns we are actually interested in, turning our output into a matrix and setting the rownames with the short version of our filenames.\n\nd <- d %>% select(ttr, word_len, starts_with(\"p_\"))\ncolnames(d)\n\n [1] \"ttr\"      \"word_len\" \"p_bigr\"   \"p_pobi\"   \"p_mw\"     \"p_c\"     \n [7] \"p_nomin\"  \"p_noun\"   \"p_ppss\"   \"p_adj\"    \"p_neg\"    \"p_adv\"   \n[13] \"p_qual\"  \n\nd_mat <- as.matrix(d)\nrownames(d_mat) <- short_names(tei_fnames)\nd_mat[1:10,]\n\n     ttr word_len p_bigr p_pobi p_mw   p_c p_nomin p_noun p_ppss p_adj p_neg\na01 0.36      5.0   0.82   0.21 13.4 0.113     357   0.24     36   464  13.4\na02 0.38      4.9   0.84   0.19 17.6 0.118     211   0.25     40   457   8.8\na03 0.34      4.9   0.81   0.19 17.6 0.110     224   0.26     31   541  13.2\na04 0.38      5.2   0.84   0.23  4.5 0.088     370   0.22     18   731   4.5\na05 0.32      4.8   0.77   0.21  0.0 0.094     241   0.22     62   584   0.0\na06 0.35      4.9   0.81   0.20 53.0 0.114     305   0.23    128   535   8.8\na07 0.37      5.1   0.83   0.20 17.6 0.105     304   0.22     48   604   8.8\na08 0.36      4.9   0.81   0.21  4.6 0.084     251   0.22     46   590   4.6\na09 0.38      4.9   0.82   0.18 17.9 0.097     255   0.27     45   389   9.0\na10 0.37      4.8   0.83   0.22  4.4 0.120     153   0.21    101   482   4.4\n    p_adv p_qual\na01   205     27\na02   193     13\na03   163     40\na04   311     99\na05   196     27\na06   163     13\na07   163     26\na08   306     78\na09   179     22\na10   184     48"
  },
  {
    "objectID": "studies/register-analysis.html#sec-full",
    "href": "studies/register-analysis.html#sec-full",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n4.10 Summary",
    "text": "4.10 Summary\nThis section showed the process of creating a matrix with proportions for Factor Analysis from a list of XML files. It paused the workflow at different stages in order to show what each step did and explain what the different functions try to achieve, but in practice it could all be done in one chain. Such code is shown below.\n\n\nd <- as_tibble(tei_fnames) %>% \n  mutate(\n    xml = map(filename, read_xml),\n    ns = map(xml, xml_ns),\n    token_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w | //d1:mw | //d1:c\", namespaces = .y)),\n    word_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w\", namespaces = .y)),\n    mw_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:mw\", namespaces = .y)),\n    punctuation_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:c\", namespaces = .y)),\n    all_tokens = map(token_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens()),\n    words = map(word_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens()),\n    pos_codes = map(token_tags, function(toks) {\n      most_pos <- xml_attr(toks, \"type\")\n      mw_pos <- xml_attr(toks, \"pos\")\n      most_pos[is.na(most_pos)] <- mw_pos[!is.na(mw_pos)]\n      as_tokens(most_pos)\n    }),\n    bigrams = map(all_tokens, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()),\n    pos_bigrams = map(pos_codes, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()),\n    n_tok = map_dbl(all_tokens, n_tokens),\n    ttr = map_dbl(all_tokens, n_types)/n_tok,\n    p_bigr = map_dbl(bigrams, n_types) / n_tok,\n    p_pobi = map_dbl(pos_bigrams, n_types) / n_tok,\n    p_mw = map_dbl(mw_tags, length) / n_tok * 10000,\n    p_c = map_dbl(punctuation_tags, length) / n_tok,\n    word_len = map_dbl(words, ~ sum(nchar(.x))/n_tokens(.x)),\n    p_nomin = map_dbl(all_tokens, ~n_tokens(.x[re(\"..+(tion|ment|ness|ity)$\")])) / n_tok * 10000,\n    p_noun = map_dbl(pos_codes, ~ n_tokens(.x[re(\"NN\")])) / n_tok,\n    sp = map2(pos_codes, n_tok, small_proportions)) %>% \n  unnest(sp) %>% \n  mutate(filename = short_names(filename)) %>% \n  select(filename, ttr, word_len, starts_with(\"p_\"))\nd_mat <- data.frame(d, row.names = \"filename\") %>% \n  as.matrix()\n\nAnd we are ready to run Factor Analysis!\n\n\n\n\n\n\nSave and read\n\n\n\nYou might have noticed that in the final piece of code we defined the rownames of the matrix in a different way! In Section 4.9, d doesn’t contain the filenames, and instead we added the rownames with rownames(d_mat) <- short_names(tei_fnames). In Section 4.10, instead, since we never removed the filename column, we turn it into its short version and then include it as the first column of the final output. If we do this, we can then set the values of the filename column as the rownames of the matrix by first turning d into a data frame with data.frame(d, row.names = \"filename\").\nWhy would we do that? Well, with the new d that includes filenames, we can save the tibble for future use and analysis using readr::write_tsv()5 and then read it with readr::read_tsv(). After reading it again, we can turn it into a matrix setting the names of the filename column as rownames. This is done in the Factor Analysis slides too!\n\nwrite_tsv(d, here::here(\"studies\", \"register-analysis.tsv\"))\nd_mat <- read_tsv(here::here(\"studies\", \"register-analysis.tsv\"), show_col_types = FALSE) %>% \n  data.frame(row.names = \"filename\") %>% \n  as.matrix()"
  },
  {
    "objectID": "studies/register-analysis.html#plotting-files",
    "href": "studies/register-analysis.html#plotting-files",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n5.1 Plotting files",
    "text": "5.1 Plotting files\nOnce we have the scores tibble, we can use it to plot the files based on certain factors. For example, Figure 1 below plots all files based on their scores in Factors 1 and 2 and highlights the files from the “a” component.\n\ncomp <- \"a\"\nthis_comp <- filter(scores, Component == comp)\nother_comps <- filter(scores, Component != comp)\nggplot(other_comps, aes(x = Factor1, y = Factor2)) +\n  geom_point(shape = 3, color = \"gray\") +\n  geom_text(data = this_comp, label = comp, color = \"steelblue\") +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\")\n\n\n\nFigure 1: Scatterplot of Brown files and their scores in the first and second factors, highlighting those from the “a” component.\n\n\n\n\nWe could also use cowplot::plot_grid() to plot a list of these plots, each of them highlighting a different component. We can create such a list with map(), applying a variant of the code from above to each of the components (obtained with unique(scores$Component)). We can then give this list of plots as plotlist argument to plot_grid(), resulting in Figure 2.\n\nlibrary(cowplot)\nplots <- map(unique(scores$Component), function(comp) {\n  this_comp <- filter(scores, Component == comp)\n  other_comps <- filter(scores, Component != comp)\n  ggplot(other_comps, aes(x = Factor1, y = Factor2)) +\n    geom_point(shape = 3, color = \"gray\") +\n    geom_point(data = this_comp, color = \"steelblue\", alpha = 0.8) +\n    geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n    geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\") +\n    theme(axis.title = element_blank())\n})\nplot_grid(plotlist = plots, labels = unique(scores$Component))\n\n\n\nFigure 2: Scatterplots of Brown files and their scores in the first and second factors, highlighting one component at a time."
  },
  {
    "objectID": "studies/register-analysis.html#plotting-registers",
    "href": "studies/register-analysis.html#plotting-registers",
    "title": "Simple register analysis (multidimensional analysis) of brown TEI",
    "section": "\n5.2 Plotting registers",
    "text": "5.2 Plotting registers\nAlternatively, we could obtain the centroid of each Brown component by computing the mean of the scores of its components. This can be achieved via group_by() combined with summarize():\n\nscores %>% \n  group_by(Component) %>% \n  summarize(Factor1 = mean(Factor1))\n\n# A tibble: 15 × 2\n   Component  Factor1\n   <chr>        <dbl>\n 1 a         -0.224  \n 2 b          0.138  \n 3 c         -0.00658\n 4 d         -0.418  \n 5 e          0.266  \n 6 f         -0.268  \n 7 g         -0.230  \n 8 h          1.15   \n 9 j          0.904  \n10 k         -1.36   \n11 l         -0.0854 \n12 m         -0.384  \n13 n         -0.724  \n14 p         -0.151  \n15 r         -0.395  \n\n\nBy default, summarize() only returns the variables requested in its call (here Factor1) and the grouping variables (here Component). We can compute the mean of all the numeric variables by adding a call to across() and selecting all numeric variables with where(is.numeric). The resulting figure is shown in Figure 3.\n\ncentroids <- scores %>% \n  group_by(Component) %>% \n  summarize(across(where(is.numeric), mean))\nhead(centroids)\n\n# A tibble: 6 × 5\n  Component  Factor1 Factor2  Factor3 Factor4\n  <chr>        <dbl>   <dbl>    <dbl>   <dbl>\n1 a         -0.224    0.154   1.17     -1.10 \n2 b          0.138   -0.0602  0.906     0.221\n3 c         -0.00658 -0.240   1.31      0.100\n4 d         -0.418   -1.21   -0.597     0.930\n5 e          0.266    0.0557 -0.00126  -0.112\n6 f         -0.268   -0.614   0.0908    0.226\n\nggplot(scores, aes(x = Factor1, y = Factor2, label = Component)) +\n  geom_text(color = \"gray\") +\n  geom_text(data = centroids, size = 6) +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\")\n\n\n\nFigure 3: Scatterplot of component centroids from the Brown corpus and their scores in Factors 1 and 2.\n\n\n\n\nGoing back to Table 1, we see that components “h” and “j”, i.e. Miscellaneous and Learned, have the highest values along the first factor: they tend to have longer words, more nominalizations and more adjectives, whereas component “k” (General Fiction) lies on the opposite pole with shorter words and fewer nominalizations and adjectives. Across the second factor, where higher values indicate a stronger tendency towards composite tags (clitics), negation and punctuation marks, we have “l” and “p” at the top (Mystery and Detective Fiction along with Romance and Love Stories) and “d” at the bottom (Religion).\nHowever, as we could also see in Figure 2, each register is very broadly distributed across these two factors; some of them have more clear tendencies (“g” is fully on the negative side of Factor 1; “h” is almost entirely on the positive side of Factor 2 whereas “k” is almost entirely on its negative side…) and others much less.\nMore remains to be done in terms of description: concretely, the rest of the factors should also be interpreted and plotted to understand how (and if) the registers can be described in terms of them."
  },
  {
    "objectID": "git/assignment.html",
    "href": "git/assignment.html",
    "title": "Corpus description",
    "section": "",
    "text": "The goal of this assignment is to help solidify what we learned on the second class of the course and get some idea of how to describe a corpus.\nI am assuming that you have an R project connected to your Github repository; if you don’t, please look at the Git Cheatsheet. I also assume you have downloaded the corpora from Toledo (the “mcl.zip” file).\nInstructions\n\nCreate a branch for the assignment, e.g. explore-corpus. You will work here and only move your changes to main if and when you want to submit.\nCopy or move the “brown” folder to your project folder, in the location of your preference (see Slides).\nAdd the “brown” folder to “.gitignore” so it is not tracked by Git (see Slides).\nCreate a Quarto document. In this document, write a description of the corpus. You may use information from the “README” and “CONTENTS” file within the “brown” corpus as well as Wikipedia information or from other sources.\nRender your Quarto document into the output of your choice (word, html, pdf…).\nStage and commit all the relevant files (see Cheatsheet if you don’t remember how).\nMerge the changes into your main branch (see Cheatsheet).\nPush the changes to the remote.\nSend me an e-mail so I check if it went ok.\nTips for the Quarto document\nIt’s ok if your file just has text for now. You could also create an R script with the code used in the lecture slides and interlace code with your text.\n\n\nCode in a chunk\nCode in a separate script\n\n\n\nInside your .qmd file, you can insert the following chunk:\n\n```{r}\n#| label: chunk\n#| output: false\n#| warning: false\nlibrary(here)\nlibrary(tidyverse)\nlibrary(mclm)\n\npath_to_corpus <- here(\"studies\", \"_corpora\", \"brown\") # adapt\nbrown_fnames <- get_fnames(path_to_corpus) %>% \n  keep_re(\"/c[a-z]\")\nflist <- freqlist(brown_fnames, re_token_splitter = re(\"\\\\s+\"))\n```\n\n\n\nYou could create a script, e.g. “script.R”, where you put all the code:\n# script.R\n# Load packages ----\nlibrary(here)\nlibrary(tidyverse)\nlibrary(mclm)\n\n# Load data ----\npath_to_corpus <- here(\"studies\", \"_corpora\", \"brown\") # adapt\n\n## List filenames ----\nbrown_fnames <- get_fnames(path_to_corpus) %>% \n  keep_re(\"/c[a-z]\")\nflist <- freqlist(brown_fnames, re_token_splitter = re(\"\\\\s+\"))\nAnd then, in your .qmd file, you call the code with one of the options in the slides, e.g.\n\n```{r}\n#| label: source\nsource(here::here(\"script.R\"))\n```\n\n\n\n\nThen you can call the flist object you created in the following piece of text:\nThe Brown corpus used in this project has `r prettyNum(n_tokens(flist))` tokens\nand `r n_types(flist)` types,\ngiving us a type-token ratio of `r round(n_types(flist)/n_tokens(flist), 2)`.\nAnd the output should read:\n\nThe Brown corpus used in this project has 1162192 tokens and 63517 types, giving us a type-token ratio of 0.05.\n\nGit workflow\ngit status # check that you're on main, nothing to commit...\ngit branch explore-corpus\ngit checkout explore-corpus\n# copy corpus folder, edit and save .gitignore\n# work on your .qmd file, render\ngit status # check that the right files are unstaged, the corpus does not show up\ngit add .\ngit commit -m \"describe corpus\"\n# you may also run `git push -u origin explore-corpus` if you want to push to your own branch\ngit checkout main\ngit status # check everything is fine. New files should not be there\ngit merge explore-corpus\n# Now the .qmd file and the rendered file should be present\ngit push\n# and send me a message!"
  },
  {
    "objectID": "git/cheatsheet.html",
    "href": "git/cheatsheet.html",
    "title": "Git Cheetsheet",
    "section": "",
    "text": "Reference cheatsheet (or as it going, reference manual) for using Git(Hub) with the course project. All the code starting with git should be typed in the (Git Bash) Terminal.\nIf you have already set up your project and repository, you may skip Section 1. Section 2 goes over the basic workflow you might do any time you work on your project, whereas Section 3 delves into the particular workflow when working with branches. Finally, Section 4 goes over the main technical terms (but do tell me if you’d like me to add any other) and Section 5 will collect common issues that you’ve reported."
  },
  {
    "objectID": "git/cheatsheet.html#create-an-r-project-that-is-also-a-git-repository.",
    "href": "git/cheatsheet.html#create-an-r-project-that-is-also-a-git-repository.",
    "title": "Git Cheetsheet",
    "section": "\n1.1 Create an R project that is also a git repository.",
    "text": "1.1 Create an R project that is also a git repository.\n\n\n\nAt project creation\nUse usethis::use_git()\n\n\n\nWhen you start an R project in R Studio, check the box that says “Create a git repository”.\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you don’t see the box, you probably still have to install Git!\n\n\n\n\nIf you created an R project that is not a Git repository, you can still turn it into one with usethis::use_git(). When an R project is also a Git repository, you will see a “Git” tab.\n\n# install.packages(\"usethis\") # if not installed\nusethis::use_git()"
  },
  {
    "objectID": "git/cheatsheet.html#stage-and-commit",
    "href": "git/cheatsheet.html#stage-and-commit",
    "title": "Git Cheetsheet",
    "section": "\n1.2 Stage and commit",
    "text": "1.2 Stage and commit\nA freshly created R project + git repository has two files: the “{project-name}.Rproj” file that indicates that it’s an R project, and the “.gitignore” file that tells Git which files not to track.\nBy setting up a git repository in an R project folder, “.gitignore” is automatically populated with types of files that should be ignored:\n# .gitignore\n.Rproj.user\n.Rhistory\n.RData\n.Ruserdata\nIn order to start up with git, you need to commit the files. Before committing, you need to stage the files you wish to commit. In other words, staging states which files you want to keep track of, and committing takes a snapshot of their current state.\ngit add .\ngit commit -m \"first commit\"\nWe stage with git add; git add . stages all files. Eventually you might want to only stage some of them, e.g. specific folders or files; in that case you type git add my-file.txt or git add some-folder.\nWith git commit we take a snapshot of all the staged files; -m indicates that the following text (in quotation marks) is the commit message. This message should be a short description of the change you’re taking a snapshot of."
  },
  {
    "objectID": "git/cheatsheet.html#connect-to-the-remote",
    "href": "git/cheatsheet.html#connect-to-the-remote",
    "title": "Git Cheetsheet",
    "section": "\n1.3 Connect to the remote",
    "text": "1.3 Connect to the remote\nThe next step after committing changes, if you want online backup or to submit something, is to push. This uploads the current commits to the remote repository, i.e. the url you received. However, first we have to tell git what that url is. Suppose that your url is https://github.com/mclm2022/montes.git (It will not be montes), then you write the following:\ngit remote add origin https://github.com/mclm2022/montes.git\nThis has set the url as a remote destination called origin. You could also add a different remote, e.g. a repository in your private account, with a different name."
  },
  {
    "objectID": "git/cheatsheet.html#push-changes",
    "href": "git/cheatsheet.html#push-changes",
    "title": "Git Cheetsheet",
    "section": "\n1.4 Push changes",
    "text": "1.4 Push changes\nFinally we can push the changes. Normally, git push will suffice. This first time, we indicate that the default remote location is origin and that we push the main branch, creating a main branch in the remote if it doesn’t exist.\nIf your main branch is not called main (but maybe master), we first rename it (with the first line below).\ngit branch -M main\ngit push -u origin main\n\n\n\n\n\n\nErrors in pushing\n\n\n\nIf you did not commit changes yet, there is nothing to push! You will get an error. Make sure you have committed something!\n\n\nIf you have never done something like this in R Studio, you will also be asked to identify yourself and/or log in to GitHub. Just follow the instructions on the Terminal :)"
  },
  {
    "objectID": "git/cheatsheet.html#work-on-your-local-version.",
    "href": "git/cheatsheet.html#work-on-your-local-version.",
    "title": "Git Cheetsheet",
    "section": "\n2.1 Work on your local version.",
    "text": "2.1 Work on your local version.\nWrite code, write your Quarto file, whatever you want."
  },
  {
    "objectID": "git/cheatsheet.html#stage-the-changes-you-will-want-to-commit.",
    "href": "git/cheatsheet.html#stage-the-changes-you-will-want-to-commit.",
    "title": "Git Cheetsheet",
    "section": "\n2.2 Stage the changes you will want to commit.",
    "text": "2.2 Stage the changes you will want to commit.\nHere you can select which files you want to keep and discard changes you’re not interested in! You’re just letting git know what it will have to track.\nThe handiest version is with the period/dot, which stages all files that have not been staged yet.\ngit add .\nAlternatively, you can specify a file or folder to stage, if there are files you don’t want to track yet. For example, you may have changes in both your code (let’s say, an “R” folder) and your Quarto file but you want to link them to different commit messages.\ngit add R\ngit commit -m \"Updated code\"\ngit add report.qmd\ngit commit -m \"fix typos in Quarto document\""
  },
  {
    "objectID": "git/cheatsheet.html#commit",
    "href": "git/cheatsheet.html#commit",
    "title": "Git Cheetsheet",
    "section": "\n2.3 Commit",
    "text": "2.3 Commit\nCommitting equals to taking a snapshot of the state of your project. You can do this several times a day if you have different steps or important changes.\nIf you just run git commit, you will be taken to an editor to introduce a message. You can avoid this by adding the -m flag followed by an informative message.\ngit commit -m \"This is a commit message\"\nUp to committing, this is simply working locally, with Git."
  },
  {
    "objectID": "git/cheatsheet.html#push",
    "href": "git/cheatsheet.html#push",
    "title": "Git Cheetsheet",
    "section": "\n2.4 Push",
    "text": "2.4 Push\nThe command to upload your changes to the remote repository, e.g. the GitHub repository, is git push.\ngit push\nIf there is any incompatibility with the remote repository, you will get a warning.\n\n\n\n\n\n\nNote\n\n\n\nIf we get to the point that I give feedback by modifying your project and pushing my notes to the local repository, you will have to retrieve the changes with git pull. You should do that before you stage or commit anything else, or it will be troublesome."
  },
  {
    "objectID": "git/cheatsheet.html#create-a-new-branch",
    "href": "git/cheatsheet.html#create-a-new-branch",
    "title": "Git Cheetsheet",
    "section": "\n3.1 Create a new branch",
    "text": "3.1 Create a new branch\nThe command to create a new branch is git branch.\ngit branch new-branch\nThis only sets up its creation, but does not move you to the branch."
  },
  {
    "objectID": "git/cheatsheet.html#switch-to-a-branch",
    "href": "git/cheatsheet.html#switch-to-a-branch",
    "title": "Git Cheetsheet",
    "section": "\n3.2 Switch to a branch",
    "text": "3.2 Switch to a branch\nYou can move from one branch to the other with git checkout <branch>:\ngit checkout new-branch\nIf you’re using Git Bash, you should see the name of your branch in parenthesis after the name of your directory in the Terminal. The output would be something like this:\nuser@machine project-directory (main)\n$ git checkout new-branch\nuser@machine project-directory (new-branch)\n$\nYou can also switch to a non-existent branch and create it simultaneously with the -b flag:\ngit checkout -b new-branch"
  },
  {
    "objectID": "git/cheatsheet.html#make-changes-and-commit",
    "href": "git/cheatsheet.html#make-changes-and-commit",
    "title": "Git Cheetsheet",
    "section": "\n3.3 Make changes and commit",
    "text": "3.3 Make changes and commit\nIn order for this branch to have any substance at all, you need to commit something. The example in class was to copy a folder with a corpus and to add the name of the corpus in the “.gitignore” file. This way, the “.gitignore” has changed and it has to be staged and committed.\nIf you want a more drastic change, you could create a file, such a script or a Quarto document. Once you have added and committed it, it exists in the new-branch branch but not in main.\ngit add .\ngit commit -m \"start new branch\"\nBy running git checkout main you will switch to the main branch and see that the new file is not there. You would also see that “.gitignore” is not ignoring the corpus folder.\n\n\nPush to remote\nMerge branches\nMove between branches\n\n\n\nIf you want to send the changes to a new-branch branch in the remote location (because you like the online-backup option), you can also push them. As before, the first time you will have to run:\ngit push -u origin new-branch\nAnd the rest of the times:\ngit push\nIf you do just git push from the beginning, it will throw you an error and give you the appropriate instructions.\n\n\n\n\n\n\nImportant\n\n\n\nI will not look at things you push to branches other than main!\nIf you want to submit something, push the changes to the remote main.\nIf you want to keep it to yourself, push it to another branch!\n\n\n\n\nOnce you are done with a branch you can merge it with another… which doesn’t mean you delete it. It just means that the current state of the new branch is brought to the previous branch. For example, if you want to bring all you have in new-branch to the main branch, you run git checkout followed by git merge. Optionally, you can delete the other branch with git branch -d new-branch… but it’s not necessary.\ngit checkout main\ngit merge new-branch\nOr alternatively\ngit merge main new-branch\n\n\nIf you just want to move part of the work in one branch to the other, instead of merging them fully, you can use git checkout <branch> <file>, e.g.\ngit checkout main\ngit checkout new-branch R"
  },
  {
    "objectID": "git/cheatsheet.html#sec-nopush",
    "href": "git/cheatsheet.html#sec-nopush",
    "title": "Git Cheetsheet",
    "section": "\n5.1 Unable to push",
    "text": "5.1 Unable to push\nError message includes the following hint:\nUpdates were rejected because the remote contains work that you do not have locally.\nThis means that your remote (what you see on the GitHub site) has history that the local version (what you have on your computer) has never had. The most likely reason is that you added or modified a file from the GitHub website instead of creating it locally and pushing it.\nThis is not wrong per se, but it complicates things (that’s why I didn’t ask you to do that). How do you solve it?\ngit pull\nThis is going to try to bring the local version up-to-date with the remote version. It might also give you instructions on how to merge it –maybe you have to write a message or decide on disagreements.\n\n\n\n\n\n\nTip\n\n\n\nSometimes git itself gives you suggestions on code you should write. For example, git pull may fail and you receive a message suggesting you to write something like:\ngit pull <remote> <branch>\nBecause your remote is called origin, and when your branch is called main, this translates to:\ngit pull origin main\nIf you do get this instruction, notice that there is an additional suggestion to run:\ngit branch --set-to-origin=origin/<branch> main\nThis means that you still need to inform what is the name of the origin branch (the branch in the remote, i.e. the GitHub website) that matches your local main branch. It will be main, so just replace <branch> with main and you will be able to just git pull in the future.\n\n\nPulling in GitHub is part of a daily workflow when you have more than one local source pushing into the remote… but that is not the case in this course. Since many of you have zero to no experience with GitHub, we’ll avoid pulling and merging on your side for now. Unless you’re confident with GitHub, focus on working on your local computer, pushing to the website and getting comfortable with those instructions, before moving on to other tasks."
  },
  {
    "objectID": "quarto/crossref.html",
    "href": "quarto/crossref.html",
    "title": "Cross-references",
    "section": "",
    "text": "The goal of this assignment is to practice creating tables and plots in Quarto and including cross-references. Cross-references are important in scholarly writing, as they allow you to refer to elements (figures, tables and sections) that are in a different location in your document. In Quarto outputs, they also become hyperlinks the location: by clicking on the reference, you are taken to the appropriate figure/table/section.\nFor this assignment you can keep working on the same file from the First assignment."
  },
  {
    "objectID": "quarto/crossref.html#invisible-chunks",
    "href": "quarto/crossref.html#invisible-chunks",
    "title": "Cross-references",
    "section": "\n2.1 Invisible chunks",
    "text": "2.1 Invisible chunks\nYou may add the include: false option to your setup chunk to hide both the code and the result from your final output. For example, I would start my reports with the following chunk:\n#| label: setup\n#| include: false\nlibrary(tidyverse)\nlibrary(mclm) # not necessary for THIS notebook, but usually yes\nlibrary(kableExtra)\ndataset <- tibble(\n  name = c(\"Fulano\", \"Mengano\", \"John Doe\", \"Someone\"),\n  age = c(35, 23, 56, 12),\n  ear_type = c(\"Pointy\", \"Rounded\", \"Rounded\", \"Pointy\")\n)"
  },
  {
    "objectID": "quarto/crossref.html#chunks-with-output",
    "href": "quarto/crossref.html#chunks-with-output",
    "title": "Cross-references",
    "section": "\n2.2 Chunks with output",
    "text": "2.2 Chunks with output\nFor tables and figures, you can either copy code from the slides from previous lessons or do something from scratch as shown below. It doesn’t have to be serious, it just has to be done properly.\nYou can either add echo: false to your chunks (which will not be shown in the examples below) or add the following lines to the metadata YAML at the top of your .qmd file to make it the default in the whole document:\nexecute:\n  echo: false\n\n\nTable example\nFigure example\nSection example\n\n\n\nTable 1 is referenced with @tbl-tblex in the text. Remember that the label of the figure must start with “tbl”, the caption must be set with “tbl-cap” and the caption location may be set with “tbl-cap-location”. A margin caption will only be shown in the margins in HTML output if the viewport is wide enough (for responsiveness). By default, the margin location of a table is at the top.\n\n```{r}\n#| label: tbl-tblex\n#| tbl-cap: Age and ear shape of people invented for this exercise.\n#| tbl-cap-location: margin\nkbl(dataset, col.names = c(\"Name\", \"Age\", \"Ear shape\")) %>% \n  kable_paper()\n```\n\n\n\n\n\n \n Name \n    Age \n    Ear shape \n  \n\n\n Fulano \n    35 \n    Pointy \n  \n\n Mengano \n    23 \n    Rounded \n  \n\n John Doe \n    56 \n    Rounded \n  \n\n Someone \n    12 \n    Pointy \n  \n\n\nTable 1:  Age and ear shape of people invented for this exercise. \n\n\n\n\nFigure 1 is referenced with @fig-figex in the text. Remember that the label of the figure must start with “fig”, the caption must be set with “fig-cap” and the caption location may be set with “fig-cap-location”. By default the caption location of a figure is at the bottom.\n\n```{r}\n#| label: fig-figex\n#| fig-cap: A lollipop chart showing the ages and ear shapes of people invented for this exercise.\nggplot(dataset, aes(x = age, y = name)) +\n  geom_segment(aes(xend = 0, yend = name), color = \"gray\") +\n  geom_point(aes(color = ear_type), size = 3) +\n  labs(x = \"Age\", y = \"\", color = \"Ear shape\") +\n  theme_classic(base_size = 15) +\n  theme(legend.position = \"top\")\n```\n\n\n\nFigure 1: A lollipop chart showing the ages and ear shapes of people invented for this exercise.\n\n\n\n\n\n\n\n\n\n\n{ggplot2} syntax\n\n\n\n\n\n{ggplot2} uses what is called a Grammar of Graphics: layers of instructions on each other. In Figure 1, this is what each line does:\n\nggplot(dataset, aes(x = age, y = name))\n\nCreates a ggplot object, which is plotted when printing. It assigns it a dataset to read data from and aesthethics. Aesthetics are mappings between columns of the dataset (variables) and markers or characteristics of the plot.\n\n\nThe aesthetics we are assigning are x and y, i.e. the horizontal and vertical axis respectively. The data in the age column of dataset will be mapped to the horizontal axis, while the data in the name column of the dataset (even if it’s categorical) will be mapped to the vertical axis.\n\n\nAesthetics can be assigned either in this stage or in a geom_ layer. Aesthetics assigned inside the ggplot() call will be used by all the following geom_ layers unless overriden.\n\ngeom_segment(aes(xend = 0, yend = name), color = \"gray\")\n\nAdd a segment geometry, i.e. one line per observation. The aesthetics needed to draw a line are x and xend (the horizontal coordinates for each end of the line) and y and yend (the vertical coordinates for each end of the line). In other words, we will draw a line between (x, y) and (xend, yend).\n\n\nFor these segments, we will use the same x and y values assigned in the ggplot() call, which are inherited automatically. But we will also need to add the values for xend and yend. xend is a constant, 0: we want all the segments to start from \\(x=0\\) and end at the value of age. yend is equal to y: we want horizontal lines that have the same value at y and yend, i.e. the value of name.\n\n\nOutside the mapping argument with the aes() call, we can also assign other constants, such as the same color for all elements. These are independent of the dataset. (xend could also have been defined here.) In this case we indicate that the segments must be gray.\n\ngeom_point(aes(color = ear_type), size = 3)\n\nAdd a point geometry, i.e. one point per observation. We inherit the aesthetics from ggplot() but not those of geom_segment() (which would be meaningless anyways).\n\n\ncolor is defined within the aes() call because we want to map the value of ear_type to the colors. size is defined outside, as a constant for all points.\n\n\nIf geom_segment() was called after geom_point(), they would be drawn on top of the points and seen in the overlapping bit.\n\nlabs(x = \"Age\", y = \"\", color = \"Ear shape\")\n\nSet the label of the x axis to “Age”, remove the label of the y axis, and set the title of the color legend to “Ear shape”.\n\ntheme_classic(base_size = 15)\n\nSet a bunch of default properties for a nice theme. We also ask for the base font size to be of 15 points.\n\ntheme(legend.position = \"top\")\n\nFurther refine the theme by stating that the legend should be on top of the plot.\n\n\nTo better understand the effect of each later, I invite you to run the code adding one line at a time and see how the plot changes.\n\n\n\n\n\nSection 3 is referenced as @sec-git. In order to be able to reference it, you should add {#sec-git} after the heading that needs referencing. Remember also to add the following number-sections: true to your metadata YAML."
  },
  {
    "objectID": "quarto/markdown.html",
    "href": "quarto/markdown.html",
    "title": "Markdown and inline code",
    "section": "",
    "text": "The goal of this assignment is to practice markdown formatting and inline code in Quarto."
  },
  {
    "objectID": "quarto/markdown.html#sec-data",
    "href": "quarto/markdown.html#sec-data",
    "title": "Markdown and inline code",
    "section": "\n2.1 Data",
    "text": "2.1 Data\nYou will start your Quarto file with a hidden chunk (include: false) in which you load {tidyverse} and {kableExtra} and open the “gonna-going to” file.\n\n```{r}\n#| label: setup\n#| message: false\nlibrary(tidyverse)\nlibrary(kableExtra)\noptions(digits = 3) # to print up to 3 digits\nurl <- \"https://raw.githubusercontent.com/mclm2022/mclm2022.github.io/main/slides/gonna_goingto.tsv\"\ngt <- read_tsv(url, show_col_types = FALSE) %>% \n  mutate(variant = fct_relevel(variant, \"gonna\"), register = fct_relevel(register, \"informal\"))\n```\n\nThe output of gt should look something like this (but don’t print it in Quarto!):\n\ngt\n\n# A tibble: 100 × 7\n   variant comp_length register variant_num source    fit1    fit2\n   <fct>         <dbl> <fct>          <dbl> <chr>    <dbl>   <dbl>\n 1 gonna          6.72 formal             0 q      0.00791 0.0496 \n 2 gonna          5.06 informal           0 x      0.348   0.137  \n 3 gonna          5.28 informal           0 z      0.233   0.0814 \n 4 gonna          4.84 informal           0 i      0.480   0.221  \n 5 gonna          5.74 informal           0 m      0.0881  0.0260 \n 6 gonna          3.68 informal           0 f      0.946   0.861  \n 7 gonna          5.15 informal           0 j      0.297   0.111  \n 8 gonna          6.22 informal           0 p      0.0274  0.00727\n 9 gonna          6.60 informal           0 p      0.0107  0.00269\n10 gonna          6.19 formal             0 c      0.0294  0.174  \n# … with 90 more rows\n\n\nThe columns fit1 and fit2 show the predicted values of the logistic regression model with only comp_length and with comp_length and register as predictors, respectively. You can use this data to obtain the Concordance Index “C” (notice you will need to install the {Hmisc} package):\n\n# install.packages(\"Hmisc\") # if not installed yet\nHmisc::somers2(gt$fit2, gt$variant_num)[[\"C\"]]\n\n[1] 0.984\n\n\nOther than describing the dataset itself, you will summarize the data in two ways: a contingency table between the variant and register columns and a boxplot that combines variant, register and comp_length. The code for both is given below. Feel free to edit them to fit your aesthetic preferences, if you want. In both cases, you may use echo: false to hide the code.\n\n\nTable\nPlot\n\n\n\n\n```{r}\n#| label: tbl-contingency\n#| tbl-cap: Contingency table between `variant` and `register` variables.\ncont_table <- table(gt$variant, gt$register)\ncont_table %>% kbl() %>% \n  kable_paper()\n```\n\n\n\n\nTable 1:  Contingency table between variant and register variables. \n \n   \n    informal \n    formal \n  \n\n\n gonna \n    43 \n    7 \n  \n\n going_to \n    15 \n    35 \n  \n\n\n\n\n\n\n\n\n```{r}\n#| label: fig-boxplot\n#| fig-cap: Distribution of complement length across variants, distinguished by register.\ngt %>% ggplot(aes(x = variant, y = comp_length, fill = register)) +\n  geom_boxplot() +\n  scale_fill_manual(values = c(\"coral\", \"darkolivegreen3\")) +\n  labs(x = \"Variant\", y = \"Complement length\", fill = \"Register\") +\n  theme_minimal(base_size = 15) +\n  theme(legend.position = \"top\")\n```\n\n\n\nFigure 1: Distribution of complement length across variants, distinguished by register."
  },
  {
    "objectID": "quarto/markdown.html#sec-task",
    "href": "quarto/markdown.html#sec-task",
    "title": "Markdown and inline code",
    "section": "\n2.2 Tasks",
    "text": "2.2 Tasks\nThe main task is to describe the dataset (particularly the table and the plot) in your own words. More concretely,\n\nUse verbatim to refer to variables (or column names), e.g. “Table 1 cross-tabulates the variant and register variables”.\nUse italics to refer to values of variables, e.g. “The instances of gonna are clearly biased towards the informal register”.\nUse bold to highlight concepts you will define (you may choose which), e.g. “Figure 1 is a boxplot, which represents the distribution of a numerical variable…”.\n\nUse bullet points to list the names of your columns like so:\n\nvariant, with values gonna and going_to, indicating the choice between one expression or the other;\ncomp_length, or complement length, with a minimum value of 1…\n\n\n\nWhen possible, use inline code to render values of your variables into your text. This will avoid typos and, if your dataset changes, the printed output will adapt. See Section 2.3.2 for examples."
  },
  {
    "objectID": "quarto/markdown.html#sec-tips",
    "href": "quarto/markdown.html#sec-tips",
    "title": "Markdown and inline code",
    "section": "\n2.3 Tips",
    "text": "2.3 Tips\n\n2.3.1 Markdown\nSee documentation.\n\n\n`verbatim`\n\n*italics* or _italics_\n\n**bold** or __bold__\n\n- Bullet one\n\n- Bullet two\n\n    + Sub bullet point\n    \n- Bullet three\n\n\n\nverbatim\nitalics or italics\nbold or bold\n\nBullet one\n\nBullet two\n\nSub bullet point\n\n\nBullet three\n\n\n\n\n2.3.2 Inline code\nWhile verbatim is rendered by surrounding text with backticks, R output can be included inline by preceding the text with “r”. Here are some examples of how you could use it in the description of your dataset.\nThe dataset has `r nrow(gt)` observations and `r length(gt)` variables.\nThe dataset has 100 observations and 7 variables.\nThere are `r cont_table[\"gonna\", \"informal\"]` instances of *gonna* in the *informal*\n`register`.\nThere are 43 instances of gonna in the informal register.\nThe values of `comp_length`, which represents the **complement length**,\nspan from `r min(gt$comp_length)` to `r max(gt$comp_length)`\nwith a median of `r median(comp_length)`.\nThe values of comp_length, which represents the complement length, spans from 1 to 6.969 with a median of 4.85.\nThe values of `variant` are *`r levels(gt$variant)[[1]]`* and\n*`r levels(gt$variant)[[2]]`*.\nThe values of variant are gonna and going_to.\nThe Concordance index C for the logistic regression model with `variant` as outcome\nand `comp_length` and `register` as main effects\nis `r Hmisc::somers2(gt$fit2, gt$variant_num)[[\"C\"]]`.\nThe Concordance index C for the logistic regression model with variant as outcome and comp_length and register as main effects is 0.984."
  },
  {
    "objectID": "tidyverse/cheatsheet.html",
    "href": "tidyverse/cheatsheet.html",
    "title": "Tidyverse cheatsheet",
    "section": "",
    "text": "Reference cheatsheet for some useful tidyverse functions related to dataframe manipulation.\nTidyverse is a collection of R packages for data wrangling and visualization (among other things). A great resource to learn how to use it is R for data Science."
  },
  {
    "objectID": "tidyverse/cheatsheet.html#numeric-filter",
    "href": "tidyverse/cheatsheet.html#numeric-filter",
    "title": "Tidyverse cheatsheet",
    "section": "Numeric filter",
    "text": "Numeric filter\n\nmy_tibble %>% filter(first_column < 3)\n\n# A tibble: 2 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            1 a             TRUE        \n2            2 b             FALSE       \n\nmy_tibble %>% filter(first_column == 2)\n\n# A tibble: 1 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            2 b             FALSE       \n\nmy_tibble %>% filter(first_column %in% c(1, 3))\n\n# A tibble: 2 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            1 a             TRUE        \n2            3 a             FALSE"
  },
  {
    "objectID": "tidyverse/cheatsheet.html#filtering-with-character-strings",
    "href": "tidyverse/cheatsheet.html#filtering-with-character-strings",
    "title": "Tidyverse cheatsheet",
    "section": "Filtering with character strings",
    "text": "Filtering with character strings\n\nmy_tibble %>% filter(second_column == \"a\")\n\n# A tibble: 2 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            1 a             TRUE        \n2            3 a             FALSE       \n\nmy_tibble %>% filter(startsWith(second_column, \"b\"))\n\n# A tibble: 2 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            2 b             FALSE       \n2            4 be            TRUE        \n\nmy_tibble %>% filter(second_column %in% c(\"a\", \"b\"))\n\n# A tibble: 3 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            1 a             TRUE        \n2            2 b             FALSE       \n3            3 a             FALSE"
  },
  {
    "objectID": "tidyverse/cheatsheet.html#filtering-with-boolean-columns",
    "href": "tidyverse/cheatsheet.html#filtering-with-boolean-columns",
    "title": "Tidyverse cheatsheet",
    "section": "Filtering with boolean columns",
    "text": "Filtering with boolean columns\n\nmy_tibble %>% filter(third_column)\n\n# A tibble: 2 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            1 a             TRUE        \n2            4 be            TRUE        \n\nmy_tibble %>% filter(!third_column)\n\n# A tibble: 2 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            2 b             FALSE       \n2            3 a             FALSE"
  },
  {
    "objectID": "tidyverse/cheatsheet.html#combinations",
    "href": "tidyverse/cheatsheet.html#combinations",
    "title": "Tidyverse cheatsheet",
    "section": "Combinations",
    "text": "Combinations\nThe comma and & combine the filters, whereas | returns rows that are TRUE for at least one of the conditions.\n\nmy_tibble %>% filter(first_column > 2, second_column == \"a\")\n\n# A tibble: 1 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            3 a             FALSE       \n\nmy_tibble %>% filter(third_column | startsWith(second_column, \"b\"))\n\n# A tibble: 3 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            1 a             TRUE        \n2            2 b             FALSE       \n3            4 be            TRUE        \n\nmy_tibble %>% filter(first_column > 1 & second_column != \"a\")\n\n# A tibble: 2 × 3\n  first_column second_column third_column\n         <dbl> <chr>         <lgl>       \n1            2 b             FALSE       \n2            4 be            TRUE"
  },
  {
    "objectID": "tidyverse/cheatsheet.html#by-number",
    "href": "tidyverse/cheatsheet.html#by-number",
    "title": "Tidyverse cheatsheet",
    "section": "By number",
    "text": "By number\nThe slice_ functions let you select rows based on position."
  },
  {
    "objectID": "tidyverse/ggplot.html",
    "href": "tidyverse/ggplot.html",
    "title": "The Grammar of Graphics",
    "section": "",
    "text": "The goal of this assignment is to gain some familiarity with the Grammar of Graphics, i.e. the principles behind {ggplot2} syntax. Hadley Wickham’s article is a good start if you’re interested in that philosophy, and you can also check the ggplot2 book.\nIt is not necessary to know all the functions in order to create graphics with {ggplot2} —I think the most important points are to understand the logic of the layers and to learn how to look up what you need. It’s ok if you need to google how to draw a barplot, how to change the position of the legend, or what the available colors and color palettes are… but you have to get comfortable looking things up, learning about package extensions and combining the functions into the plot you are thinking of."
  },
  {
    "objectID": "tidyverse/ggplot.html#code-to-copy",
    "href": "tidyverse/ggplot.html#code-to-copy",
    "title": "The Grammar of Graphics",
    "section": "\n1.1 Code to copy",
    "text": "1.1 Code to copy\n\n\nSetup\nPlot\n\n\n\nCopy the following code as is in an invisible chunk in your Quarto file (use include: false in the options).\n\nlibrary(tidyverse)\nset.seed(2022)\nrows <- 20\ncake_eaten <- tibble(\n  hunger_level = sample(1:100, rows, replace = TRUE),\n  cake_eaten = sample(1:100, rows, replace = TRUE)/100,\n  cake_flavor = sample(rep_len(c(\"vanilla\", \"chocolate\"), rows))\n  )\n\n\n\n\n\n\n\nTip\n\n\n\nIf you want, you can also write a small section in your file in which you print the table and/or describe what it represents! (If you don’t remember what I said you can make it up, as long as it kind of makes sense.)\nDescribing the table can be a good opportunity to practice inline code, e.g. `r nrow(cake_eaten)` to print the number of rows in the dataframe, or using other functions such as length(cake_eaten) to get the number of columns.\n\n\n\n\nThis is the final code for the plot you have to build, and the final output should look like the plot under the code. In total you should have at least 10 different plots: one for each of the layers!\n\nggplot(cake_eaten, aes(x = cake_eaten, y = hunger_level)) +\n  geom_point(aes(color = cake_flavor), size = 3) +\n  geom_hline(yintercept = 80, linetype = 2) +\n  geom_line(aes(group = cake_flavor), color = \"gray40\", linetype = 4) +\n  scale_color_manual(values = c(\"coral3\", \"darkcyan\")) +\n  scale_x_continuous(labels = scales::label_percent()) +\n  labs(x = \"Percentage of cake eaten\", y = \"Hunger level\", color = \"Cake flavor\") +\n  annotate(\"text\", x = 0, y = 80, label = \"Would eat anything\",\n           vjust = -0.6, hjust = 0, family = \"mono\") +\n  theme_light(base_size = 20) +\n  theme(legend.position = \"top\", text = element_text(family = \"serif\"))\n\n\n\n\n\n\n\n\n\n\nOrder of the layers\n\n\n\nThe order of the layers is not crucial to the output. If the geom_line() layers goes before the geom_point() layer, it will be drawn before it (and therefore under it), but otherwise, it doesn’t change much in which order you do them. That said, it can be useful to respect a certain logical order –it’s just not necessary and sometimes it doesn’t make a difference. It won’t throw an error.\nFor the assignment, please keep the order, but you’re welcome to explore and experiment what happens if you change it."
  },
  {
    "objectID": "tidyverse/tibbles.html",
    "href": "tidyverse/tibbles.html",
    "title": "Tibble manipulation",
    "section": "",
    "text": "The goal of this assignment is to practice working with tables using tidyverse. Tidyverse is a collection of R packages for data wrangling and visualization (among other things). A great resource to learn how to use it is R for data Science.\nFor this assignment I ask that you create both a script to manipulate the table and then a Quarto file to print and cross-reference the table (combining what you learned in the second assignment)."
  },
  {
    "objectID": "tidyverse/tibbles.html#manipulating-the-table",
    "href": "tidyverse/tibbles.html#manipulating-the-table",
    "title": "Tibble manipulation",
    "section": "\n2.1 Manipulating the table",
    "text": "2.1 Manipulating the table\n\nUse mutate() to change the values of a column.\n\nUse filter() to subset the rows based on values in the columns. You can also use the slice_ family of functions to subset with other criteria:\n\nslice_head(n = 3) to select the first three rows; slice_tail(n = 5) to select the last five rows.\nslice_sample(n = 10) to select ten random rows, slice_sample(prop = 0.5) to select a random 50% of the rows.\n\n\nUse select() to subset the columns. You can also use rename() to rename columns without removing the rest.\nUse arrange() to sort the tibble based on the values in a column.\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nAn example is the code below, which starts with an assoc object (product of assoc_scores()) and ends with a tibble with a selection of rows and columns.\n\nsubsetted <- hot_assoc %>% \n  as_tibble() %>% \n  filter(PMI > 1, G_signed >= 5, endsWith(type, \"nn\")) %>% \n  select(type, freq = a, PMI, ends_with(\"signed\"), OR) %>% \n  mutate(\n    log_OR = log(OR),\n    type = str_remove(type, \"/nn\")\n  )\n\n\nIn line 2 we use as_tibble() to turn the assoc object into a tibble to manipulate with {tidyverse} functions.\nIn line 3 we use filter() to subset the rows that have PMI larger than 1, G_signed larger than or equal to 5, and a type ending with “nn”, i.e. nouns.\nIn line 4 we use select() to subset the columns type, a, PMI and OR as well as those ending in “signed” and at the same time rename a to “freq”.\nIn lines 5 through 8 we use mutate() to create a new column log_OR that contains the logarithm of the OR column, and we modify the type column to remove the “/nn” ending from its elements.\nIn line 1 we assign the whole operation, initially applied to hot_assoc, to a variable called subsetted.\n\nEach operation of the pipe acts on the output of the operation before it."
  },
  {
    "objectID": "tidyverse/tibbles.html#association-scores",
    "href": "tidyverse/tibbles.html#association-scores",
    "title": "Tibble manipulation",
    "section": "\n2.2 Association scores",
    "text": "2.2 Association scores\n\nUse assoc_scores() after surf_cooc() to create an association scores object.\nUse write_assoc(scores_object, filename) to save the object from your R script.\nUse scores_object <- read_assoc(filename) to read the object in the Quarto file."
  },
  {
    "objectID": "tidyverse/tibbles.html#kableextra",
    "href": "tidyverse/tibbles.html#kableextra",
    "title": "Tibble manipulation",
    "section": "\n2.3 KableExtra",
    "text": "2.3 KableExtra\nCheck out the documentation for HTML or PDF output to learn about {kableExtra} features."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site gathers slides and case studies for the Master in Corpus Linguistics course at KU Leuven, Fall of 2022 (taught by Mariana Montes).\n\nSchedule\n\n\n\nClass number\nDate\nTopic\nMaterial in website\nAssignment\n\n\n\n\n1\n29/09\nIntroduction\nSlides\nInstall software and packages\n\n\n2\n06/10\nReading corpora and using Git\nSlides\nCorpus description\n\n\n3\n13/10\nContingency tables\nSlides\nCross-references\n\n\n4\n20/10\nAssociation measures\nSlides\nTibble manipulation\n\n\n5\n27/10\nLinear regression\nSlides\nThe Grammar of Graphics\n\n\n6\n03/11\nLogistic regression\nSlides\nMarkdown\n\n\n7\n10/11\nCorrespondence analysis\nSlides\nMCLM Tutorials\n\n\n8\n17/11\nFactor analysis\nSlides\nPaper proposal\n\n\n9\n24/11\nCollocations and keywords: example\nStudy\nCitations\n\n\n10\n01/12\nRetrieval and analysis of variants in alternation\nStudy\nInterlinear glosses\n\n\n11\n08/12\nCase study with logistic regression\nTBD\nTBD\n\n\n12\n15/12\nLectometry example\nStudy\nTBD\n\n\n13\n22/12\nRegister analysis\nStudy\nTBD"
  }
]