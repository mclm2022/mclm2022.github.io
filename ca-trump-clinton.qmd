---
title: "CA applied to the Trump-Clinton speeches"
---

# Setup

```{r}
#| include: false

corpus_folder <- "_corpora"
```

This document illustrates how to use *correspondence analysis* (CA) to investigate the degree to which the Trump speeches and the Clinton speeches in the `clinton_trump` corpus are different from, or similar to, one another. The approach boils down to adopting a bird's-eye perspective, looking at the broad picture we get if we aggregate over a wide range of **features** of the speeches. More specifically, we compare the frequencies of a wide range of features in the documents. A major advantage of the technique is that it allows us to see how similar or different the documents are when taking into account the full range of features, but at the same time to also see how similarly or differently the features are distributed over the documents, and, importantly, which features or groups of features are the ones that account most for the most important dissimilarities between the documents. 

## Activate packages

We start by activating the R packages we will use. Note that activating `{mclm}` automatically also activates the package `{ca}` which it relies on. This is the reason why we do not have to explicitly activate `{ca}`, even though we will conduct *correspondence analysis* using the function `ca::ca()`. We'll also load `{kableExtra}` to create some tables.

```{r}
#| label: loadpkg
#| message: false

library(tidyverse)
library(mclm)
library(kableExtra)
```

## Collecte the corpus filenames

First, we collect the names of the corpus files and store them in an object called `fnames`. From this we also derive the short filenames (stored in `short_fnames`) and a character vector with, for each filename, the subcorpus (stored in `sub_corp`).

```{r}
# retrieve list of corpus file names
fnames <- get_fnames(file.path(corpus_folder, "clinton_trump")) %>%
  print()
short_fnames <- short_names(fnames) %>%
  print()
sub_corp <- fnames %>%
  re_retrieve_first("/clinton_trump/([^/]+)", requested_group = 1) %>%
  print()
```


# A first CA: based on function words

In a first approach, we use so-called *function words* as features (i.e. closed word classes such as pronouns, prepositions, auxiliaries, ..., as opposed to so-called *content words* such as nouns, verbs, adjectives, ...). In the field of *stylometry*, which is devoted to the quantitative study of linguistics style (e.g. in support of authorship attribution), it has long been established that function words are a powerful instrument in support of the identification of typical or even unique characteristics of the style of a specific author (or group of authors). Zooming in on function words steers the analysis away from differences related to topic/content and foregrounds differences related to style. Using function words by no means is the only way to do this (alternatives are (i) zooming in on longer n-grams, e.g. 3-grams, 4-grams, or, (ii) if the information is available, looking at the frequencies of POS tags or POS tag n-grams, or, (iii) looking at derived features such as lexical density, word length, sentence length, etc.).

## Specifying the features

We start by reading the names of the features for this analysis from the file *function-words.txt* and storing then into an object called `features`.

```{r}
features <- read_types("assets/ca-trump-clinton/function-words.txt") %>%
  print()
```

The object `features` now contains about `r n_types(features)` function words.

## Building the file-by-feature matrix

Now, we are ready to build the matrix that we will apply the correspondence analysis to. Eventually, this matrix will be of the data type *matrix* with as its rows the files and as its columns the features, and in the cells the absolute frequencies of the features in the files.

A convenient way in R to incrementally build that data set, file by file, is to first store the data in a data frame with as its rows the features and as its columns the files. This is convenient, because we will 'grow' the data set file by file, and adding new columns to a data frame is a straigthforward and (in most situations sufficiently) time- and memory-efficient procedure in R. So that's how we will go about. Once we have all the data, we will, as a final step, turn the data frame into the data type *matrix* and then transpose the matrix in order to make the files the rows and the features the columns.

```{r}
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]             # identify i-th filename
  short_fname <- short_fnames[[i]] # identify i-th short filename
  flist <- freqlist(fname)         # build frequency list for file
  flist <- flist[features]         # filter that list to just features
  d[[short_fname]] <- flist        # add column to d named after filename
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()
```

::: {.column-margin}

The top-left part of the resulting matrix (first ten rows, first ten columns) looks like this:

```{r}
kbl(d[1:10, 1:10])
```

:::

## Running correspondence analysis

Then we run the actual correspondence analysis and store the result in an object called `d_ca`.
For reasons of brevity, we will skip the inspection of the summary report of the analysis, which we would normally do with `summary(d_ca)`.
  
```{r}
d_ca <- ca(d)
# summary(d_ca)
```

## Biplot

Next, we build the biplot, with color indication the subcorpus of each file. First, we prepare the data needed for the plot:

```{r}
text_coord <- row_pcoord(d_ca)                # coordinates of texts
word_coord <- col_pcoord(d_ca)                # coordinates of function words
words_df <- tibble(
  word = colnames(d),
  x = word_coord[, 1],
  y = word_coord[, 2])
texts_df <- tibble(
  text = short_fnames,
  sub_corp = sub_corp,
  x = text_coord[, 1],
  y = text_coord[, 2])

```

Then we build the plot:

```{r}
ggplot(words_df, aes(x = x, y = y)) +
  geom_text(aes(label = word), col = "gray") +
  geom_point(data = texts_df, aes(x = x, y = y, col = sub_corp))
```

## Typical Clinton features vs. typical Trump features

Since the dimension that sets apart the Clinton speeches (top) most clearly from the Trump speeches (bottom) is the second dimension (y-axis), we may want to inspect the features that occupy extreme positions on the y-axis (and that hence contribute importantly to the y-axis). 

```{r}
# Top of y-axis
words_df %>%
  arrange(desc(y)) %>%
  head(20) %>%
  kbl() %>% 
  kable_paper()

# Bottom of y-axis
words_df %>%
  arrange(y) %>%
  head(20) %>%
  kbl() %>% 
  kable_paper()
```

# A second CA: based on high frequency content words

In a second approach, we look at *content words* instead. Obviously, then, topic will start playing an important role (although some stylistic differences may still be present in the patterns that emerge). So the nature of this analysis is very different, and it serves different purposes. Even so, it is a legitimate question whether the Clinton speeches and the Trump speeches turn out to be as clearly separated as in the previous analysis if we zoom in on the content words. Also, we can inspect which (groups of) content words will emerge as the ones that are most characteristic of the different areas in the map that will emerge.

## Specifying the features

Technically, the only difference, compared to the previous analysis, is how we define the features. This time, we build a complete frequency list of the whole corpus, we then remove a number of stop words (mostly function words, but also some undesired types such as `000` and `--`), and finally we treat the top 150 (i.e. the 150 highest frequency items) of the remaining items as our features. The number 150 is an arbitrary choice. We chose it so we could make a fair comparison to the performance of the analysis that used function words.

```{r}
stop_list <- read_types("assets/ca-trump-clinton/stop_list.txt") %>%
  print()

features <- freqlist(fnames) %>%
  drop_types(stop_list) %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types() %>%
  print()
```

## Next steps and plot

The next steps, all the way up to the creation of the plot, are completely analogous to the previous analysis.

```{r}
# build file by feature frequency matrix
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]
  short_fname <- short_fnames[[i]]
  flist <- freqlist(fname)
  flist <- flist[features]
  d[[short_fname]] <- flist
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()

# conduct CA
d_ca <- ca(d)
# summary(d_ca)

# preparations for biplot
text_coord <- row_pcoord(d_ca)  # coordinates of texts
word_coord <- col_pcoord(d_ca)  # coordinates of function words
words_df <- tibble(
  word = colnames(d),
  x = word_coord[, 1],
  y = word_coord[, 2])
texts_df <- tibble(
  text = short_fnames,
  sub_corp = sub_corp,
  x = text_coord[, 1],
  y = text_coord[, 2])

# biplot, color coding sub_corp
ggplot(data = words_df, aes(x = x, y = y)) +
  geom_text(aes(label = word), col = "gray") +
  geom_point(data = texts_df, aes(x = x, y = y, col = sub_corp))
```

This time it is the combination of the two dimensions that sets apart the two groups of speeches. This makes it a bit more difficult to select the items on the basis of their co-ordinates. We will do so in a minute, but first, we use a more visual approach. We will demonstrate how we can zoom in on a specific area, if we want to be able to better read the words in that area. This is demonstrated here for the region of x-values ranging from `0` to `0.5` and y-values ranging from `-0.5` to `0`:

```{r}
ggplot(data = words_df, aes(x = x, y = y)) +
  geom_text(aes(label = word), col = "gray") +
  geom_point(data = texts_df, aes(x = x, y = y, col = sub_corp)) +
  coord_cartesian(xlim = c(0, 0.5), ylim = c(-0.5, 0))
```

Identifying the same items by co-ordinates can be done as follows:

```{r}
words_df %>%
  filter(x >= 0 & x <= 0.5 & y >= -0.5 & y <= 0) %>%
  kbl() %>% 
  kable_paper()
```

# A third CA: based on high frequency bigrams

In our third approach, we work with bigrams. Our features are the top 150 items from the frequency list of bigrams. Notice in the code snippet below how we use the argument `ngram_size = 2` to make sure that `freqlist()` builds a frequency list of bigrams. Notice also that prior to identifying the top 150, we remove all bigrams with either `applause`, `cheering`, or `--` in them. We do this, because in this corpus those items (almost always) are part of the annotation instead of being real tokens.  

```{r}
features <- fnames %>%
  freqlist(ngram_size = 2) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>% # or keep_pos(1:150)
  as_types() %>%
  print()
```

The rest of the steps again is identical to what it was in the previous analysis, except that when we build frequency lists for each file, we must make sure that here too we build a frequency list of bigram (so again you see the argument `ngram_size = 2` popping up). Note that the step `drop_re("(applause|cheering|--)")` is not necessary here, because bigrams containing `applause`, `cheering`, or `--` are not part of the feature list anyway.

```{r}
# build file by feature frequency matrix
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]
  short_fname <- short_fnames[[i]]
  flist <- freqlist(fname, ngram_size = 2)
  flist <- flist[features]
  d[[short_fname]] <- flist
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()

# conduct CA
d_ca <- ca(d)
# summary(d_ca)

# preparations for biplot
text_coord <- row_pcoord(d_ca)  # coordinates of texts
word_coord <- col_pcoord(d_ca)  # coordinates of function words
words_df <- tibble(
  word = colnames(d),
  x = word_coord[,1],
  y = word_coord[,2])
texts_df <- tibble(
  text = short_fnames,
  sub_corp = sub_corp,
  x = text_coord[,1],
  y = text_coord[,2])

# biplot, color coding sub_corp
ggplot(data = words_df, aes(x = x, y = y)) +
  geom_text(aes(label = word), col = "gray") +
  geom_point(data = texts_df, aes(x = x, y = y, col = sub_corp))
```

Let's inspect the most extreme items on the x-axis:

```{r}
# So what are the 20 left-most words/items?
words_df %>%
  arrange(x) %>%
  head(20) %>%
  kbl() %>% 
  kable_paper()

# And the 20 right-most words/items?
words_df %>%
  arrange(desc(x)) %>%
  head(20) %>%
  kbl() %>% 
  kable_paper()
```

# A fourth CA: based on high frequency trigrams

In our fourth approach, we work with trigrams. Our features are the top 150 items from the frequency list of trigrams. Notice the argument `ngram_size = 3`.  

```{r}
features <- fnames %>%
  freqlist(ngram_size = 3) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types() %>%
  print()
```

The rest of the steps again is identical to what it was in the previous analysis, except for the `ngram_size = 3`.

```{r}
# build file by feature frequency matrix
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]
  short_fname <- short_fnames[[i]]
  flist <- freqlist(fname, ngram_size = 3)
  flist <- flist[features]
  d[[short_fname]] <- flist
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()

# conduct CA
d_ca <- ca(d)
# summary(d_ca)

# preparations for biplot
text_coord <- row_pcoord(d_ca)  # coordinates of texts
word_coord <- col_pcoord(d_ca)  # coordinates of function words
words_df <- tibble(
  word = colnames(d),
  x = word_coord[,1],
  y = word_coord[,2])
texts_df <- tibble(
  text = short_fnames,
  sub_corp = sub_corp,
  x = text_coord[,1],
  y = text_coord[,2])

# biplot, color coding sub_corp
ggplot(data = words_df, aes(x = x, y = y)) +
  geom_text(aes(label = word), col = "gray") +
  geom_point(data = texts_df, aes(x = x, y = y, col = sub_corp))
```

Let's inspect the most extreme items on the x-axis:

```{r}
# So what are the 20 left-most words/items?
words_df %>%
  arrange(x) %>%
  head(20) %>%
  kbl() %>% 
  kable_paper()

# And the 20 right-most words/items?
words_df %>%
  arrange(desc(x)) %>%
  head(20) %>%
  kbl() %>% 
  kable_paper()
```

