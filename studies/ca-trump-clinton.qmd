---
title: "CA applied to the Trump-Clinton speeches"
number-sections: true
---

This document illustrates how to use *correspondence analysis* (CA) to investigate the similarities between Clinton speeches and Trump speeches in a corpus that collects them. The approach adopts a bird's-eye perspective, looking at the broad picture we get if we aggregate over a wide range of **features** of the speeches.

A major advantage of the technique is that we can both compare the different documents based on the selected features, but also compare the features based on the documents they occur in and also see which features or groups of features are the ones that account for the most important dissimilarities between the documents.

We will illustrate four analyses with only minimal differences between each other. In all of them, the workflow consists of collecting frequency lists from each of the files in our corpus and create a matrix with one row per document and one column per type that occurs in the corpus. The cells contain the occurrence frequency: a cell $ij$ will contain the occurrence of type $j$ in the document $i$. Such **matrix of counts** will be given to `ca::ca()`, a function that runs correspondence analysis and returns, among other things, the coordinates of row items and of column items in the principal components as well as the variance covered by each dimension. @sec-code will show the full code to obtain the data and plot it before delving into the actual case studies.

The difference between the analyses is the definition of the types collected in the frequency lists. @sec-function will select only function words from a predefined list as features, since they have been shown to reliably discriminate between different authors and registers. @sec-content, on the other hand, will rely on high frequency content words by first excluding the types in a stoplist and then only keeping the top 150 items. Finally, @sec-bigrams and @sec-trigrams will use bigrams and trigrams as types, respectively.

# Setup

```{r}
#| include: false
corpus_folder <- "_corpora"
var_percent <- scales::label_percent(scale = 1, accuracy = 0.01)
```

## Activate packages

We start by activating the R packages we will use, namely `{tidyverse}`, `{mclm}` and, for nice tables when reporting, `{kableExtra}`.

::: callout-note
While we conduct correspondence analysis with `ca::ca()`, we don't need to attach the `{ca}` package separately, since it's already loaded with `{mclm}`.
:::

```{r}
#| label: loadpkg
#| message: false

library(tidyverse)
library(mclm)
library(kableExtra)
```

## Collect the corpus filenames

First, we collect the names of the corpus files and store them in an object called `fnames`. From this we also derive the short filenames (stored in `short_fnames`) and a character vector with, for each filename, the subcorpus (stored in `sub_corp`). The first variable, `short_fnames` is also an `fnames` object and will provide more practical names for plotting. The second variable, `sub_corp` is a character string and will help us assign colors in the plots based on whether the documents correspond to Clinton or Trump speeches. @tbl-fnames illustrates a few of these values.

```{r}
#| label: fnames
fnames <- get_fnames(file.path(corpus_folder, "clinton_trump"))
short_fnames <- short_names(fnames)
sub_corp <- fnames %>%
  re_retrieve_first("/clinton_trump/([^/]+)", requested_group = 1)
```

```{r}
#| label: tbl-fnames
#| tbl-cap: Five random filenames and their corresponding values in short_fnames and sub_corp
#| echo: false
set.seed(8541)
tibble(
  fnames = fnames,
  short_fnames = short_fnames, sub_corp = sub_corp
) %>% 
  slice_sample(n = 5) %>% 
  kbl() %>% 
  kable_paper()
```

## General code {#sec-code}

The code below will be implemented in each of the case studies below with the only difference in the `freqlist()` call in line 5.

::: callout-tip
In practice, if you were to run an analysis where this much code will be used over and over again with multiple modifications, it would be preferable to wrap it in functions. @sec-custom will show you how, if you're interested.
:::

The first part of the code builds the matrix that we will apply the correspondence analysis to. This is an object of the data type `matrix` with as its rows the files and as its columns the features, and in the cells the absolute frequencies of the features in the files.

A convenient way in R to incrementally build that dataset, file by file, is to first store the data in a data frame with as its rows the features and as its columns the files. This is convenient, because we will 'grow' the data set file by file, and adding new columns to a data frame is a straigthforward and (in most situations sufficiently) time- and memory-efficient procedure in R. So that's how we will go about. This is what lines 2-8 do in a loop along the filenames.

Lines 9-12 turn this dataframe into a matrix, transpose it and drop the rows and columns with only zero frequencies.

In line 14, we run correspondence analysis on this matrix, which we store in the variable `d_ca`. We could then inspect it by calling `d_ca` or `summary(d_ca)`.

Then, lines 16-21 extract the coordinates of the rows and columns for plotting and create tibbles with the labels, positions and, in the case of the rows, the color coding based on the subcorpora. `mclm::row_pcoord()` and `mclm::col_pcoord()` extract the coordinates of the rows and columns from the object `d_ca`; we are only interested in the first two columns, which are the coordinates of the two first principal components, therefore we add `[,c(1,2)]`. We then turn the matrix into a tibble assigning the column names "text" and "word" to the rownames, since tibbles don't use rownames. In the case of the documents, we also add the color coding column by creating a column "Subcorpus" and filling it with the vector `sub_corp`.

Lines 23 and 24 use the variance information from `summary(d_ca)` to enrich the labels of the axes in the plot.

The rest of the lines create a biplot.

```{r}
#| label: pre-code
#| eval: false
#| code-line-numbers: true
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]             # identify i-th filename
  short_fname <- short_fnames[[i]] # identify i-th short filename
  flist <- freqlist(fname, ...)    # build frequency list for file
  flist <- flist[features]         # filter that list to just features
  d[[short_fname]] <- flist        # add column to d named after filename
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()

d_ca <- ca(d)

texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")

dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), col = "gray60") +
  geom_point(data = texts_df, aes(x = V1, y = V2, col = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

::: {.callout-note collapse="true"}
### Parsing the `ggplot()` call

What does each line from 26 to 34 do in the code chunk above?

-   Line 26 calls `ggplot()` with the dataframe of columns (the features), assigning the variables "V1" and "V2" to the `x` and `y` coordiantes respectively. These are the automatic names given by `as_tibble()` in line 21 when we converted a matrix without column names.

-   Line 27 plots the items of the variable "word" (the types of our features) as text, in the color "gray60".

-   Line 28 plots the items of the dataframe of rows (documents) as dots, also with "V1" and "V2" in the axes but moreover with "Subcorpus" providing the color aesthetics.

-   Line 29 defines manually the colors to map to the "Subcorpus" variable. We could also use a variety of palettes, different colors, or remove the line and use the default color scheme of `{ggplot2}`.

-   Lines 30 and 31 add a horizontal and vertical line respectively, the former where `y = 0` and the latter where `x = 0`, both in dark gray.

-   Line 32 sets the theme and the base size for the font. This only matters aesthetically and could be made different.

-   Line 33 assign the variables `dim_1` and `dim_2`, defined in lines 23 and 24, as the labels of the `x` and `y` axes.

-   Line 34 indicates that the units of both axes should be the same, which is not the default case for `{ggplot2}` plots. The numbers themselves of the coordinates are not meaningful, but the distances between the points are, and they assume that the distance between the point in $(0,1)$ (where $x = 0$ and $y = 1$) and the centroid $(0, 0)$ is the same as the distance between the point $(1,0)$ and the centroid $(0,0)$. `coord_fixed()` makes sure that's the case.
:::

# Function words as features {#sec-function}

In a first approach, we use so-called *function words* as features.[Function words include closed word classes such as pronouns, prepositions, auxiliaries, etc., as opposed to so-called *content words* such as nouns, verbs, adjectives...]{.aside} In the field of *stylometry*, which is devoted to the quantitative study of linguistics style (e.g. in support of authorship attribution), it has long been established that function words are a powerful instrument in support of the identification of typical or even unique characteristics of the style of a specific author (or group of authors).

::: callout-note
Zooming in on function words steers the analysis away from differences related to topic/content and foregrounds differences related to style. Using function words by no means is the only way to do this (alternatives are (i) zooming in on longer n-grams, e.g. 3-grams, 4-grams, or, (ii) if the information is available, looking at the frequencies of POS tags or POS tag n-grams, or (iii) looking at derived features such as lexical density, word length, sentence length, etc..
:::

## Specifying the features

We start by reading the names of the features for this analysis from the file *function-words.txt* and storing then into an object called `features`.

```{r}
#| label: features-1
features <- read_types("assets/ca-trump-clinton/function-words.txt") %>%
  print(n = 10)
```

As shown when printing it, the object `features` contains `r n_types(features)` function words.

## Building the file-by-feature matrix

Once we have collected our features we can create the matrix with one row per document and one column per feature.

```{r}
#| label: d-1
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]
  short_fname <- short_fnames[[i]]
  flist <- freqlist(fname)
  flist <- flist[features]
  d[[short_fname]] <- flist
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()
```

The top-left part of the resulting matrix (first ten rows, first ten columns) is shown in @tbl-mtx.

```{r}
#| label: tbl-mtx
#| tbl-cap: First rows and columns of a document-by-feature matrix where the rows are Clinton and Trump speeches and the columns are function words.
kbl(d[1:10, 1:10])
```

## Running correspondence analysis

Then we run the actual correspondence analysis and store the result in an object called `d_ca`. For reasons of brevity, we will skip the inspection of the summary report of the analysis, which we would normally do with `summary(d_ca)`.

```{r}
#| label: ca-1
d_ca <- ca(d)
# summary(d_ca)
```

## Biplot

Next, we build the biplot, with color indicating the subcorpus that each file belongs to. First, we prepare the data needed for the plot. If you wanted to inspect the full output of `row_pcoord()` or `col_pcoord()`, you can run them separately first (e.g. `row_pcoord(d_ca) %>% View()`).

@tbl-carows1 shows a random sample from `texts_df`, i.e. the rows of the dataset with their principal component coordinates and the corpus they correspond to. The columns "V1" and "V2" contain the positions in the first and second dimension respectively.

```{r}
#| label: preplot1
#| warning: false
texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)
words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

```{r}
#| label: tbl-carows1
#| tbl-cap: Subset of rows with principal components.
#| echo: false
set.seed(8541)
texts_df %>% 
  head() %>% 
  kbl() %>% 
  kable_paper(full_width = FALSE)
```

Then we build the plot, shown in @fig-caplot1. We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. In addition, both subcorpora form very clear clusters, mostly divided by the second dimension: features higher on the y-axis are more characteristic of Trump's speeches, and those lower on the y-axis are more characteristic of Clinton's.

```{r}
#| label: fig-caplot1
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on function words.
#| fig-alt: Biplot of correspondence analysis of Clinton and Trump speeches based on function words. The dots corresponding to Clinton, in blue, form a group on top of the group of Trump speeches, in red. The features are plotted as text in gray and are all over the place. The axes read "Dimension 1 (16.61%)" and "Dimension 2 (12.99 %)".
dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), col = "gray60") +
  geom_point(data = texts_df, aes(x = V1, y = V2, col = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

## Typical Clinton features vs. typical Trump features

Since the dimension that sets apart the Clinton speeches (top) most clearly from the Trump speeches (bottom) is the second dimension (y-axis), we may want to inspect the features that occupy extreme positions on the y-axis (and that hence contribute importantly to the y-axis). This is shown in @tbl-extremes1

The code below selects sorts the dataframe with features information based on their coordinates in the second dimension ("V2") and extracts the first 20 elements. @tbl-extremes1-1 has the highest values in the y-axis, i.e. the 20 features most typical of Clinton's speeches, whereas @tbl-extremes1-2 has the lowest values in the y-axis, i.e. the 20features most typical of Trump's speeches.

```{r}
#| label: tbl-extremes1
#| tbl-cap: Words most characteristic of Clinton's or Trump's speeches.
#| tbl-subcap: 
#|   - "Clinton"
#|   - "Trump"
#| layout-ncol: 2
words_df %>%
  arrange(desc(V2)) %>%
  head(20) %>%
  kbl(digits = 3) %>% 
  kable_paper(full_width = FALSE)

words_df %>%
  arrange(V2) %>%
  head(20) %>%
  kbl(digits = 3) %>% 
  kable_paper(full_width = FALSE)
```

# High frequency content words as features {#sec-content}

In a second approach, we look at *content words* instead. Obviously, then, topic will start playing an important role (although some stylistic differences may still be present in the patterns that emerge). The nature of this analysis is very different to the one shown in @sec-function, and it serves different purposes. Even so, it is a legitimate question whether the Clinton speeches and the Trump speeches turn out to be as clearly separated as in the previous analysis from the perspective of content words. Moreover, we can inspect which (groups of) content words will emerge as the ones that are most characteristic of the different areas in the resulting map.

## Specifying the features

From a technical perspective, the only difference with the case study in @sec-function is how we define the features. This time, we build a complete frequency list of the whole corpus, we then remove a number of stop words (mostly function words, but also some undesired types such as `000` and `--`), and finally we treat the top 150 (i.e. the 150 highest frequency items) of the remaining items as our features. The number 150 is an arbitrary choice. We chose it so we could make a fair comparison to the performance of the analysis that used function words.

```{r}
#| label: setup2
stop_list <- read_types("assets/ca-trump-clinton/stop_list.txt")
print(stop_list, n = 5)

features <- freqlist(fnames) %>%
  drop_types(stop_list) %>%
  keep_pos(1:150) %>%
  as_types() %>%
  print(n = 10)
```

## Next steps and plot

The next steps, all the way up to the creation of the plot, are completely analogous to the previous analysis. @fig-ca2 shows the biplot.

```{r}
#| label: ca2
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]
  short_fname <- short_fnames[[i]]
  flist <- freqlist(fname)
  flist <- flist[features]
  d[[short_fname]] <- flist
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()

d_ca <- ca(d)

texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

```{r}
#| label: fig-ca2
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on content words.
dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), col = "gray60") +
  geom_point(data = texts_df, aes(x = V1, y = V2, col = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. This time it is the combination of the two dimensions that sets apart the two groups of speeches. This makes it a bit more difficult to select the items on the basis of their co-ordinates. We will do so in a minute, but first, we use a more visual approach. We will demonstrate how we can zoom in on a specific area, if we want to be able to better read the words in that area. This is demonstrated in @fig-ca3 for the region of x-values ranging from `0` to `0.5` and y-values ranging from `-0.5` to `0`.

```{r}
#| label: fig-ca3
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on content words, zoomed in for the bottom right quadrant.
ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), col = "gray60") +
  geom_point(data = texts_df, aes(x = V1, y = V2, col = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed(xlim = c(0, 0.5), ylim = c(-0.5, 0))
```

Identifying the same items by co-ordinates can be done by filtering the items based on their values in the x-axis ("V1") and y-axis ("V2"); the output is shown @tbl-words2:

```{r}
#| label: tbl-words2
#| tbl-cap: Words from the bottom right quadrant of the biplot with content words as figures.
words_df %>%
  filter(V1 >= 0 & V1 <= 0.5 & V2 >= -0.5 & V2 <= 0) %>%
  kbl(digits = 3) %>% 
  kable_paper(full_width = FALSE) %>% 
  scroll_box(height = "400px")
```

# High frequency bigrams as features {#sec-bigrams}

In our third approach, we work with the 150 most frequent bigrams. Notice in the code snippet below how we use the argument `ngram_size = 2` to make sure that `freqlist()` builds a frequency list of bigrams instead of single word forms. Notice also that prior to identifying the top 150, we remove all bigrams with either `applause`, `cheering`, or `--` in them. We do this because in this corpus those items (almost always) are part of the annotation instead of real tokens.

```{r}
#| label: features3
features <- fnames %>%
  freqlist(ngram_size = 2) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>% # or keep_pos(1:150)
  as_types() %>%
  print(n = 10)
```

The rest of the steps again is identical to what it was in the previous analysis, except that when we build frequency lists for each file, we must make sure that here too we build a frequency list of bigrams.

::: callout-tip
The step `drop_re("(applause|cheering|--)")` is not necessary here, because bigrams containing `applause`, `cheering`, or `--` are not part of the feature list anyway.
:::

```{r}
# build file by feature frequency matrix
d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]
  short_fname <- short_fnames[[i]]
  flist <- freqlist(fname, ngram_size = 2)
  flist <- flist[features]
  d[[short_fname]] <- flist
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()

# conduct CA
d_ca <- ca(d)
# summary(d_ca)

# preparations for biplot
texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

```{r}
#| label: fig-ca4
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on frequent bigrams.
dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), col = "gray60") +
  geom_point(data = texts_df, aes(x = V1, y = V2, col = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. Moreover, Clinton's speeches are tightly grouped on the left side of the plot whereas Trump's extend over the right side, so the x-axis is now the relevant dimension. @tbl-extremes2 shows the most characteristic bigrams for each of the authors, identified by having the lowest values in the first dimension (for Clinton's) or the highest values (for Trump's).

```{r}
#| label: tbl-extremes2
#| tbl-cap: 20 most characteristic bigrams of Clinton's or Trump's speeches.
#| tbl-subcap: 
#|   - "Clinton"
#|   - "Trump"
#| layout-ncol: 2

words_df %>%
  arrange(V1) %>%
  head(20) %>%
  kbl(digits = 3) %>% 
  kable_paper()

words_df %>%
  arrange(desc(V1)) %>%
  head(20) %>%
  kbl(digits = 3) %>% 
  kable_paper()
```

# High frequency trigrams as features {#sec-trigrams}

In our fourth approach, we work with the 150 most frequent trigrams; the workflow is the same as in @sec-bigrams, but with `ngram_size = 3` instead of 2. @fig-ca5 shows the biplot.

```{r}
#| label: features4
features <- fnames %>%
  freqlist(ngram_size = 3) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types() %>%
  print(n = 10)

d <- data.frame(row.names = features)
for (i in seq_along(fnames)) {
  fname <- fnames[[i]]
  short_fname <- short_fnames[[i]]
  flist <- freqlist(fname, ngram_size = 3)
  flist <- flist[features]
  d[[short_fname]] <- flist
}
d <- d %>%
  as.matrix() %>%
  t() %>%
  drop_empty_rc()

d_ca <- ca(d)

texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

```{r}
#| label: fig-ca5
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on frequent trigrams.
dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), col = "gray60") +
  geom_point(data = texts_df, aes(x = V1, y = V2, col = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. Moreover, Clinton's speeches are again tightly grouped on the left side of the plot whereas Trump's extend over the right side with an impressive proportion of features, so the x-axis is now the relevant dimension. @tbl-extremes3 shows the most characteristic bigrams for each of the authors, identified by having the lowest values in the first dimension (for Clinton's) or the highest values (for Trump's).

```{r}
#| label: tbl-extremes3
#| tbl-cap: 20 most characteristic bigrams of Clinton's or Trump's speeches.
#| tbl-subcap: 
#|   - "Clinton"
#|   - "Trump"
#| layout-ncol: 2

words_df %>%
  arrange(V1) %>%
  head(20) %>%
  kbl(digits = 3) %>% 
  kable_paper()

words_df %>%
  arrange(desc(V1)) %>%
  head(20) %>%
  kbl(digits = 3) %>% 
  kable_paper()
```

If you're curious about a specific item, you can use `mclm::conc()` to collect its occurrences. Notice that even infrequent expressions can be typical if they are significantly more frequent in one group of documents than in the other.

```{r}
#| label: conc
conc(fnames, "(?xi) i \\s+ want \\s+ you") %>% 
  print_kwic(n = 5)
```

# Extra: writing functions {#sec-custom}

If you have a lot of code that needs to be run over and over again with minimal modifications, writing a function is a good idea. However, putting *everything* into one overarching function is not always the best solution. Both to facilitate debugging and to give you the opportunity to inspect elements at different stages, you might want to write smaller functions for functional units of code.

One way of going about this with the code in this document is by writing one function to compile the frequencies, one to preprocess the result for plotting and one to generate the plot.

## Compile frequencies

The function below, `compile_frequencies()`, takes as arguments the `features`, the `fnames`, the `short_fnames` and the different arguments we want to give to `freqlist()`, since that will change in each case study.

Line 5 creates the empty dataframe with the `features` as rows. Lines 6 through 12 run the for loop across filenames that creates one frequency list per document, filters it by the features and adds them as a column to the dataframe. The `...` in the `freqlist()` call in line 9 provides whatever arguments we have given at the end of `compile_frequencies()` (other than the three main ones), if any, to `freqlist()`. Finally, lines 14 through 17 turn the dataframe into a matrix, transpose it and remove all empty rows and columns. This is what is returned by the function: the output of `compile_frequencies()` is a matrix of frequencies that we can inspect and manipulate as needed, and that would be given to `ca()`.

```{r}
#| label: compile-freqs
#| code-line-numbers: true
compile_frequencies <- function(features,
                                fnames,
                                short_fnames,
                                ...) {
  d <- data.frame(row.names = features)
  for (i in seq_along(fnames)) {
    fname <- fnames[[i]]
    short_fname <- short_fnames[[i]]
    flist <- freqlist(fname, ...)
    flist <- flist[features]
    d[[short_fname]] <- flist
    }
  
  d %>%
    as.matrix() %>%
    t() %>%
    drop_empty_rc()
}
```

## Obtain coordinates

By giving the output of `compile_frequencies()` to `ca()`, we obtain a correspondence analysis object that we could call `d_ca`. Next, we might want to write one or two functions to create the small tibbles with the row and column coordinates and the variables for plotting. Here I will show how to create one function `get_coords()` that takes the object `d_ca` and the subcorpora vector and returns a named list with two elements. Alternatively, you could also write two separate functions: one for the rows and one for the columns (the latter wouldn't need the subcorpora vector).

Here, lines 2-4 replicate the `texts_df` definition and lines 6-7, that of `words_df`. Lines 9-12 create and return a named list with both elements.

```{r}
#| label: get-coords
#| code-line-numbers: true
get_coords <- function(d_ca, sub_corp) {
  texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
    as_tibble(rownames = "text") %>% 
    mutate(Subcorpus = sub_corp)
  
  words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
    as_tibble(rownames = "word")
  
  list(
    rows = texts_df,
    cols = words_df
  )
}
```

If you wrote two separate functions, you could call them to generate `texts_df` and `words_df` separately. Instead, the output of `get_coords()` will be a list, let's say `ca_coords`, of which the element `ca_coords$rows` will correspond to `texts_df` and `ca_coords$cols` will correspond to `words_df`.

## Plot

Finally, we can write a plotting function that covers everything we need to generate our plot. Such functions are extremely useful when writing papers in which you need to print multiple different plots with similar aesthetic characteristics. Moreover, if you have one function to generate a lot of different plots, i.e. scatterplots for different datasets, and you decide to change the color palette, the background color or the size of the points, you can simply adjust the function, and all the plots generated with it will change in response.

The function below, `plot_ca()`, takes the result of `get_coords()` and a list with variance data extracted from `summary(d_ca)$scree[,3]`. Lines 2 and 3 take the variance information to include it in the axis labels, later defined in line 12. In lines 5 and 7, what used to be `texts_df` and `words_df` are now `ca_coords$rows` and `ca_coords$cols`, i.e. the elements "rows" and "cols" from the output of `get_coords()`.

```{r}
#| label: plot-function
#| code-line-numbers: true
plot_ca <- function(ca_coords, variances) {
  dim_1 <- sprintf(
    "Dimension 1 (%.2f %%)",
    variances[[1]]
    )
  dim_2 <- sprintf(
    "Dimension 2 (%.2f %%)",
    variances[[2]]
    )
  
  ggplot(ca_coords$cols, aes(x = V1, y = V2)) +
    geom_text(aes(label = word), col = "gray60") +
    geom_point(
      data = ca_coords$rows,
      aes(x = V1, y = V2, col = Subcorpus)
      ) +
    scale_color_manual(values = c("#0000CD","#DC143C")) +
    geom_hline(yintercept = 0, color = "darkgray") +
    geom_vline(xintercept = 0, color = "darkgray") +
    theme_bw(base_size = 12) +
    labs(x = dim_1, y = dim_2) +
    coord_fixed()
}
```

## Example: first case study

With these functions, the code for the case study in @sec-function (after loading `fnames`, `short_fnames` and `sub_corp`, of course) becomes the one below. After each line, we have a significant unit that we might want to inspect or manipulate.

```{r}
#| label: fig-redo
#| fig-column: margin
#| fig-cap: "CA plot with function words as features."
#| code-line-numbers: true
features <- read_types("assets/ca-trump-clinton/function-words.txt")
d <- compile_frequencies(features, fnames, short_fnames)
d_ca <- ca(d)
ca_coords <- get_coords(d_ca, sub_corp)
plot_ca(ca_coords, summary(d_ca)$scree[,3])
```

-   Line 1 reads the files with features; after it we can check whether it actually contains the items we wanted.

-   Line 2 creates the matrix of documents by features, and we can inspect it afterwards with something like `d[1:10,1:10]` or `dim(d)`.

-   Line 3 runs the correspondence analysis and we can inspect the output with `d_ca` or `summary(d_ca)`.

-   Line 4 prepares the coordinates for plotting. We can then also check the most extreme features with `head(arrange(ca_coords$cols, desc(V1)))`, for example.

-   Line 5 plots the dots on the components (@fig-redo).

## Full code

The full code of this document could then be compressed as in the chunk below. Notice that here I have excluded all calls to `print()`, `kbl()` and `kable_paper()`. I have also used different variable names for the different studies instead of overwriting variables, since it can cause problems when you lose track of what is stored as what. Finally, I dedicated a line to define the path to the corpus and one to define the path to the lists of function words and the stoplist.

```{r}
#| label: final
#| eval: false
assets <- "assets/ca-trump-clinton"
ct_corpus <- file.path(corpus_folder, "clinton_trump")
fnames <- get_fnames(ct_corpus)
short_fnames <- short_names(fnames)
sub_corp <- fnames %>%
  re_retrieve_first(
    "/clinton_trump/([^/]+)",
    requested_group = 1
    )

# First case study ----
features1 <- read_types(
  file.path(assets, "function-words.txt")
  )
d1 <- compile_frequencies(features1, fnames, short_fnames)
d1[1:10, 1:10]

d_ca1 <- ca(d)
ca_coords1 <- get_coords(d_ca1, sub_corp)

head(ca_coords1$rows)
plot_ca(ca_coords1, summary(d_ca1)$scree[,3])

## Inspect extremes ----
arrange(ca_coords1$cols, desc(V2)) %>% head(20)
arrange(ca_coords1$cols, V2) %>% head(20)

# Second case study ----
stop_list <- read_types(
  file.path(assets, "stop_list.txt")
)
features2 <- freqlist(fnames) %>%
  drop_types(stop_list) %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types()

d2 <- compile_frequencies(features2, fnames, short_fnames)
d_ca2 <- ca(d2)
ca_coords2 <- get_coords(d_ca2, sub_corp)
plot_ca(ca_coords2, summary(d_ca2)$scree[,3])

## Zoomed-in plot ----
plot_ca(ca_coords2, summary(d_ca2)$scree[,3]) +
  coord_fixed(xlim = c(0, 0.5), ylim = c(-0.5, 0))

## Select items ----
ca_coords2$cols %>%
  filter(V1 >= 0 & V1 <= 0.5 & V2 >= -0.5 & V2 <= 0)

# Third case study ----
features3 <- fnames %>%
  freqlist(ngram_size = 2) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types()

d3 <- compile_frequencies(
  features3,
  fnames,
  short_fnames,
  ngram_size = 2
  )
d_ca3 <- ca(d3)
ca_coords3 <- get_coords(d_ca3, sub_corp)
plot_ca(ca_coords3, summary(d_ca3)$scree[,3])

## Inspect extremes ----
arrange(ca_coords3$cols, V1) %>% head(20)
arrange(ca_coords3$cols, desc(V1)) %>% head(20)

# Fourth case study ----
features4 <- fnames %>%
  freqlist(ngram_size = 3) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types()

d4 <- compile_frequencies(
  features4,
  fnames,
  short_fnames,
  ngram_size = 4
  )
d_ca4 <- ca(d4)
ca_coords4 <- get_coords(d_ca4, sub_corp)
plot_ca(ca_coords4, summary(d_ca4)$scree[,])

## Inspect extremes ----
arrange(ca_coords4$cols, V1) %>% head(20)
arrange(ca_coords4$cols, desc(V1)) %>% head(20)
```
