---
title: "CA applied to the Trump-Clinton speeches"
number-sections: true
subtitle: "Based on Dirk Speelman's course material"
---

This document illustrates how to use *correspondence analysis* (CA) to investigate the similarities between Clinton speeches and Trump speeches in a corpus that collects them. The approach adopts a bird's-eye perspective, looking at the broad picture we get if we aggregate over a wide range of **features** of the speeches.

A major advantage of the technique is that we can both compare the different documents based on the selected features, but also compare the features based on the documents they occur in and also see which features or groups of features are the ones that account for the most important dissimilarities between the documents.

We will illustrate four analyses with only minimal differences between each other. In all of them, the workflow consists of collecting frequency lists from each of the files in our corpus and creating a matrix with one row per document and one column per type that occurs in the corpus. The cells contain the occurrence frequency: a cell $ij$ will contain the occurrence of type $j$ in the document $i$. Such **matrix of counts** will be given to `ca::ca()`, a function that runs correspondence analysis and returns, among other things, the coordinates of row items and of column items in the principal components as well as the variance covered by each dimension. @sec-code will show the full code to obtain the data and plot it before delving into the actual case studies.

The difference between the analyses is the definition of the types collected in the frequency lists. @sec-function will select only function words from a predefined list as features, since they have been shown to reliably discriminate between different authors and registers. @sec-content, on the other hand, will rely on high frequency content words by first excluding the types in a stoplist and then only keeping the top 150 items. Finally, @sec-bigrams and @sec-trigrams will use bigrams and trigrams as types, respectively. @sec-custom will round up showing how to condense the code of all studies in one shorter script.

# Setup

```{r}
#| label: setup
#| include: false
var_percent <- scales::label_percent(scale = 1, accuracy = 0.01)
```

For these studies we will need the following R packages: `{tidyverse}`, `{mclm}`, `{here}` and for nice tables when reporting, `{kableExtra}`.

```{r}
#| label: loadpkg
#| message: false

library(tidyverse)
library(mclm)
library(here)
library(kableExtra)
```

::: callout-note
While we conduct correspondence analysis with `ca::ca()`, we don't need to attach the `{ca}` package separately, since it's already loaded with `{mclm}`.
:::

## Collect the corpus filenames

First, we collect the names of the corpus files and store them in an object called `fnames`. From this we also derive the short filenames (stored in `short_fnames`) and a character vector with, for each filename, the subcorpus (stored in `sub_corp`). The first variable, `short_fnames`, is also an `fnames` object and will provide more practical names for plotting. The second variable, `sub_corp`, is a character string and will help us assign colors in the plots based on whether the documents correspond to Clinton or Trump speeches. @tbl-fnames illustrates a few of these values.

::: callout-warning
Don't forget to adapt the `corpus_folder` variable with the path to your copy of the corpus!
:::

```{r}
#| label: fnames
corpus_folder <- here("studies", "_corpora", "clinton_trump")
fnames <- get_fnames(corpus_folder)
short_fnames <- short_names(fnames)
sub_corp <- fnames %>%
  re_retrieve_first("/clinton_trump/([^/]+)", requested_group = 1)
```

```{r}
#| label: tbl-fnames
#| tbl-cap: Five random filenames and their corresponding values in `short_fnames` and `sub_corp`.
#| code-fold: true
set.seed(8541)
tibble(
  fnames = gsub(corpus_folder, "", fnames),
  short_fnames = short_fnames, sub_corp = sub_corp
) %>%
  group_by(sub_corp) %>% 
  slice_sample(n = 6) %>%
  kbl() %>% 
  kable_paper()
```

## General code {#sec-code}

The code below will be implemented in each of the case studies below with the only difference in the `freqlist()` call in line 2 and the actual value of `features` (in lines 2 and 5).

::: aside
In practice, if you were to run an analysis where this much code will be used over and over again with multiple modifications, it would be preferable to wrap it in custom functions. @sec-custom will show you how, if you're interested.
:::

```{r}
#| label: pre-code
#| eval: false
#| code-line-numbers: true
d <- map(setNames(fnames, short_fnames), function(fname) {
    freqlist(fname, ...)[features]
  }) %>%
  bind_cols() %>% 
  data.frame(row.names = features) %>% 
  as.matrix() %>% 
  t() %>% 
  drop_empty_rc()

d_ca <- ca(d)

texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")

dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), color = "gray60") +
  geom_point(data = texts_df, aes(color = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

The first part of the code builds the matrix that we will apply the correspondence analysis to. This is an object of the data type `matrix` with as its rows the files and as its columns the features, and in the cells the absolute frequencies of the features in the files.

The code in lines 1-3 uses the `map()` function from the `{purrr}` package to apply the same process to each file in the corpus, returning a comparable output. This output is a frequency list computed with `freqlist()` and then adjusted to return values for all and only the elements in the `features` vector, which will be different in each study. The output of `map()` is a list, and we will combine its elements into a tibble with one column per file by means of `bind_cols()` in line 4. Because we *named* the files with the short names in line 1, via the `setNames()` call, the column names are the elements in `short_fnames`. Line 5 turns the tibble into a dataframe so we can define `features` as the row names.

Lines 6-8 turn this dataframe into a matrix, transpose it (so that the features become the columns and the files, the rows) and drop the rows and columns with only zero frequencies.

In line 10, we run correspondence analysis on this matrix, which we store in the variable `d_ca`. We could then inspect it by calling `d_ca` or `summary(d_ca)`.

Then, lines 12-17 extract the coordinates of the rows and columns for plotting and create tibbles with the labels, positions and, in the case of the rows, the color coding based on the subcorpora. `mclm::row_pcoord()` and `mclm::col_pcoord()` extract the coordinates of the rows and columns from the object `d_ca`; we are only interested in the first two columns, which are the coordinates of the two first principal components, therefore we add `[,c(1,2)]`. We then turn the matrix into a tibble assigning the column names "text" and "word" to the row names of the matrices. In the case of the documents, we also add the color coding column by creating a column "Subcorpus" and filling it with the vector `sub_corp`.

Lines 19 and 20 use the variance information from `summary(d_ca)` to enrich the labels of the axes in the plot.
The rest of the lines create a biplot.

::: {.callout-note collapse="true"}
### Parsing the `ggplot()` call

What does each line from 22 to 30 do in the code chunk above?

-   Line 22 calls `ggplot()` with the dataframe of features (the columns in our matrix) as dataset, assigning the variables "V1" and "V2" to the `x` and `y` coordiantes respectively. These are the automatic names given by `as_tibble()` in line 17 when we converted a matrix without column names^[The original matrix `d` and `d_ca` do have column names, but the matrices of coordinates returned by `row_pcoord()` and `col_pcoord()` don't.].

-   Line 23 plots the items of the variable "word" (the types of our features) as text, in the color "gray60".

-   Line 24 plots the items of the dataframe of documents (the rows of the original matrix) as dots. The call inherits the `x` and `y` mappings from the `ggplot()` call, so that "V1" and "V2" will automatically be mapped into the axes. In addition, we map "Subcorpus" to the color aesthetics.

-   Line 25 defines manually the colors to map to the "Subcorpus" variable. We could also use a variety of palettes, different colors, or remove the line and use the default color scheme of `{ggplot2}`.

-   Lines 26 and 27 add a horizontal and vertical line respectively, the former where $y = 0$ and the latter where $x = 0$, both in dark gray.

-   Line 28 sets the theme and the base size for the font. If you create multiple plots, you can set this for all your plots with `theme_set()`.

-   Line 29 assigns the variables `dim_1` and `dim_2`, defined in lines 19 and 20, as the labels of the `x` and `y` axes.

-   Line 30 indicates that the units of both axes should be the same, which is not the default case for `{ggplot2}` plots. The numbers themselves of the coordinates are not meaningful, but the distances between the points are, and they assume that the distance between the point in $(0,1)$ (where $x = 0$ and $y = 1$) and the centroid $(0, 0)$ is the same as the distance between the point $(1,0)$ and the centroid $(0,0)$. `coord_fixed()` makes sure that's the case.
:::

# Function words as features {#sec-function}

In a first approach, we use so-called *function words* as features.[Function words include closed word classes such as pronouns, prepositions, auxiliaries, etc., as opposed to so-called *content words* such as nouns, verbs, adjectives...]{.aside} In the field of *stylometry*, which is devoted to the quantitative study of linguistics style (e.g. in support of authorship attribution), it has long been established that function words are a powerful instrument in support of the identification of typical or even unique characteristics of the style of a specific author (or group of authors).

::: callout-note
Zooming in on function words steers the analysis away from differences related to topic/content and foregrounds differences related to style. Other alternatives are:

i) Zooming in on longer n-grams, e.g. 3-grams, 4-grams.

i) If the information is available, looking at the frequencies of POS tags or POS tag n-grams.

i) Looking at derived features such as lexical density, word length, sentence length, etc.
:::

## Specifying the features

We start by reading the names of the features for this analysis from the file [function-words.txt](https://raw.githubusercontent.com/mclm2022/mclm2022.github.io/main/studies/assets/ca-trump-clinton/function-words.txt) and storing then into an object called `features`.

```{r}
#| label: func-features
features <- read_types(here("studies", "assets", "ca-trump-clinton", "function-words.txt"))
print(features, n = 10)
```

As shown when printing it, the object `features` contains `r n_types(features)` function words.^[You can also obtain the number of types in an object of class `types` (such as `features`), with `n_types()`.]

## Building the file-by-feature matrix

Once we have collected our features we can create the matrix with one row per document and one column per feature.

```{r}
#| label: func-d
d <- map(setNames(fnames, short_fnames), function(fname) {
    freqlist(fname)[features]
  }) %>%
  bind_cols() %>% 
  data.frame(row.names = features) %>% 
  as.matrix() %>% 
  t() %>% 
  drop_empty_rc()
```

The top-left part of the resulting matrix (first ten rows, first ten columns) is shown in @tbl-mtx. Each row is a speech, each column is a function word, and the values are the absolute frequencies of a given function word in a igven document.

```{r}
#| label: tbl-mtx
#| tbl-cap: First rows and columns of a document-by-feature matrix where the rows are Clinton and Trump speeches and the columns are function words.
#| code-fold: true
kbl(d[1:10, 1:10]) %>% kable_paper()
```

## Running correspondence analysis

Then we run the actual correspondence analysis and store the result in an object called `d_ca`. For reasons of brevity, we will skip the inspection of the summary report of the analysis, which we would normally do with `summary(d_ca)`.

```{r}
#| label: func-ca
d_ca <- ca(d)
# summary(d_ca)
```

## Biplot

Next, we build the biplot, with color indicating the subcorpus that each file belongs to. First, we prepare the data needed for the plot. If you wanted to inspect the full output of `row_pcoord()` or `col_pcoord()`, you can run them separately first (e.g. `row_pcoord(d_ca) %>% View()`).

```{r}
#| label: func-plotprep
#| warning: false
texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)
words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

@tbl-func shows a random sample from `texts_df` and `words_df`, i.e. the rows and columns of the dataset with their principal component coordinates and the corpus they correspond to. The columns "V1" and "V2" contain the positions in the first and second dimension respectively. In @tbl-func-1 we can already see that Clinton documents tend to be in the bottom half of the plot ("V2" is negative), whereas Trump documents tend to be in the top half ("V2" is positive).

```{r}
#| label: tbl-func
#| tbl-cap: Subset of speeches and function words with principal components.
#| tbl-subcap: 
#|   - "Texts"
#|   - "Features"
#| code-fold: true
#| layout-ncol: 2
set.seed(2022)
texts_df %>% group_by(Subcorpus) %>% 
  slice_sample(n = 5) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
words_df %>%
  slice_sample(n = 10) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
```

The plot is shown in @fig-func. We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. In addition, both subcorpora form very clear clusters, mostly divided by the second dimension: features higher on the y-axis are more characteristic of Trump's speeches, and those lower on the y-axis are more characteristic of Clinton's, as we could already see from @tbl-func-1.

```{r}
#| label: fig-func
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on function words.
#| fig-alt: !expr sprintf('Biplot of correspondence analysis of Clinton and Trump speeches based on function words. The dots corresponding to Clinton, in blue, form a group on top of the group of Trump speeches, in red. The features are plotted as text in gray and are all over the place. The axes read "Dimension 1 (%.2f%%)" and "Dimension 2 (%.2f%%)".', summary(d_ca)$scree[1,3], summary(d_ca)$scree[2,3])
dim_1 <- sprintf("Dimension 1 (%.2f%%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f%%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), color = "gray60") +
  geom_point(data = texts_df, aes(color = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

## Typical Clinton features vs. typical Trump features

Since the dimension that sets apart the Clinton speeches (top) most clearly from the Trump speeches (bottom) is the second dimension (y-axis), we may want to inspect the features that occupy extreme positions on the y-axis (and that hence contribute importantly to the y-axis).

The code below sorts the dataframe with features information based on their coordinates in the second dimension ("V2", i.e. the vertical axis) and extracts the first 20 elements. `clinton_words` and `trump_words` are thus vectors with the 20 words most characteristic of Clinton's and Trump's speeches respectively.

```{r}
#| label: func-extremes
trump_words <- words_df %>%
  arrange(desc(V2)) %>%
  head(20) %>% pull(word)
clinton_words <- words_df %>%
  arrange(V2) %>%
  head(20) %>% pull(word)
```

The markdown text below adds asterisks to each word to print them in italics and brings them together in an enumeration with the help of `glue::glue_collapse()`, resulting in the text right below it.

```md
The words most characteristic of Clinton's speeches are
`r knitr::inline_expr('glue::glue_collapse(paste0("*", clinton_words, "*"), sep = ", ", last = " and ")')`.
Those most characteristic of Trump's speeches, instead, are
`r knitr::inline_expr('glue::glue_collapse(paste0("*", trump_words, "*"), sep = ", ", last = " and ")')`.
```

> The words most characteristic of Clinton's speeches are `r glue::glue_collapse(paste0("*", clinton_words, "*"), sep = ", ", last = " and ")`. Those most characteristic of Trump's speeches, instead, are `r glue::glue_collapse(paste0("*", trump_words, "*"), sep = ", ", last = " and ")`.

# High frequency content words as features {#sec-content}

In a second approach, we look at *content words* instead. Obviously, then, topic will start playing an important role (although some stylistic differences may still be present in the patterns that emerge). The nature of this analysis is very different to the one shown in @sec-function, and it serves different purposes. Even so, it is a legitimate question whether the Clinton speeches and the Trump speeches turn out to be as clearly separated as in the previous analysis from the perspective of content words. Moreover, we can inspect which (groups of) content words will emerge as the ones that are most characteristic of the different areas in the resulting map.

## Specifying the features

From a technical perspective, the only difference with the case study in @sec-function is how we define the features. This time, we build a complete frequency list of the whole corpus, we then remove a number of [stop words](https://raw.githubusercontent.com/mclm2022/mclm2022.github.io/main/studies/assets/ca-trump-clinton/stop_list.txt) (mostly function words, but also some undesired types such as `000` and `--`), and finally we treat the top 150 (i.e. the 150 highest frequency items) of the remaining items as our features. The number 150 is an arbitrary choice but it makes this study more comparable to the one in @sec-function.

```{r}
#| label: content-features
stop_list <- read_types(here("studies", "assets", "ca-trump-clinton", "stop_list.txt"))
print(stop_list, n = 5)

features <- freqlist(fnames) %>%
  drop_types(stop_list) %>%
  keep_pos(1:150) %>%
  as_types() %>%
  print(n = 10)
```

## Next steps and plot

The next steps, all the way up to the creation of the plot, are completely analogous to the previous analysis.

```{r}
#| label: content-d
d <- map(setNames(fnames, short_fnames), function(fname) {
    freqlist(fname)[features]
  }) %>%
  bind_cols() %>% 
  data.frame(row.names = features) %>% 
  as.matrix() %>% 
  t() %>% 
  drop_empty_rc()

d_ca <- ca(d)

texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

@tbl-content shows again a random subset of documents with their new coordinates, as well as a random subset of features ---this time content words--- and their coordinates. @fig-content shows the biplot.

```{r}
#| label: tbl-content
#| tbl-cap: Subset of speeches and content words with principal components.
#| tbl-subcap: 
#|   - "Texts"
#|   - "Features"
#| code-fold: true
#| layout-ncol: 2
set.seed(2022)
texts_df %>% group_by(Subcorpus) %>% 
  slice_sample(n = 5) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
words_df %>%
  slice_sample(n = 10) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
```

```{r}
#| label: fig-content
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on content words.
#| fig-alt: !expr sprintf('Biplot of correspondence analysis of Clinton and Trump speeches based on content words. The dots corresponding to Clinton, in blue, form a tight group on the top left quadrant, whereas the red dots representing Trump speeches spread across the bottom right quadrant, spilling to the top and left. The features are plotted as text in gray and are all over the place. The axes read "Dimension 1 (%.2f%%)" and "Dimension 2 (%.2f%%)".', summary(d_ca)$scree[1,3], summary(d_ca)$scree[2,3])
dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), color = "gray60") +
  geom_point(data = texts_df, aes(color = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. This time it is the combination of the two dimensions that sets apart the two groups of speeches. This makes it a bit more difficult to select the items on the basis of their coordinates. One approach is to select quadrants or regions, either to zoom in on the plot (@fig-zoom) or to obtain a selection of words.

```{r}
#| label: fig-zoom
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on content words, zoomed in for the bottom right quadrant.
#| fig-alt: Biplot of correspondence analysis of Clinton and Trump speeches based on content words, zoomed in for the bottom right quadrant. Content words are seen in gray and some Trump speeches are represented as red dots.
ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), color = "gray60") +
  geom_point(data = texts_df, aes(color = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed(xlim = c(0, 0.5), ylim = c(-0.5, 0))
```

We can also compute the euclidean distance from each point to the center of the plot to identify the items that are farthest from it (the `dist_to_center` variable). The code below selects all words in the bottom right quadrant (even beyond the zoomed-in region in @fig-zoom) and order thems by distance to the center; the markdown text below it lists some of them, resulting in the block text underneath.

```{r}
#| label: words2
bottom_right <- words_df %>%
  filter(V1 >= 0 & V2 <= 0) %>% 
  mutate(dist_to_center = sqrt(V1^2 + V2^2)) %>% 
  arrange(desc(dist_to_center)) %>% 
  pull(word)
```

```md
There are `r knitr::inline_expr('length(bottom_right)')` words in the bottom right quadrant of @fig-content;
the fifteen farthest from the center of the plot are
`r knitr::inline_expr('glue::glue_collapse(paste0("*", head(bottom_right, 15), "*"), sep = ", ", last = " and ")')`.
```

> There are `r length(bottom_right)` words in the bottom right quadrant of @fig-content; the fifteen farthest from the center of the plot are `r glue::glue_collapse(paste0("*", head(bottom_right, 15), "*"), sep = ", ", last = " and ")`.

# High frequency bigrams as features {#sec-bigrams}

In our third approach, we work with the 150 most frequent bigrams. In the code snippet below we use the argument `ngram_size = 2` to make sure that `freqlist()` builds a frequency list of bigrams instead of single word forms. In addition, prior to identifying the top 150, we remove all bigrams with either `applause`, `cheering`, or `--` in them. We do this because in this corpus those items are (almost always) part of the annotation instead of real tokens.

```{r}
#| label: bg-features
features <- fnames %>%
  freqlist(ngram_size = 2) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>% # or keep_pos(1:150)
  as_types() %>%
  print(n = 10)
```

The rest of the steps again are identical to the previous analyses, except that when we build frequency lists for each file, we must make sure that here too we build a frequency list of bigrams.

::: callout-tip
The step `drop_re("(applause|cheering|--)")` is not necessary when collecting those frequency lists, as the elements are already filtered via `[features]`.
:::


```{r}
#| label: bg-d
d <- map(setNames(fnames, short_fnames), function(fname) {
    freqlist(fname, ngram_size = 2)[features]
  }) %>%
  bind_cols() %>% 
  data.frame(row.names = features) %>% 
  as.matrix() %>% 
  t() %>% 
  drop_empty_rc()

d_ca <- ca(d)

texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

@tbl-bg shows again a random subset of documents with their new coordinates, as well as a random subset of features ---this time bigrams--- and their coordinates. @fig-bg shows the biplot.

```{r}
#| label: tbl-bg
#| tbl-cap: Subset of speeches and bigrams with principal components.
#| tbl-subcap: 
#|   - "Texts"
#|   - "Bigrams"
#| code-fold: true
#| layout-ncol: 2
set.seed(2022)
texts_df %>% group_by(Subcorpus) %>% 
  slice_sample(n = 5) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
words_df %>%
  slice_sample(n = 10) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
```

```{r}
#| label: fig-bg
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on frequent bigrams.
#| fig-alt: !expr sprintf('Biplot of correspondence analysis of Clinton and Trump speeches based on the most frequent bigrams. The blue dots, corresponding to Clinton, tend towards the left and the red ones, corresponding to Trump, to the right. The latter are also surrounded by many more bigrams, represented as gray text, than the former. The axes read "Dimension 1 (%.2f%%)" and "Dimension 2 (%.2f%%)".', summary(d_ca)$scree[1,3], summary(d_ca)$scree[2,3])
dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), color = "gray60") +
  geom_point(data = texts_df, aes(color = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. Moreover, Clinton's speeches are tightly grouped on the left side of the plot whereas Trump's extend over the right side, so the x-axis is now the relevant dimension. The code below sorts the dataframe with features information based on their coordinates in the first dimension ("V1", i.e. the horizontal axis) and extracts the first 20 elements. `clinton_bg` and `trump_bg` are thus vectors with the 20 bigrams most characteristic of Clinton's and Trump's speeches respectively.

```{r}
#| label: bg-extremes
clinton_bg <- words_df %>%
  arrange(V1) %>%
  head(20) %>% pull(word)
trump_bg <- words_df %>%
  arrange(desc(V1)) %>%
  head(20) %>% pull(word)
```

```md
The bigrams most characteristic of Clinton's speeches are
`r knitr::inline_expr('glue::glue_collapse(paste0("*", clinton_bg, "*"), sep = ", ", last = " and ")')`.
Those most characteristic of Trump's speeches, instead, are
`r knitr::inline_expr('glue::glue_collapse(paste0("*", trump_bg, "*"), sep = ", ", last = " and ")')`.
```

> The bigrams most characteristic of Clinton's speeches are `r glue::glue_collapse(paste0("*", clinton_bg, "*"), sep = ", ", last = " and ")`. Those most characteristic of Trump's speeches, instead, are `r glue::glue_collapse(paste0("*", trump_bg, "*"), sep = ", ", last = " and ")`.

# High frequency trigrams as features {#sec-trigrams}

In our third approach, we work with the 150 most frequent trigrams --- the workflow is the same as in @sec-bigrams, but `ngram_size` must now be set to 3 instead of 2.

```{r}
#| label: tg-d
features <- fnames %>%
  freqlist(ngram_size = 3) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>% # or keep_pos(1:150)
  as_types() %>%
  print(n = 10)

d <- map(setNames(fnames, short_fnames), function(fname) {
    freqlist(fname, ngram_size = 3)[features]
  }) %>%
  bind_cols() %>% 
  data.frame(row.names = features) %>% 
  as.matrix() %>% 
  t() %>% 
  drop_empty_rc()

d_ca <- ca(d)

texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "text") %>% 
  mutate(Subcorpus = sub_corp)

words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
  as_tibble(rownames = "word")
```

@tbl-tg shows again a random subset of documents with their new coordinates, as well as a random subset of features ---this time trigrams--- and their coordinates. @fig-tg shows the biplot.

```{r}
#| label: tbl-tg
#| tbl-cap: Subset of speeches and trigrams with principal components.
#| tbl-subcap: 
#|   - "Texts"
#|   - "Trigrams"
#| code-fold: true
#| layout-ncol: 2
set.seed(2022)
texts_df %>% group_by(Subcorpus) %>% 
  slice_sample(n = 5) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
words_df %>%
  slice_sample(n = 10) %>% kbl() %>% 
  kable_paper(full_width = FALSE)
```

```{r}
#| label: fig-tg
#| fig-cap: Biplot of correspondence analysis of Clinton and Trump speeches based on frequent trigrams.
#| fig-alt: !expr sprintf('Biplot of correspondence analysis of Clinton and Trump speeches based on the most frequent trigrams. The dots corresponding to Clinton, in blue, form a small group on the left, while the dots representing Trump speeches, in red, are on the right side. The latter are surrounded by many more trigrams, represented as gray text. The axes read "Dimension 1 (%.2f%%)" and "Dimension 2 (%.2f%%)".', summary(d_ca)$scree[1,3], summary(d_ca)$scree[2,3])
dim_1 <- sprintf("Dimension 1 (%.2f %%)", summary(d_ca)$scree[1,3])
dim_2 <- sprintf("Dimension 2 (%.2f %%)", summary(d_ca)$scree[2,3])

ggplot(words_df, aes(x = V1, y = V2)) +
  geom_text(aes(label = word), color = "gray60") +
  geom_point(data = texts_df, aes(color = Subcorpus)) +
  scale_color_manual(values = c("#0000CD","#DC143C")) +
  geom_hline(yintercept = 0, color = "darkgray") +
  geom_vline(xintercept = 0, color = "darkgray") +
  theme_bw(base_size = 12) +
  labs(x = dim_1, y = dim_2) +
  coord_fixed()
```

We can see that `r var_percent(summary(d_ca)$scree[1,3])` of the variation is covered by the first dimension and `r var_percent(summary(d_ca)$scree[2,3])` by the second dimension. Like in @fig-bg, Clinton's speeches are tightly grouped on the left side of the plot whereas Trump's extend over the right side, so the x-axis is now the relevant dimension. The code below sorts the dataframe with features information based on their coordinates in the first dimension ("V1", i.e. the horizontal axis) and extracts the first 10 elements. `clinton_tg` and `trump_tg` are thus vectors with the 10 trigrams most characteristic of Clinton's and Trump's speeches respectively.

```{r}
#| label: tg-extremes
clinton_tg <- words_df %>%
  arrange(V1) %>%
  head(10) %>% pull(word)
trump_tg <- words_df %>%
  arrange(desc(V1)) %>%
  head(10) %>% pull(word)
```

```md
The trigrams most characteristic of Clinton's speeches are
`r knitr::inline_expr('glue::glue_collapse(paste0("*", clinton_tg, "*"), sep = ", ", last = " and ")')`.
Those most characteristic of Trump's speeches, instead, are
`r knitr::inline_expr('glue::glue_collapse(paste0("*", trump_tg, "*"), sep = ", ", last = " and ")')`.
```

> The trigrams most characteristic of Clinton's speeches are `r glue::glue_collapse(paste0("*", clinton_tg, "*"), sep = ", ", last = " and ")`. Those most characteristic of Trump's speeches, instead, are `r glue::glue_collapse(paste0("*", trump_tg, "*"), sep = ", ", last = " and ")`.

If you're curious about a specific item, you can use `mclm::conc()` to collect its occurrences, as shown in @tbl-conc. Notice that even infrequent expressions can be considered typical if they are significantly more frequent in one group of documents than in the other.

```{r}
#| label: tbl-conc
#| tbl-cap: Concordances of _I want you_ in the corpus.
#| code-fold: true
#| column: screen
conc(fnames, r"--[(?xi) i \s+ want \s+ you]--") %>% 
  as_tibble() %>% 
  select(source, left, match, right) %>%
  mutate(source = short_names(source)) %>% 
  kbl(align = c("r", "r", "c", "l")) %>%
  kable_paper(font_size = 15) %>% 
  scroll_box(height = "400px")
```

# Extra: writing functions {#sec-custom}

If you have a lot of code that needs to be run over and over again with minimal modifications, writing a function is a good idea. However, putting *everything* into one overarching function is not always the best solution. Both to facilitate debugging and to give you the opportunity to inspect elements at different stages, you might want to write smaller functions for functional units of code.

One way of going about this with the code in this document is by writing one function to compile the frequencies, one to preprocess the result for plotting and one to generate the plot.

## Compile frequencies

The function below, `compile_frequencies()`, takes as arguments the `features`, the `fnames`, the `short_fnames` and the different arguments we want to give to `freqlist()`, since that will change in each case study.
In line 1 we name the function, and in lines 1-4 we list the possible arguments. The first three are named, whereas `...` allows us to add an undefined number or arguments. They will be given to `freqlist()` in line 6, e.g. `ngram_size = 2`.

```{r}
#| label: compile-freqs
#| code-line-numbers: true
compile_frequencies <- function(features,
                                fnames,
                                short_fnames,
                                ...) {
  d <- map(setNames(fnames, short_fnames), function(fname) {
    freqlist(fname, ...)[features]
  }) %>%
  bind_cols() %>% 
  data.frame(row.names = features) %>% 
  as.matrix() %>% 
  t() %>% 
  drop_empty_rc()
}
```

## Obtain coordinates

By giving the output of `compile_frequencies()` to `ca()`, we obtain a correspondence analysis object that we could call `d_ca`. Next, we might want to write one or two functions to create the small tibbles with the row and column coordinates and the variables for plotting. Here I will show how to create one function `get_coords()` that takes the object `d_ca` and the subcorpora vector and returns a named list with two elements. Alternatively, you could also write two separate functions: one for the rows and one for the columns (the latter wouldn't need the subcorpora vector).

Here, lines 2-4 replicate the `texts_df` definition and lines 6-7, that of `words_df`. Lines 9-12 create and return a named list with both elements.

```{r}
#| label: get-coords
#| code-line-numbers: true
get_coords <- function(d_ca, sub_corp) {
  texts_df <- row_pcoord(d_ca)[,c(1, 2)] %>% 
    as_tibble(rownames = "text") %>% 
    mutate(Subcorpus = sub_corp)
  
  words_df <- col_pcoord(d_ca)[,c(1, 2)] %>% 
    as_tibble(rownames = "word")
  
  list(
    rows = texts_df,
    cols = words_df
  )
}
```

If you wrote two separate functions, you could call them to generate `texts_df` and `words_df` separately. Instead, the output of `get_coords()` will be a list, let's say `ca_coords`, of which the element `ca_coords$rows` will correspond to `texts_df` and `ca_coords$cols` will correspond to `words_df`.

## Plot

Finally, we can write a plotting function that covers everything we need to generate our plot. Such functions are extremely useful when writing papers in which you need to print multiple different plots with similar aesthetic characteristics. Moreover, if you have one function to generate a lot of different plots, i.e. scatterplots for different datasets, and you decide to change the color palette, the background color or the size of the points, you can simply adjust the function, and all the plots generated with it will change in response.^[`theme_set()` is also an useful tool for this purpose.]

The function below, `plot_ca()`, takes the result of `get_coords()` and a list with variance data extracted from `summary(d_ca)$scree[,3]`. Lines 2 and 3 take the variance information to include it in the axis labels, later defined in line 12. In lines 5 and 7, what used to be `texts_df` and `words_df` are now `ca_coords$rows` and `ca_coords$cols`, i.e. the elements "rows" and "cols" from the output of `get_coords()`.

```{r}
#| label: plot-function
#| code-line-numbers: true
plot_ca <- function(ca_coords, variances) {
  dim_1 <- sprintf("Dimension 1 (%.2f %%)", variances[[1]])
  dim_2 <- sprintf("Dimension 2 (%.2f %%)", variances[[2]])
  
  ggplot(ca_coords$cols, aes(x = V1, y = V2)) +
    geom_text(aes(label = word), color = "gray60") +
    geom_point(data = ca_coords$rows, aes(color = Subcorpus)) +
    scale_color_manual(values = c("#0000CD","#DC143C")) +
    geom_hline(yintercept = 0, color = "darkgray") +
    geom_vline(xintercept = 0, color = "darkgray") +
    theme_bw(base_size = 12) +
    labs(x = dim_1, y = dim_2) +
    coord_fixed()
}
```

## Example: first case study

With these functions, the code for the case study in @sec-function (after loading `fnames`, `short_fnames` and `sub_corp`, of course) becomes the one below. After each line, we have a significant unit that we might want to inspect or manipulate.

```{r}
#| label: fig-redo
#| fig-column: margin
#| fig-cap: "CA plot with function words as features."
#| code-line-numbers: true
features <- read_types(here("studies", "assets", "ca-trump-clinton", "function-words.txt"))
d <- compile_frequencies(features, fnames, short_fnames)
d_ca <- ca(d)
ca_coords <- get_coords(d_ca, sub_corp)
plot_ca(ca_coords, summary(d_ca)$scree[,3])
```

-   Line 1 reads the files with features; after it we can check whether it actually contains the items we wanted.

-   Line 2 creates the matrix of documents by features, and we can inspect it afterwards with something like `d[1:10,1:10]` or `dim(d)`.

-   Line 3 runs the correspondence analysis and we can inspect the output with `d_ca` or `summary(d_ca)`.

-   Line 4 prepares the coordinates for plotting. We can then also check the most extreme features with `head(arrange(ca_coords$cols, desc(V1)))`, for example.

-   Line 5 plots the dots on the components (@fig-redo).

## Full code

The full code of this document could then be compressed as in the chunk below. Notice that here I have excluded all calls to `print()`, `kbl()` and `kable_paper()`. I have also used different variable names for the different studies instead of overwriting variables, since it can cause problems when you lose track of what is stored as what. Finally, I dedicated a line to define the path to the corpus and one to define the path to the lists of function words and the stoplist.

```{r}
#| label: final
#| eval: false
corpus_folder <- here("studies", "_corpora", "clinton_trump")
assets <- here("studies", "assets", "ca-trump-clinton")
fnames <- get_fnames(corpus_folder)
short_fnames <- short_names(fnames)
sub_corp <- fnames %>%
  re_retrieve_first("/clinton_trump/([^/]+)", requested_group = 1)

# Function words as features ----
features1 <- read_types(file.path(assets, "function-words.txt"))
d1 <- compile_frequencies(features1, fnames, short_fnames)

d_ca1 <- ca(d1)
ca_coords1 <- get_coords(d_ca1, sub_corp)

plot_ca(ca_coords1, summary(d_ca1)$scree[,3])

## Inspect extremes ----
ca_coords1$cols %>% 
  arrange(desc(V2)) %>% 
  head(20) %>% 
  pull(word)
ca_coords1$cols %>% 
  arrange(V2) %>% 
  head(20) %>% 
  pull(word)

# Content words as features ----
stop_list <- read_types(file.path(assets, "stop_list.txt"))
features2 <- freqlist(fnames) %>%
  drop_types(stop_list) %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types()

d2 <- compile_frequencies(features2, fnames, short_fnames)
d_ca2 <- ca(d2)
ca_coords2 <- get_coords(d_ca2, sub_corp)
plot_ca(ca_coords2, summary(d_ca2)$scree[,3])

## Zoomed-in plot ----
plot_ca(ca_coords2, summary(d_ca2)$scree[,3]) +
  coord_fixed(xlim = c(0, 0.5), ylim = c(-0.5, 0))

## Select items ----
ca_coords2$cols %>%
  filter(V1 >= 0 & V2 <= 0) %>% 
  mutate(dist_to_center = sqrt(V1^2 + V2^2)) %>% 
  arrange(desc(dist_to_center)) %>% 
  pull(word)
  
# Bigrams as features ----
features3 <- fnames %>%
  freqlist(ngram_size = 2) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types()

d3 <- compile_frequencies(features3, fnames, short_fnames, ngram_size = 2)
d_ca3 <- ca(d3)
ca_coords3 <- get_coords(d_ca3, sub_corp)
plot_ca(ca_coords3, summary(d_ca3)$scree[,3])

## Inspect extremes ----
ca_coords3$cols %>% 
  arrange(V1) %>%
  head(20) %>% 
  pull(word)
ca_coords3$cols %>% 
  arrange(desc(V1)) %>%
  head(20) %>% 
  pull(word)

# Trigrams as features ----
features4 <- fnames %>%
  freqlist(ngram_size = 3) %>%
  drop_re("(applause|cheering|--)") %>%
  keep_bool(ranks(.) <= 150) %>%
  as_types()

d4 <- compile_frequencies(features4, fnames, short_fnames, ngram_size = 3)
d_ca4 <- ca(d4)
ca_coords4 <- get_coords(d_ca4, sub_corp)
plot_ca(ca_coords4, summary(d_ca4)$scree[,])

## Inspect extremes ----
ca_coords4$cols %>% 
  arrange(V1) %>%
  head(10) %>% 
  pull(word)
ca_coords4$cols %>% 
  arrange(desc(V1)) %>%
  head(10) %>% 
  pull(word)
```
