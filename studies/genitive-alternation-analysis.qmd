---
title: "Genitive Alternation II: Analysis"
execute:
  warning: false
---

This is the second part of a variation study focusing on the genitive alternation in English, using the Brown corpus. This part illustrates the analysis with conditional inference trees and mixed-effects logistic regression, whereas the [first part](genitive-alternation-retrieval.qmd) focused on retrieving observations of the alternation.

```{r}
#| include: false
#| label: setup
data_folder <- "assets/genitive-alternation"
```

# Setup

For this study, as always, we need to activate the packages `{tidyverse}` and `{mclm}`. We will also use `kableExtra()` to print some tables.

```{r}
#| message: false
#| label: packages-1
library(tidyverse)
library(mclm)
library(kableExtra)
```

In addition, we'll need the packages `{lmerTest}`, which loads `{lme4}`, for the mixed-effects logistic regression; `{partykit}` for the conditional tree; `{afex}` for comparing models with different predictors and `{ggeffects}` for plotting effects. [If you have not installed these packages, remember to do so with `install.packages()`.]{.aside}

```{r}
#| message:  false
#| label: packages-2
library(lmerTest)
library(partykit)
library(afex)
library(ggeffects)
```

This time, we don't need to read any corpora. Instead, we'll open the concordance file we saved in the [first step](genitive-alternation-retrieval.qmd#save) and work with it. In between, you might also have gone through the examples for manual cleaning or to add other variables, either with a spreadsheet software or with [concAnnotator](https://github.com/montesmariana/concAnnotator/).

As we read the dataset, we'll also change the type of the categorical variables, i.e. `gen_type` and `possessor_type`, from character vectors to factors. Factors are *actually* numeric vectors with named levels. Crucially, they have predefined levels that keep their order: they will complain if you try to add an item with a different value, will remember an original level that has frequency 0, and will always keep the designed order.[The tidyverse package `{forcats}` has a lot of useful functions to manipulate factors, all starting with `fct_`.]{.aside} Modelling functions such as `lme4::glmer()` and `partykit::ctree()`, which we'll use below, cannot work with character vectors and need factors instead.

```{r}
#| label: read
cd <- read_conc(
  file.path(data_folder, "genitive-alternation.tab")) %>% 
    mutate(gen_type = factor(gen_type, levels = c("of", "s")),
           possessor_type = factor(possessor_type, levels = c("common", "proper")))
```

The first level of a factor is a reference level. For the response variable, in most modelling functions (not in tidymodels), this counts as the failure output; therefore, the model will show the odds of the *second level* (in this case *s*-genitive) occurring.

## Exploring the data

It is good practice to get familiar with your dataset before delving into the modelling. On the one hand, it may expose annotation errors, such as mistyped values in manually annotated variables. On the other hand, it gives you a foundation for the interpretation of the output and a notion of what you may encounter.

In the [retrieval section](genitive-alternation-retrieval.qmd#automatic-annotation) we already saw the `xtabs()` function, which quickly returns a contingency table between two variables.

```{r}
#| label: xtabs
xtabs(~ gen_type + possessor_type, data = cd)
```

With `{tidyverse}`, particularly for a more tabular format, you can use `count()` combined with `pivot_wider()` to obtain a similar result, as shown in @tbl-count.

```{r}
#| label: tbl-count
#| tbl-cap: Contingency table between the response variable 'genitive type' and the predictor 'possessor_type'.
#| tbl-cap-location: margin
cd %>% 
  count(gen_type, possessor_type) %>% 
  pivot_wider(names_from = possessor_type, values_from = n) %>% 
  kbl() %>%
  kable_paper(full_width = FALSE)
```

For quantitative variables, we can extract the corresponding column from the dataset as a vector, e.g. `pull(cd, size_possessor)` or `cd$size_possessor`, and apply `summary()`. In order to correlate this with a categorical variable, we can first split the dataset based on the values of that variable with `split()` and then apply `summary()` on the columns with `map()`.[`{purrr}` functions such as `map()` apply the same function to each element of a list or vector. Here, we take each element of the split dataframe and apply the same code.]{.aside}

```{r}
#| label: split
cd %>% 
  split(.$gen_type) %>% 
  map(~ summary(.x$size_possessor))
```

We can already see here that the tiniest and the longest Possessors tend to go with the *of*-genitive. Remember that `size_possessor` doesn't contain the length in characters but its logarithm. Therefore, the minimum size of 0 corresponds to a character length of 1. @tbl-sizes collects these examples for deeper inspection.

```{r}
#| label: tbl-sizes
#| tbl-cap: Concordances where the Possessor constituent has only one character.
#| tbl-cap-location: margin
cd %>% 
  filter(size_possessor == 0) %>% 
  as_tibble() %>% 
  select(match, possessor) %>% 
  kbl() %>% kable_paper(full_width = FALSE)
```

In the [last section of this document](#Easystats) we will learn what the `{report}` package can offer us.

Finally, we could also do some visualization. We could use barplots for the categorical variables, mosaic plots for the contingency table, histograms for the quantitative variables, scatterplots for the relationship between two quantitative variables or boxplots/violin plots to compare quantities and/or relate them to qualitative variables.

::: panel-tabset
### Barplot

The `position` argument controls whether the bars of different colors are stacked or put next to each other.

```{r}
#| label: fig-barplot
#| fig-cap: Example of a barplot. If you already have the counts for the y-axis, use `geom_col()` instead.

ggplot(cd, aes(x = gen_type, fill = possessor_type)) +
  geom_bar(position = position_dodge()) +
  scale_fill_manual(values = c("orange", "darkgreen")) +
  theme_minimal(base_size = 15)
```

### Mosaic plot

While the `{ggmosaic}` package offers a way of creating mosaic plots with `{ggplot2}`, `graphics::mosaic()` has the nice feature of shading the sections based on the residuals (with `shade = TRUE`). Red squares indicate that the frequency of a combination is *significantly* higher than expected (i.e. if the distribution was proportional), and blue squares, that the frequency is significantly lower than expected. In other words, you can already see if a pair of values of two categorical variables are "attracted" to each other. Here it is very clear that the Possessor types are not balanced across genitive variants: the common noun Possessor is much more attracted to the *of*-genitive, and the proper noun to the *s*-genitive.

```{r}
#| label: fig-mosaic
#| fig-cap: Example of a mosaic plot.

mosaicplot(gen_type ~ possessor_type,
           data = cd, shade = TRUE)
```

### Histograms

```{r}
#| label: fig-hist
#| fig-cap: Examples of histograms.
#| layout-ncol: 2
#| fig-subcap: 
#|    - "Size of the possessor slot"
#|    - "Size of the possessed slot"
#| message: false

ggplot(cd, aes(x = size_possessor)) +
  geom_histogram() +
  theme_minimal(base_size = 20)

ggplot(cd, aes(x = size_possessed)) +
  geom_histogram() +
  theme_minimal(base_size = 20)
```

### Scatterplots

In the second plot, we retrieve the original character lengths with `exp()`.

```{r}
#| label: fig-scatter
#| fig-cap: Example of a scatterplot.
#| layout-ncol: 2
#| fig-subcap: 
#|    - "Log values"
#|    - "Character length"

ggplot(cd, aes(x = size_possessor, y = size_possessed)) +
  geom_point(size = 3, alpha = 0.5) +
  theme_minimal(base_size = 20)

ggplot(cd, aes(x = exp(size_possessor), y = exp(size_possessed))) +
  geom_point(size = 3, alpha = 0.5) +
  theme_minimal(base_size = 20) +
  labs(x = "possessor", y = "possessed")
```

### Violin plots

In order to compare the ranges of `size_possessor` and `size_possessed` in a violin plot, we need to turn the dataset into a long-form version, so that the value of possessor/possessed is another variable. For that we use `pivot_longer()`, the complement of `pivot_wider()`, with the names of the columns that we want to change and a few useful arguments. First, for the selection of columns, we can also use `{dplyr}` selectors such as `starts_with()`, which selects all columns that start with a given string. Then, `names_to` and `values_to` indicate the new names for the columns to which the original *names* of the columns (here `size_possessor` and `size_possessed`) and their *values* will go. In this case, where `names_to = "slot", values_to = "size"`, each row of the original dataset will become two rows, and the columns `size_possessor` and `size_possessed` will have disappeared. Instead, there will be a column `slot` with values "size_possessor" and "size_possessed", and a column `size` with the original values of that row for `size_possessor` and `size_possessed` respectively. This allows us to use `slot` as a variable for the ggplot aesthetics. Finally, the `names_transform` argument allows us to transform the values that will go into the name-column, i.e. `slot`. In particular, this `stringr::str_remove()` call will remove the "size\_" prefix from `size_possessed` and `size_possessor`.

::: callout-note
#### Violins and boxes

If you're wondering why I use violin plots instead of boxplots, I invite you to read the article [*Same Stats, Different Graphs*](https://www.autodesk.com/research/publications/same-stats-different-graphs), by Justin Matejka and George Fitzmaurice.

And if you are interested in more advanced graphs, combining half-vioilin plots and half-boxplots or scatterplots, check out [the `{gghalves}` package](https://erocoar.github.io/gghalves/).
:::

```{r}
#| label: fig-violin
#| fig-cap: Example of a violin plot.

as_tibble(cd) %>% 
  select(starts_with("size_p"), gen_type) %>% 
  pivot_longer(starts_with("size"),
               names_to = "slot", values_to = "size",
               names_transform = ~ str_remove(.x, "size_")) %>% 
  ggplot(aes(x = slot, y = size, fill = gen_type)) +
  geom_violin() +
  scale_fill_manual(values = c("orange", "darkgreen")) +
  theme_minimal(base_size = 15)
```
:::

# Conditional inference tree

The first analysis we'll run is a conditional inference tree, which we'll obtain with `partykit::ctree()`. The first argument a formula, indicating on the left side of the tilde `~` the response variable, and on the right side the predictors, separated by `+`. The second argument is the dataset from which the variables are taken. The output is an object of class `party`, which can be stored on a file, if needed with `saveRDS()`.[`saveRDS()` is handy function from base R to write R objects to files; you can restore them with `readRDS()`. The downside is that only R can read these files, so if you have an object that could be stored in a more *interoperable* way, such as tab-separated files or images, then that is preferable.]{.aside}

```{r}
#| label: ctree
cd_ctree <- ctree(gen_type ~ possessor_type + size_possessor +
                    size_possessed + size_diff,
                  data = cd)
```

The easiest way to inspect a tree is by plotting it, as shown in @fig-ctree.

```{r}
#| label: fig-ctree
#| fig-cap: Conditional inference tree predicting the genitive alternation.
#| fig-cap-location: margin

plot(cd_ctree)
```

A conditional inference trees first tries to find a variable by which you can split the dataset in significantly different values, and then repeats the procedure iteratively until no other split returns significantly different groups. In the case of a classification task, i.e. when the response variable is categorical, the splits should have significantly different proportions of the success level.

Concretely, @fig-ctree shows that the first split is given by `possessor_type`, followed by `size_possessor` and then `size_possessed`.[The split variable at a given level doesn't have to be the same in all branches.]{.aside} That means that the largest significant difference is given between the observations where the Possessor is a common noun, in which the chances of *of*-genitive are substantially and significantly higher than those of *s*-genitive, against those in which the Possessor is a proper noun, which favor the *s*-genitive a bit more.

```{r}
#| label: ctree-data
#| include: false

sp1 <- cd_ctree[[2]]$node$split$breaks
sp2 <- cd_ctree[[3]]$node$split$breaks
sp3 <- cd_ctree[[7]]$node$split$breaks
```

Within the split based on `possessor_type`, the next significant split is given by `size_possessor`. When the Possessor is a common noun, if its size is larger than `r formatC(sp1, digits=3)` (`r exp(sp1)` characters) the chances of the *of*-genitive variant increase in relation to observations where the Possessor has fewer characters. Notice than in both cases the *of*-variant is dominant, proportion is significantly different. Within the group with `r exp(sp1)` characters or fewer, the `size_possessed` is now relevant and the *of*-genitive is favored more when it is `r formatC(sp2, digits=3)` or lower (`r exp(sp2)` characters). Notice, however, that the size of the node is much lower: whereas, within the occurrences with common noun possessor of `r exp(sp1)` characters or fewer, more than 75% of those with a shorter possessed constituent exhibit the *of*-genitive, in absolute numbers there are more longer possessed constituents with the *of*-genitive.

The right side of the plot shows the significant splits within the cases with proper nouns in the Possessor slot: this time, if the possessor is longer, with `size_possessor` larger than `r formatC(sp3, digits = 3)` (`r exp(sp3)` characters), the chances of the *of*-genitive variant also increase, becoming equal to those of the *s*-genitive within that group.

::: {.callout-tip collapse="true"}
## Plotting with `{ggparty}`

Plotting with the default method of a `party` object is straightforward and returns the information you want, but it doesn't give you much room for manipulation. If the plotting space is too small, the different parts of the plot may overlap --- if it's way too small, you might even get a warning.

The [`{ggparty}` package](https://cran.r-project.org/web/packages/ggparty/vignettes/ggparty-graphic-partying.html) offers a `{ggplot2}` extension to plot objects from the `{partykit}` package. Designing the plot is now more complicated, but it gives you more freedom, in case you want to manipulate the labels, colors, sizes, etc. For a conditional tree such as the one shown in @fig-ctree, `{ggparty}` might be a bit of an overkill, but @fig-ggparty is an example in which I change the colors in the barplots and color the node labels based on the splitting variables.

A downside of `{ggparty}` is that it doesn't automatically round the numerical values on the edges, but you can copy-paste the code in lines 5:12 to fix that in any other `ggparty` object (just make sure to change `cd_party` to the name of the `ggparty` object). I won't explain the rest of the code; if you're interested in the capabilities of the package, check out [the vignette](https://cran.r-project.org/web/packages/ggparty/vignettes/ggparty-graphic-partying.html).

```{r}
#| label: fig-ggparty
#| fig-cap: Conditional inference tree predicting the genitive alternation, plotted with ggparty.
#| code-line-numbers: true
library(ggparty)

cd_party <- ggparty(cd_ctree)

round_edges <- function(break_label) {
  rounded <- str_extract(break_label, "[0-9.]+") %>% 
    as.numeric() %>% 
    formatC(digits = 4)
  str_replace(break_label, "[0-9.]+", rounded)
}

cd_party$data$breaks_label <- map(cd_party$data$breaks_label, round_edges)

cd_party + # ggparty object
  geom_edge() + # add edges
  geom_edge_label(size = 4) + # add edge labels
  geom_node_label( # add and edit node labels
    mapping = aes(col = splitvar),
    line_list = list(
      aes(label = splitvar),
      aes(label = sprintf("p = %.2e", p.value))
    ),
    line_gpar = list(
      list(size = 15),
      list(size = 10)
    ),
    ids = "inner") +
  geom_node_label( # add node size to terminal nodes
    aes(label = paste("N= ", nodesize)),
    ids = "terminal"
  ) +
  geom_node_plot( # add plots
    gglist = list(
      geom_bar(aes(x = "", fill = gen_type),
               position = position_fill()),
      labs(y = "Proportion", x = NULL,
           fill = "Genitive variant"),
      scale_fill_manual(values = c("orange", "darkgreen")),
      theme_minimal(base_size = 15)),
    shared_axis_label = TRUE) +
  scale_color_brewer(palette = "Dark2", guide = "none") # set node labels color
```
:::

## Conditional trees in relation to other techniques

Unlike logistic regression analysis, conditional inference trees have the ability to deal with collinearity. This is the case where two variables basically do the same thing, or one variable has the same effect as a combination of two other variables. Therefore, we can use a conditional tree to see which predictors to use in the regression model. For example, in our dataset `size_difference` is redundant with `size_possessor` and `size_possessed` because it depends on them, and @fig-ctree tells us that we can safely discard `size_difference` because it doesn't have a significant effect.

On the other hand, conditional trees look at one variable at a time, and once a split is performed, the effect of the following variables is examined in relation to the split dataset. That is how the effect of `size_possessor` is different in the subset where `possessor_type` is "common" compared to the subset where `possessor_type` is "proper", and how `size_possessed` is only a relevant predictor in certain splits. In contrast, regression models look at the effect of each variable on the whole dataset, while accounting for the effects of the rest of the variables.

Finally, the conditional tree chooses the most relevant variable but does not tell us how close other variables were --- it seems that `size_possessor` is not as effective as `possessor_type` when splitting the full dataset, but how less effective is it? A way of finding out a ranking of importance of the variables, we can run random forests. This technique generates a large number of trees with different subsets of the dataset (different subsets of rows and of columns) and checks which variable takes the first place in each of the trees. This is a handy technique when we have many different variables, but it is not very time-efficient and therefore it's overkill for this dataset.

# Logistic regression analysis

Based on the results of the conditional tree, we can decide which predictors to use in the logistic regression model. We'll run it with `lme4::glmer()`, which was loaded when we loaded `{lme4Test}`. We'll obtain a summary of the results with the `summary()` method, which, if `{lme4Test}` is loaded, will include p-values.

`lme4::glmer()` is a function for "mixed-effects generalized linear models"; when run with the argument `family = binomial`, it runs mixed-effects logistic regression. A mixed effects model combines fixed effects and random effects. For a model with only fixed effects, we can use the base R function `glm(, family = binomial)`.[`lme4::glmer()` and `glm()` have counterparts for linear regression: `lme4::lmer()` and `lm()`, for mixed and fixed effects, respectively.]{.aside}

A fixed effect is a predictor that we are interested in, such as `possessor_type` and `size_possessor`. We want to know their effect on the response variable and we expect the values attested in our sample (our dataset) to represent the values that occur in the population (language and general). A random effect, in contrast, is a variable that we want to take into account but whose effect on the response variable we are not interested in. The values they take in the sample are only a sample of those that can be attested in the population, and might not be replicable in a different dataset. Examples in linguistic studies are the speaker, the topic and sometimes lexical effects. In our case, we can use the filename (`source`) and the register (`comp`) as a proxy for speakers and topic. We want to take them into account because it could be that a certain speaker prefers to use one variant over the other, but we cannot take them as fixed effects because it's unlikely that a study in a different dataset will have the same speakers. A different dataset, however, can certainly have the same values of `possessor_type` and comparable values in `size_possessor` and `size_possessed`.

::: {.callout-note collapse="true"}
## Extended formula notation

The first argument of `lme4::glmer()` is a formula, indicating the response variable and the predictors, as used for `partykit::ctree()`.

> `y ~ x1 + x2`: `y` in function of main effects `x1` and `x2`

However, for mixed-effects models we can use an extended notation to refer to interactions and random effects.

Interactions are cases where the effect of a variable depends on the effect on another variable. For example, that would be the case if a higher `size_possessor` favors *of* more intensely when `possessor_type` is "common" than when it is "proper". It could also be the case that a higher `size_possessor` favors *of* when `possessor_type` is "common" but disfavors it when it is "proper".

> `y ~ x1 + x2 + x1:x2`: `y` in function of main effect `x1` and `x2` and their interaction
>
> `y ~ x1 * x2`: `x1 * x2` is short for `x1 + x2 + x1:x2`
>
> `y ~ (x1 + x2 + x3)^2`: `y` in function of main effect `x1`, `x2`, and `x3` and all their two-way interactions; short for `y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3`

Random effects include random intercepts and random slopes. A random intercept would represent, for this dataset, that different speakers have different baseline preferences for one variant or the other. A random slope for a given predictor, in contrast, indicates that the same predictor may have different effects for different speakers, e.g. that the effect size of `size_possessor` changes somewhat across speakers.

> `y ~ x1 + (1 | r1)`: random intercepts for `r1`
>
> `y ~ x1 + (1 + x1 | r1)`: random intercepts and random `x1`-slopes for `r1`
>
> `y ~ x1 + (x1 | r1)`: same as (= short notation) previous formula

Random effects are always crossed but can also be nested, when the names of the levels make the nesting clear. In our dataset, `source` is nested within `comp` because the level names of `source` are unique across levels of `comp`: there is no file name that can belong to different registers. R is smart enough to realize this without us needing to make it explicit.

> `y ~ x1 + (1 | r1) + (1 | r2)`: random intercepts for both `r1` and `r2`
>
> `y ~ x1 + (1 | r1) + (1 | r1:r2)`: explicit nesting of `r2` in `r1`; the analysis will treat the same-name levels of `r2` within different levels of `r1` as different levels.
:::

## Maximal model

The first model we fit is a maximal model, using all the predictors suggested by the conditional tree. Because complex mixed-effect logistic regression models often have a hard time trying to converge, we add a `lme4::glmerControl()` call to help it. Here we set the optimizer to "bobyqa", but we could also increase the number of iteration with the argument `optCtr = list(maxfun = 100000)`.

```{r}
#| label: glmer-maximal
cd_glmer <- glmer(
    gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +
               (1 | comp) + (1 | source),
    family = "binomial", data = cd,
    control = glmerControl(optimizer = "bobyqa")
  )
summary(cd_glmer) %>%
  print(correlation = FALSE)
```

For the interpretation of the model, we want to look at:

-   The AIC value: the lower, the better. We can use it to compare models to each other.

-   The variance of the random effects: if it is not 0, it means we should keep them.

-   The p-values and estimates of the intercept and the effects. We do not care about non-significant effects, so we might want to try a new model without those predictors.

An useful way to check the significance of a predictor on the model is to run different models, each of which has removed one predictor, and compare them to the model with all the predictors. With the likelihood ratio test (LRT) we can see if removing anything from the model makes it significantly worse. We will do this with one handy function in the next section.

::: callout-caution
### Selection of predictors

While p-values and LRT tests are useful statistical guides when selecting or discarding predictors, your selection may also be theoretically motivated. Regardless of these values, if a certain predictor was significant in previous research, you can decide to keep it in your models.
:::

## Removing one predictor

The function `afex::mixed()` can take a model, run different iterations in which one predictor was removed, and compare if the model without a predictor is significantly worse than the model with all predictors. For that comparison we'll use likelihood ratio (LRT).[In linear models we would use an F-test instead.]{.aside}

```{r}
#| label: mixed-maximal
#| warning: true

cd_glmer_lrt <- mixed(
    gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +
               (1 | comp) + (1 | source),
    family = "binomial", data = cd, 
    method = "LRT",
    control = glmerControl(optimizer = "bobyqa")
  )
```

::: callout-note
The call to `mixed()` sends us a warning about the distribution of our numerical variables. This is useful but we don't need to heed it as long as we don't try to interpret the main effects of variables in interactions.
:::

```{r}
#| label: mixed-maximal-print
print(cd_glmer_lrt)
```

The output confirms that `possessor_type` and its interaction with `size_possessed` are significant. We can interpret as well that the other two interactions are not significant and must be dropped. However, we don't know if the main effect of `size_possessor` is significant, because it's being distorted by the interactions in the model. Therefore, we will try a new model with `possessor_type`, `size_possessor`, `size_possessed` and the interaction `possessor_type:size_possessed`.

::: callout-caution
### Random effects and variable selection

If you model an interaction, you also have to keep the main effects of the predictors that participate in the interaction.
:::

## Second model

Since the new model is not as complex, we don't need `glmerControl()` to help it converge.

```{r}
#| label: glmer-second
cd_glmer2 <- glmer(
    gen_type ~ possessor_type + size_possessor + size_possessed +
      possessor_type:size_possessed +
      (1 | comp) + (1 | source),
    family = "binomial", data = cd
  )

summary(cd_glmer2) %>%
  print(correlation = FALSE)
```

In this model, the AIC value is slightly lower (so, the model is better) and the variance of the random effects is larger than zero. In addition, all the current predictors and the interaction are significant.

We could still run `afex::mixed()` to check whether removing any predictor would make the model worse.

```{r}
#| label: mixed-second
#| message: false

cd_glmer2_lrt <- mixed(
    gen_type ~ possessor_type + size_possessor + size_possessed +
      possessor_type:size_possessed +
      (1 | comp) + (1 | source),
    family = "binomial", data = cd,
    method = "LRT")
print(cd_glmer2_lrt)
```

Indeed, the only predictor that we could "safely" remove is `size_possessed`, but we need it in order to keep the significant interaction in which it participates.

## Interpretation

```{r}
#| label: print-odds
#| include: false
cd_coef <- coefficients(summary(cd_glmer2))[,"Estimate"]
print_odds <- function(coefficient) {formatC(exp(coefficient), digits = 2)}
```

One of the first things we see in the output is that, when the Possessor is a proper noun, the chances of the *s*-variant are significantly higher --- about `r print_odds(cd_coef['possessor_typeproper'])` times more likely than the alternative, all other things being equal.

Moreover, the larger the Possessed constituent, the higher the chances of the *s*-variant, and the larger the Possessor constituent, the lower its chances. This is consistent with the theoretical assumption that the longer constituent is placed at the end: the longer a constituent, the higher the chances of a construction that places it at the end.

However, that effect of the size of the Possessed constituent is valid when the Possessor slot is a common noun, which in general favors the *of* construction (as indicated by the Intercept, although it is not significant). In contrast, when the Possessor is a proper noun (which favors the *s*-construction, all other things being equal), a larger Possessed constituent actually favors the *of*-construction.

::: callout-tip
You can always retrieve the coefficients with `coefficients(summary(model))`, which is a dataframe. For example `coefficients(summary(cd_glmer2))["possessor_typeproper","Estimate"]` returns the estimate coefficient of `possessor_type`. Adding `exp()`, you can retrieve the odds of the *s*-variant over the *of*-variant when the Possessor is a proper noun, all other things being equal.
:::

## Plotting effects

@fig-effects plots the effects in the models. @fig-effects-1 shows that the probabilities of the *s*-genitive decrease as the size of the Possessor constituent increases. These probabilities take the other predictors into account by assuming their typical cases.

@fig-effects-2 illustrates the interaction between the type of the Possessor and the size of the Possessed constituent. As we saw in the printed output of the model, when the Possessor is a common noun, a larger size of the Possessed constituent increases the probabilities of the *s*-genitive, whereas the effect inverts when the Possessor is a proper noun. In addition, we see that the probability of the *s*-genitive is always higher for the proper noun, regardless of the size of the Possessed constituent.

```{r}
#| label: fig-effects
#| fig-cap: Effect plots predicting the probability of the *s*-genitive.
#| fig-subcap: 
#|   - "Effect of the size of the possessor slot."
#|   - "Interaction between size of the possessed slot and type of possessor."
#| layout-ncol: 2
#| message: false

ggpredict(cd_glmer2, "size_possessor") %>%
  plot()

ggpredict(cd_glmer2, c("size_possessed", "possessor_type")) %>%
  plot()
```

# Easystats {#easystats}

In this final section we will showcase some functions from the `{easystats}` group of packages for reporting models.

```{r}
#| label: easystats
#| message: false
library(easystats)
```

::: panel-tabset
## Report

::: callout-caution
### Reporting random effects

Notice that the `{merDeriv}` package needs to be installed to compute confidence intervals for random effect parameters.
:::

```{r}
#| label: report-text
#| message: false
report(cd_glmer2)
```

## Parameters

```{r}
#| label: tbl-parameters
#| tbl-cap: Fixed effect parameters.

model_parameters(cd_glmer2,
                 effects = "fixed") %>% 
  print_md()
```

```{r}
#| label: fig-parameters
#| fig-cap: Coefficients of the fixed effect parameters.

model_parameters(cd_glmer2,
                 effects = "fixed") %>% 
  plot()
```

## Performance

The outliers can be further inspected with `cd %>% filter(check_outliers(cd_glmer2))`.

```{r}
#| label: fig-collinearity
#| fig-cap: Performance checks on minimal model
#| fig-subcap: 
#|   - "VIF, or collinearity"
#|   - "Outliers"
#| layout-ncol: 2
check_collinearity(cd_glmer2) %>% 
  plot()

check_outliers(cd_glmer2) %>% plot()
```

## Comparison

The `{easystats}` packages also offer ways of comparing different models. `parameters::compare_models()` compares the effect size of the parameters, whereas `performance::compare_performance()` compares them across metrics. Both objects have `plot()` methods, but the one for the performance comparison is currently failing.

```{r}
#| label: comparisons
#| message: false
param_comparison <- compare_models(
  "maximal" = cd_glmer, "minimal" = cd_glmer2
  )
performance_comparison <- compare_performance(
  "maximal" = cd_glmer, "minimal" = cd_glmer2,
  metrics = "common"
  )
```

```{r}
#| label: tbl-paramcomp
#| tbl-cap: Parameter comparison between maximal and minimal model.

print_md(param_comparison)
```

```{r}
#| label: fig-paramcomp
#| fig-cap: Parameter comparison between maximal and minimal model.

plot(param_comparison)
```

```{r}
#| label: tbl-perfcomp
#| tbl-cap: Parameter comparison between maximal and minimal model.

print_md(performance_comparison)
```
:::
