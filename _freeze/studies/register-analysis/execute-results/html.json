{
  "hash": "e683c9c15ab8d1b253b2d68e8d269e8d",
  "result": {
    "markdown": "---\ntitle: \"Simple register analysis (multidimensional analysis) of brown TEI\"\nnumber-sections: true\nsubtitle: \"Based on Dirk Speelman's course material\"\nbibliography: ../slides/bibliography.bib\n---\n\n\nThis document shows how to perform a simple register analysis with Factor Analysis on the Brown corpus. For that purpose we will use the TEI-compliant XML version of the corpus, which will allow us to extract a bit more information from the corpus than other formats. @sec-xml will show what the XML file looks like and how we can read and parse it with `{xml2}` and `{mclm}`. @sec-ann will build on that knowledge to compute a number of frequencies as variables for the Factor Analysis. As we do so, we will illustrate how to take advantage of the possibility of having any type of data in a tibble column. Finally @sec-fa will show the Factor Analysis itself with `factanal()` and illustrate some plotting options.\n\n# Introduction\n\nIn this study we will use the 500 files of the Brown corpus in XML format. We will compute the frequencies of different linguistic features as described in @sec-features and use Factor Analysis to extract theoretically relevant underlying dimensions. We will then try to describe different components of the Brown corpus (@tbl-components) in terms of these dimensions.\n\n\n::: {#tbl-components .cell tbl-cap='List of components of the Brown corpus with the registers they represent, number of files and number of tokens.'}\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Section </th>\n   <th style=\"text-align:right;\"> Component </th>\n   <th style=\"text-align:left;\"> Register </th>\n   <th style=\"text-align:right;\"> Files </th>\n   <th style=\"text-align:right;\"> Tokens </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;vertical-align: middle !important;\" rowspan=\"9\"> Informative </td>\n   <td style=\"text-align:right;\"> a </td>\n   <td style=\"text-align:left;\"> Press-reportage </td>\n   <td style=\"text-align:right;\"> 44 </td>\n   <td style=\"text-align:right;\"> 100 554 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> b </td>\n   <td style=\"text-align:left;\"> Press-editorial </td>\n   <td style=\"text-align:right;\"> 27 </td>\n   <td style=\"text-align:right;\"> 61 604 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> c </td>\n   <td style=\"text-align:left;\"> Press-reviews </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;\"> 40 704 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> d </td>\n   <td style=\"text-align:left;\"> Religion </td>\n   <td style=\"text-align:right;\"> 17 </td>\n   <td style=\"text-align:right;\"> 39 399 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> e </td>\n   <td style=\"text-align:left;\"> Skills &amp; hobbies </td>\n   <td style=\"text-align:right;\"> 36 </td>\n   <td style=\"text-align:right;\"> 82 345 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> f </td>\n   <td style=\"text-align:left;\"> Popular Lore </td>\n   <td style=\"text-align:right;\"> 48 </td>\n   <td style=\"text-align:right;\"> 110 299 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> g </td>\n   <td style=\"text-align:left;\"> Belles Lettres, Biography, Memoirs, etc. </td>\n   <td style=\"text-align:right;\"> 75 </td>\n   <td style=\"text-align:right;\"> 173 096 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> h </td>\n   <td style=\"text-align:left;\"> Miscellaneous </td>\n   <td style=\"text-align:right;\"> 30 </td>\n   <td style=\"text-align:right;\"> 70 117 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> j </td>\n   <td style=\"text-align:left;\"> Learned </td>\n   <td style=\"text-align:right;\"> 80 </td>\n   <td style=\"text-align:right;\"> 181 888 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;vertical-align: middle !important;\" rowspan=\"6\"> Imaginative </td>\n   <td style=\"text-align:right;\"> k </td>\n   <td style=\"text-align:left;\"> General Fiction </td>\n   <td style=\"text-align:right;\"> 29 </td>\n   <td style=\"text-align:right;\"> 68 488 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> l </td>\n   <td style=\"text-align:left;\"> Mystery and Detective Fiction </td>\n   <td style=\"text-align:right;\"> 24 </td>\n   <td style=\"text-align:right;\"> 57 169 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> m </td>\n   <td style=\"text-align:left;\"> Science Fiction </td>\n   <td style=\"text-align:right;\"> 6 </td>\n   <td style=\"text-align:right;\"> 14 470 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> n </td>\n   <td style=\"text-align:left;\"> Adventure and Western Fiction </td>\n   <td style=\"text-align:right;\"> 29 </td>\n   <td style=\"text-align:right;\"> 69 342 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> p </td>\n   <td style=\"text-align:left;\"> Romance and Love Stories </td>\n   <td style=\"text-align:right;\"> 29 </td>\n   <td style=\"text-align:right;\"> 70 022 </td>\n  </tr>\n  <tr>\n   \n   <td style=\"text-align:right;\"> r </td>\n   <td style=\"text-align:left;\"> Humor </td>\n   <td style=\"text-align:right;\"> 9 </td>\n   <td style=\"text-align:right;\"> 21 695 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n## Features of interest {#sec-features}\n\nFor this simple analysis we will annotate 13 numerical variables, which is quite modest compared to @biber_1988:\n\nName | Value\n-- | --------\n`ttr` | Type token ratio (number of types divided by number of tokens)^[The type token ratio of different texts can only be compared if they have similar lengths, which is the case with the Brown corpus.]\n`word_len` | Average word length, in characters\n`p_mw` | Proportion of combined tags; typically clitics as in *she's*\n`p_c` | Proportion of punctuation characters\n`p_ppss` | Proportion of personal pronouns nominative, besides third person singular\n`p_noun` | Proportion of nouns\n`p_bigr` | Number of unique word bigrams, relative to document size\n`p_nomin` | Proportion of nominalisations (nouns ending in *-tion*, *-ment*, *-ness*, or *-ity*)\n`p_pobi` | Number of unique pos tag bigrams, relative to document size\n`p_adj` | Proportion of adjectives\n`p_neg` | Number of negations, relative to document size\n`p_adv` | Proportion of adverbs\n`p_qual` | Number of qualifiers, relative to document size\n\nThe idea is that these variables are relatively straightforward to annotate --- with some programming, they can be easily extracted automatically. The theoretically relevant dimensions, instead, are much harder to operationalize: Factor Analysis is meant to extract these dimensions from the operationalizable linguistic features.\n\n# Setup\n\nThe first step is to load the libraries: we will use `{tidyverse}`, `{mclm}`, `{here}` and, for some XML processing, `{xml2}`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(mclm)\nlibrary(here)\nlibrary(xml2)\ntheme_set(theme_minimal())\n```\n:::\n\n\nThe directory with the corpus, for the purposes of this document, is defined below. If you follow along, make sure to set `corpus_directory` to the path to wherever your copy of the corpus is stored.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_directory <- here::here(\"studies\", \"_corpora\", \"brown_tei\")\n```\n:::\n\n\nThe corpus directory contains some documentation files other than the corpus files themselves, so we need to filter them out when creating our `fnames` object. After visual inspection of the contents of the folder we can set up a regular expression that matches the names of the files we want: we will request for files ending in the following sequence: slash, one lowercase letter, two digits, \".xml\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntei_fnames <- get_fnames(corpus_directory) %>% \n  keep_re(r\"--[(?xi) / [a-z] \\d\\d [.]xml $ ]--\")\n\nprint(tei_fnames, n = 6, hide_path = corpus_directory)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFilename collection of length 500\n  filename\n  --------\n1  a01.xml\n2  a02.xml\n3  a03.xml\n4  a04.xml\n5  a05.xml\n6  a06.xml\n...\n```\n:::\n:::\n\n\n# Read from XML {#sec-xml}\n\nThe first 12 lines of the first file in `tei_fnames` look like this:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n<TEI xmlns=\"http://www.tei-c.org/ns/\n1.0\"><teiHeader><fileDesc><titleStmt><title>Sample A01 from The Atlanta\nConstitution</title><title type=\"sub\"> November 4, 1961, p.1 \"Atlanta\nPrimary ...\"    \n    \"Hartsfield Files\"    \n    August 17, 1961, \"Urged strongly ...\"    \n    \"Sam Caldwell Joins\"    \n    March 6,1961, p.1 \"Legislators Are Moving\" by Reg Murphy    \n    \"Legislator to fight\" by Richard Ashworth    \n    \"House Due Bid...\"    \n    p.18 \"Harry Miller Wins...\"    \n    </title></titleStmt><editionStmt><edition>A part of the XML version of the Brown\nCorpus</edition></editionStmt><extent>1,988 words 431 (21.7%) quotes 2 symbols</\nextent><publicationStmt><idno>A01</idno><availability><p>Used by permission\nof The Atlanta ConstitutionState News Service (H), and Reg Murphy (E).</p></\navailability></publicationStmt><sourceDesc><bibl> The Atlanta Constitution</\nbibl></sourceDesc></fileDesc><encodingDesc><p>Arbitrary Hyphen: multi-million\n[0520]</p></encodingDesc><revisionDesc><change when=\"2008-04-27\">Header auto-\ngenerated for TEI version</change></revisionDesc></teiHeader>    \n    <text xml:id=\"A01\" decls=\"A\">    \n    <body><p><s n=\"1\"><w type=\"AT\">The</w> <w type=\"NP\" subtype=\"TL\">Fulton</w>\n<w type=\"NN\" subtype=\"TL\">County</w> <w type=\"JJ\" subtype=\"TL\">Grand</w> <w\ntype=\"NN\" subtype=\"TL\">Jury</w> <w type=\"VBD\">said</w> <w type=\"NR\">Friday</\nw> <w type=\"AT\">an</w> <w type=\"NN\">investigation</w> <w type=\"IN\">of</w>\n<w type=\"NPg\">Atlanta's</w> <w type=\"JJ\">recent</w> <w type=\"NN\">primary</\nw> <w type=\"NN\">election</w> <w type=\"VBD\">produced</w> <c type=\"pct\">``</\nc> <w type=\"AT\">no</w> <w type=\"NN\">evidence</w> <c type=\"pct\">''</c> <w\ntype=\"CS\">that</w> <w type=\"DTI\">any</w> <w type=\"NNS\">irregularities</w> <w\ntype=\"VBD\">took</w> <w type=\"NN\">place</w> <c type=\"pct\">.</c> </s>    \n    </p>    \n    \n```\n:::\n:::\n\n\nAn XML file has a hierarchical structure, with elements indicated by tags and sometime containing other elements. For example, we could have a paragraph, starting with the `<p>` tag and ending with the following `</p>`, which can contain a series of sentences preceded by `<s>` and ending in `</s>`, and each can contain a series of words, preceded by `<w>` tags, each ending in the following `</w>` tag.^[Tags can also open and close at once, if they have no \"content\", e.g. `<single/>`.] The opening tag can also contain attribute-value pairs, e.g. `<w type=\"IN\">of</w>` corresponds to a word (indicated by the `w` tag) with content \"of\" and an attribute \"type\" with value \"IN\" ---in this case, the attribute points to the part of speech. Some words also have a \"subtype\" attribute: `<w type=\"NP\" subtype=\"TL\">Fulton</w>` in this corpus is equivalent to \"fulton/np-tl\" in the other version of the Brown corpus.\n\nThe first tag in these files is `<TEI>` with an \"xmlns\" attribute, whose value is an URL. This URL points to the \"namespace\", the standards to which the annotation applies, which indicate what each type of tag means.\n\nBecause the file is hierarchical, we can read it with special parsers that interpret the hierarchical structure. Concretely, we will use `xml2::read_xml()`, which returns an object of class `xml_document`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntei1 <- read_xml(tei_fnames[[1]])\ntei1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_document}\n<TEI xmlns=\"http://www.tei-c.org/ns/1.0\">\n[1] <teiHeader>\\n  <fileDesc>\\n    <titleStmt>\\n      <title>Sample A01 from  ...\n[2] <text xml:id=\"A01\" decls=\"A\">\\n  <body>\\n    <p>\\n      <s n=\"1\">\\n       ...\n```\n:::\n:::\n\n\nWe could extract the first sentence of the corpus with `xml_find_first()`, providing the XPath expression \"//d1:s\", i.e. an `s` element from namespace `d1`. In `{xml2}`, the first namespace in a file is called `d1`, as you can see with `xml_ns()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxml_ns(tei1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nd1 <-> http://www.tei-c.org/ns/1.0\n```\n:::\n\n```{.r .cell-code}\nfirst_sentence <- xml_find_first(tei1, \"//d1:s\")\nprint(first_sentence, max_n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_node}\n<s n=\"1\">\n [1] <w type=\"AT\">The</w>\n [2] <w type=\"NP\" subtype=\"TL\">Fulton</w>\n [3] <w type=\"NN\" subtype=\"TL\">County</w>\n [4] <w type=\"JJ\" subtype=\"TL\">Grand</w>\n [5] <w type=\"NN\" subtype=\"TL\">Jury</w>\n [6] <w type=\"VBD\">said</w>\n [7] <w type=\"NR\">Friday</w>\n [8] <w type=\"AT\">an</w>\n [9] <w type=\"NN\">investigation</w>\n[10] <w type=\"IN\">of</w>\n...\n```\n:::\n:::\n\n\nThe output is an `xml_nodeset` object, which contains `xml_node` objects; `xml_children()` can then retrieve the elements inside it. Further, `xml_text()` retrieves the contents of the tags as text and `xml_attr()`, the values of specific attributes. The output of these functions are simple character vectors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsentence_contents <- xml_children(first_sentence)\nprint(sentence_contents, max_n = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (25)}\n [1] <w type=\"AT\">The</w>\n [2] <w type=\"NP\" subtype=\"TL\">Fulton</w>\n [3] <w type=\"NN\" subtype=\"TL\">County</w>\n [4] <w type=\"JJ\" subtype=\"TL\">Grand</w>\n [5] <w type=\"NN\" subtype=\"TL\">Jury</w>\n [6] <w type=\"VBD\">said</w>\n [7] <w type=\"NR\">Friday</w>\n [8] <w type=\"AT\">an</w>\n [9] <w type=\"NN\">investigation</w>\n[10] <w type=\"IN\">of</w>\n...\n```\n:::\n\n```{.r .cell-code}\nxml_text(sentence_contents)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"The\"            \"Fulton\"         \"County\"         \"Grand\"         \n [5] \"Jury\"           \"said\"           \"Friday\"         \"an\"            \n [9] \"investigation\"  \"of\"             \"Atlanta's\"      \"recent\"        \n[13] \"primary\"        \"election\"       \"produced\"       \"``\"            \n[17] \"no\"             \"evidence\"       \"''\"             \"that\"          \n[21] \"any\"            \"irregularities\" \"took\"           \"place\"         \n[25] \".\"             \n```\n:::\n\n```{.r .cell-code}\nxml_attr(sentence_contents, \"type\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"AT\"  \"NP\"  \"NN\"  \"JJ\"  \"NN\"  \"VBD\" \"NR\"  \"AT\"  \"NN\"  \"IN\"  \"NPg\" \"JJ\" \n[13] \"NN\"  \"NN\"  \"VBD\" \"pct\" \"AT\"  \"NN\"  \"pct\" \"CS\"  \"DTI\" \"NNS\" \"VBD\" \"NN\" \n[25] \"pct\"\n```\n:::\n:::\n\n\nWith this knowledge, we can now read the files in the corpus and obtain the information we need for our Factor Analsyis. We will collect different kinds of tokens with `mclm::find_xpath()`, process them with `{xml2}` functions, count frequencies and generate a matrix.\n\n# Annotate the dataset {#sec-ann}\n\nIn this section we will go step by step through the automatic annotation procedure and thus illustrate how to use tibbles and `{purrr}` functions for this purpose. If you want to see the final code directly, you may skip to @sec-full.\n\n::: callout-important\n### tibbles and `{purrr}`\n\nThere are other ways of achieving the same result next to what we will do in this section, i.e. by opening one file at a time, processing it and storing the output, either with a `map()`-like function or a `for` loop. However, in this case we will work with a tibble to show how the columns can have different kinds of data and how to work with such a format. In particular, we will work with `purrr::map()` and a few variations --- these functions take a list or vector and a function^[It can also be an index. For example, `map(list_of_lists, 1)` will return the first element of each list in `list_of_lists`.] as input, run the same function on each element of the list, and return the result as either a list or a vector. `map()` always returns a list; `map_dbl()` returns a numerical vector, `map_chr()` returns a character vector, etc. This is useful both when trying to apply the same function to all elements of a list or when you want to apply a function to all elements of a vector but the function is not vectorized (it cannot take vectors with length larger than 1).\n:::\n\nAs a first step to generating our frequencies, we will create a tibble based on the filenames. Each piece of information we will retrieve, whatever its type, will be stored in its own column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- as_tibble(tei_fnames)\n```\n:::\n\n\n## Read from XML\n\nFirst, we will read the files by applying `read_xml()` to each element of the `filename` column in `d` via `map()`. This results in a column that is a list of `xml_document` objects. `d$xml[[1]]` is the same as `read_xml(d$filename[[1]])`.\n\n::: callout-tip\n### Example\n\n`map(some_list, tolower)` returns a list in which the first element is `tolower(some_list[[1]])`, the second is `tolower(some_list[[2]])`, etc.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% mutate(xml = map(filename, read_xml))\nd %>% mutate(filename = short_names(filename)) # just hiding the path when printing\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 2\n   filename xml       \n   <fnames> <list>    \n 1 a01      <xml_dcmn>\n 2 a02      <xml_dcmn>\n 3 a03      <xml_dcmn>\n 4 a04      <xml_dcmn>\n 5 a05      <xml_dcmn>\n 6 a06      <xml_dcmn>\n 7 a07      <xml_dcmn>\n 8 a08      <xml_dcmn>\n 9 a09      <xml_dcmn>\n10 a10      <xml_dcmn>\n# … with 490 more rows\n```\n:::\n\n```{.r .cell-code}\nd$xml[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_document}\n<TEI xmlns=\"http://www.tei-c.org/ns/1.0\">\n[1] <teiHeader>\\n  <fileDesc>\\n    <titleStmt>\\n      <title>Sample A01 from  ...\n[2] <text xml:id=\"A01\" decls=\"A\">\\n  <body>\\n    <p>\\n      <s n=\"1\">\\n       ...\n```\n:::\n:::\n\n\n## Extract tags with `find_xpath()`\n\nIn order to extract lists of tags from each of the file, we will use `mclm::find_xpath()`. We need to provide a namespace definition, so we will create a column `ns` to store the output of `xml_ns()` on each file. Then we will create four columns with the output of `find_xpath()` and different XPath patterns: one to collect the word tags `w`, one to collect the multiword (or \"combined tags\") `mw`, one to collect the punctuation tags `c` and one to collect all of them, i.e. all the tokens.  For this purpose we need to combine both the elements in the `xml` column and the elements in the newly made `ns` column. The function to achieve this is `map2`, which applies a function to each element of two paired lists of the same length.\n\n::: callout-tip\n### Example\n\n`map2(list1, list2, mean)` returns a list whose first element is `mean(list1[[1]], list2[[1]])`, the second element is `mean(list1[[2]], list2[[2]])`, etc.\n:::\n\nThe call to `find_xpath()` takes two positional arguments (the `xml_document` and an XPath pattern) and a few named arguments, among which `namespaces`. In our call inside `map2()`, we can use a **formula**, which allows us to call the function and replace the argument corresponding to the first list with `.x` and the one corresponding to the second list `.y`.\n\n::: callout-tip\n### Example\n\n`map2(list1, list2, mean)` can also be written as `map2(list1, list2, ~ mean(.x, .y))`.\n\nThis notation is useful when the elements in the lists do not correspond to the positional arguments of the function.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% \n  select(-filename) %>%\n  mutate(\n    ns = map(xml, xml_ns),\n    token_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w | //d1:mw | //d1:c\", namespaces = .y)),\n    word_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w\", namespaces = .y)),\n    mw_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:mw\", namespaces = .y)),\n    punctuation_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:c\", namespaces = .y))\n    )\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 6\n   xml        ns             token_tags word_tags  mw_tags    punctuation_tags\n   <list>     <list>         <list>     <list>     <list>     <list>          \n 1 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 2 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 3 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 4 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 5 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 6 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 7 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 8 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n 9 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n10 <xml_dcmn> <xml_nmsp [1]> <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>      \n# … with 490 more rows\n```\n:::\n:::\n\n\n::: aside\nAfter this, we don't need the `filename` column anymore --- in this stepwise illustration of the process, I will be removing obsolete columns, but when running everything at once, as shown in @sec-full, we won't be doing that; we'll just select the useful columns at the end.\n:::\n\nEach new column is a **list** with elements of different kinds. The elements in the `token_tags` and `mw_tags` columns, for example, are `xml_nodeset` objects. Below we can see the first element of each of these columns, i.e. the tokens and the combined tags of the first file in the corpus.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nprint(d$token_tags[[1]], max_n = 10) # left side\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (2242)}\n [1] <w type=\"AT\">The</w>\n [2] <w type=\"NP\" subtype=\"TL\">Fulton</w>\n [3] <w type=\"NN\" subtype=\"TL\">County</w>\n [4] <w type=\"JJ\" subtype=\"TL\">Grand</w>\n [5] <w type=\"NN\" subtype=\"TL\">Jury</w>\n [6] <w type=\"VBD\">said</w>\n [7] <w type=\"NR\">Friday</w>\n [8] <w type=\"AT\">an</w>\n [9] <w type=\"NN\">investigation</w>\n[10] <w type=\"IN\">of</w>\n...\n```\n:::\n\n```{.r .cell-code}\nd$mw_tags[[1]] # right side\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n{xml_nodeset (3)}\n[1] <mw pos=\"DOD *\">didn't </mw>\n[2] <mw pos=\"DOD *\">didn't </mw>\n[3] <mw pos=\"BEDZ *\">wasn't </mw>\n```\n:::\n:::\n\n\n## Retrieve text with `xml_text()`\n\nWith the new columns we can create `tokens` objects by retrieving the contents of the tags (for word form tokens) or the POS-tags. The columns `all_tokens` and `words` are created by applying a chain of functions to the elements of the `token_tags` and `word_tags` columns: `xml_text()` to obtain the content, `tolower()` to turn it to lower case, `cleanup_spaces()` to remove any spaces that might be accidentally included in the token, and `as_tokens()` to turn the resulting character vector into a `tokens` object. Therefore, the columns `all_tokens` and `words` will be lists of `tokens` objects. Below is an example of the chain of functions as applied to the first element of `d$token_tags`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd$token_tags[[1]] %>% xml_text() %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"The\"    \"Fulton\" \"County\" \"Grand\"  \"Jury\"   \"said\"  \n```\n:::\n\n```{.r .cell-code}\nd$token_tags[[1]] %>% xml_text() %>% tolower() %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"the\"    \"fulton\" \"county\" \"grand\"  \"jury\"   \"said\"  \n```\n:::\n\n```{.r .cell-code}\nd$token_tags[[1]] %>% xml_text() %>% tolower() %>% cleanup_spaces() %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"the\"    \"fulton\" \"county\" \"grand\"  \"jury\"   \"said\"  \n```\n:::\n\n```{.r .cell-code}\nd$token_tags[[1]] %>% xml_text() %>% tolower() %>% as_tokens() %>% print(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 2242\nidx         token\n--- -------------\n  1           the\n  2        fulton\n  3        county\n  4         grand\n  5          jury\n  6          said\n  7        friday\n  8            an\n  9 investigation\n 10            of\n...\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>%\n  select(-xml, -ns) %>% \n  mutate(\n    all_tokens = map(token_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens()),\n    words = map(word_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens())\n  )\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 6\n   token_tags word_tags  mw_tags    punctuation_tags all_tokens       words   \n   <list>     <list>     <list>     <list>           <list>           <list>  \n 1 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,242]> <tokens>\n 2 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,277]> <tokens>\n 3 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,275]> <tokens>\n 4 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,216]> <tokens>\n 5 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,244]> <tokens>\n 6 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,263]> <tokens>\n 7 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,270]> <tokens>\n 8 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,187]> <tokens>\n 9 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,234]> <tokens>\n10 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens [2,282]> <tokens>\n# … with 490 more rows\n```\n:::\n:::\n\n\n::: aside\nNow that we have extracted the terms that we are interested in, we don't really need the `xml` and `ns` columns anymore,\n:::\n\nThe lists below don't look very different because there are no punctuation marks in the first 10 tokens.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nprint(d$all_tokens[[1]], 10) # left\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 2242\nidx         token\n--- -------------\n  1           the\n  2        fulton\n  3        county\n  4         grand\n  5          jury\n  6          said\n  7        friday\n  8            an\n  9 investigation\n 10            of\n...\n```\n:::\n\n```{.r .cell-code}\nprint(d$words[[1]], 10) # right\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 1985\nidx         token\n--- -------------\n  1           the\n  2        fulton\n  3        county\n  4         grand\n  5          jury\n  6          said\n  7        friday\n  8            an\n  9 investigation\n 10            of\n...\n```\n:::\n:::\n\n\n## Retrieve POS-tags with `xml_attr()`\n\nIn order to collect the POS-tags, we could simply apply `xml_attr()` in the same way that we were calling `xml_text()`, but the situation is a bit more complicated. As it turns out, the attribute indicating the part-of-speech for `w` and `c` elements is \"type\", but in a `mw` element it's called \"pos\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxml_attr(d$token_tags[[1]], \"type\") %>% head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"AT\"  \"NP\"  \"NN\"  \"JJ\"  \"NN\"  \"VBD\"\n```\n:::\n\n```{.r .cell-code}\nxml_attr(d$mw_tags[[1]], \"type\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] NA NA NA\n```\n:::\n\n```{.r .cell-code}\nxml_attr(d$mw_tags[[1]], \"pos\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"DOD *\"  \"DOD *\"  \"BEDZ *\"\n```\n:::\n:::\n\n\nAs a consequence, we need to apply a more complicated workflow to create the `pos_codes` column, i.e. an equivalent of the `all_tokens` column but with POS-tags instead of word forms. For each element of `token_tags` (lets call them `toks`), we will do the following:\n\n- Extract the \"type\" attribute, which will contain the POS-tag for the `w` and `c`elements but `NA` for `mw` elements, and store it in a `most_pos` vector: `most_pos <- xml_attr(toks, \"type\")`.\n\n- Extract the \"pos\" attribute, which will contain the POS-tag for `mw` elements but `NA` for the rest, and store it in a `mw_pos` element: `mw_pos <- xml_attr(toks, \"pos\")`.\n\n- Replace the `NA` values in `most_pos` (`most_pos[is.na(most_pos)]`) with the non-`NA` values in `mw_pos` (`mw_pos[!is.na(mw_pos)]`).\n\n- Turn the resulting, fixed `most_pos` into a `tokens` object with `as_tokens(most_pos)`.\n\nIn order to provide such a workflow with `map()`, we will do it with an anonymous function defined as `function(argument) { the code... }`.\n\n::: callout-tip\n### Example\n\n`map(some_list, tolower)` is the same as `map(some_list, ~ tolower(.x)`) and the same as `map(some_list, function(list_element) { tolower(list_element) })`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>%\n  mutate(\n    pos_codes = map(token_tags, function(toks) {\n      most_pos <- xml_attr(toks, \"type\")\n      mw_pos <- xml_attr(toks, \"pos\")\n      most_pos[is.na(most_pos)] <- mw_pos[!is.na(mw_pos)]\n      as_tokens(most_pos) # return value\n      })\n    )\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 7\n   token_tags word_tags  mw_tags    punctuation_tags all_tok…¹ words    pos_co…²\n   <list>     <list>     <list>     <list>           <list>    <list>   <list>  \n 1 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 2 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 3 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 4 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 5 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 6 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 7 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 8 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n 9 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n10 <xml_ndst> <xml_ndst> <xml_ndst> <xml_ndst>       <tokens>  <tokens> <tokens>\n# … with 490 more rows, and abbreviated variable names ¹​all_tokens, ²​pos_codes\n```\n:::\n\n```{.r .cell-code}\nprint(d$pos_codes[[1]], 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 2242\nidx token\n--- -----\n  1    AT\n  2    NP\n  3    NN\n  4    JJ\n  5    NN\n  6   VBD\n  7    NR\n  8    AT\n  9    NN\n 10    IN\n...\n```\n:::\n:::\n\n\n## Build bigrams\n\nThe following step, before we can compute the frequencies we want, is to obtain bigrams. To each element `x` of `all_tokens` (for word form bigrams) and `pos_codes` (for POS-tags bigrams) we will implement the following code: `paste(x, c(x[-1], \"EOF\"), sep = \"|\")`. As shown below, `x[-1]` returns the vector `x` minus its first element, `c(x[-1], \"EOF\")` appends \"EOF\" (which stands for End Of File) to the vector, and `paste()` glues both vectors. `paste()` is a vectorized function, so it will automatically \"paste\" the first element of the first vector with the first element of the second vector, the second element of each vector, the third element of each vector, etc. The `sep` argument lets us decide what character will be used when joining those elements.^[In addition, if you wanted to join the elements *of a vector* together, the `collapse` argument lets you decide how.]\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- head(letters, 10)\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"a\" \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n```\n:::\n\n```{.r .cell-code}\nx[-1]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"b\" \"c\" \"d\" \"e\" \"f\" \"g\" \"h\" \"i\" \"j\"\n```\n:::\n\n```{.r .cell-code}\nc(x[-1], \"EOF\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"b\"   \"c\"   \"d\"   \"e\"   \"f\"   \"g\"   \"h\"   \"i\"   \"j\"   \"EOF\"\n```\n:::\n\n```{.r .cell-code}\npaste(x, c(x[-1], \"EOF\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"a b\"   \"b c\"   \"c d\"   \"d e\"   \"e f\"   \"f g\"   \"g h\"   \"h i\"   \"i j\"  \n[10] \"j EOF\"\n```\n:::\n\n```{.r .cell-code}\npaste(x, c(x[-1], \"EOF\"), sep = \"|\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"a|b\"   \"b|c\"   \"c|d\"   \"d|e\"   \"e|f\"   \"f|g\"   \"g|h\"   \"h|i\"   \"i|j\"  \n[10] \"j|EOF\"\n```\n:::\n\n```{.r .cell-code}\npaste(x, c(x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 10\nidx token\n--- -----\n  1   a|b\n  2   b|c\n  3   c|d\n  4   d|e\n  5   e|f\n  6   f|g\n  7   g|h\n  8   h|i\n  9   i|j\n 10 j|EOF\n```\n:::\n:::\n\n\nThe result of this code is a vector of bigrams, i.e. each element corresponds to one element of `x` along with its following element.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% \n  select(-token_tags, -word_tags) %>% \n  mutate(\n    bigrams = map(all_tokens, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()),\n    pos_bigrams = map(pos_codes, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens())\n  )\nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 7\n   mw_tags    punctuation_tags all_tokens words    pos_codes bigrams  pos_bigr…¹\n   <list>     <list>           <list>     <list>   <list>    <list>   <list>    \n 1 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 2 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 3 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 4 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 5 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 6 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 7 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 8 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n 9 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n10 <xml_ndst> <xml_ndst>       <tokens>   <tokens> <tokens>  <tokens> <tokens>  \n# … with 490 more rows, and abbreviated variable name ¹​pos_bigrams\n```\n:::\n:::\n\n\n::: aside\nAt this point we don't need `token_tags` or `word_tags` anymore either.\n:::\n\nBelow we see the first token and POS-tag bigrams found in the first file of the corpus.\n\n\n::: {.cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nprint(d$bigrams[[1]], 10) # left\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 2242\nidx            token\n--- ----------------\n  1       the|fulton\n  2    fulton|county\n  3     county|grand\n  4       grand|jury\n  5        jury|said\n  6      said|friday\n  7        friday|an\n  8 an|investigation\n  9 investigation|of\n 10     of|atlanta's\n...\n```\n:::\n\n```{.r .cell-code}\nprint(d$pos_bigrams[[1]], 10) # right\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 2242\nidx  token\n--- ------\n  1  AT|NP\n  2  NP|NN\n  3  NN|JJ\n  4  JJ|NN\n  5 NN|VBD\n  6 VBD|NR\n  7  NR|AT\n  8  AT|NN\n  9  NN|IN\n 10 IN|NPg\n...\n```\n:::\n:::\n\n\n## Get counts with `n_tokens()`\n\nNow we have everything we need to compute the frequencies we were interested in. The new columns won't be lists anymore but simple numeric vectors, so we will use `map_dbl()` instead of `map()`. This function can be used when the output of the function applied to each element is a single number. For example, `n_tokens()` returns the number of tokens in a `tokens`, `types` or `freqlist` object, so applying it to each element of the `all_tokens` column, which is a list of `tokens` objects, will return a series of numbers.\n\n::: callout-important\n### Lists and vectors\n\nIn contrast to `map()`, `map_dbl()` returns a numeric vector rather than a *list* of numbers, which is a different kind of object in R. For example, if we have a vector `c(1, 2, 3)`, we can divide each element of it by the same number with a vectorized function such as `/`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnumeric_vector <- c(1, 2, 3)\nnumeric_vector\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1 2 3\n```\n:::\n\n```{.r .cell-code}\nnumeric_vector/2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5 1.0 1.5\n```\n:::\n:::\n\n\nHowever, if it is a list, that won't work; we would need a function such as `map()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\na_list <- list(1, 2, 3)\na_list\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n```\n:::\n\n```{.r .cell-code}\na_list/2\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in a_list/2: non-numeric argument to binary operator\n```\n:::\n:::\n\n:::\n\nOnce we have the `n_tok` column, we can create other numeric columns with `map_dbl()`, applying different functions that compute quantities, and then divide them by the contents of `n_tok` to obtain proportions. The number of types can be obtained by calling `n_types()` on the elements of the `all_tokens`, `bigrams` and `pos_bigrams` columns; the number of `mw` and `c` tags can be obtained by applying `length` to the elements of the `mw_tags` and `punctuation_tags` columns. For `mw` elements, which are very infrequent, we will multiply the resulting proportion by 10 000. The mean word length can be be obtained by applying `sum(nchar(x))/n_tokens(x)` to each element of the `words` columns: we count the number of characters of each element, sum them and then divide them by the number of elements.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% \n  mutate(\n    n_tok = map_dbl(all_tokens, n_tokens),\n    ttr = map_dbl(all_tokens, n_types)/n_tok,\n    p_bigr = map_dbl(bigrams, n_types) / n_tok,\n    p_pobi = map_dbl(pos_bigrams, n_types) / n_tok,\n    p_mw = map_dbl(mw_tags, length) / n_tok * 10000,\n    p_c = map_dbl(punctuation_tags, length) / n_tok,\n    word_len = map_dbl(words, ~ sum(nchar(.x))/n_tokens(.x))\n  )\nd %>% select(all_tokens, n_tok, ttr, starts_with(\"p_\"), word_len)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 8\n   all_tokens       n_tok   ttr p_bigr p_pobi  p_mw    p_c word_len\n   <list>           <dbl> <dbl>  <dbl>  <dbl> <dbl>  <dbl>    <dbl>\n 1 <tokens [2,242]>  2242 0.357  0.824  0.207 13.4  0.113      4.99\n 2 <tokens [2,277]>  2277 0.381  0.836  0.185 17.6  0.118      4.95\n 3 <tokens [2,275]>  2275 0.340  0.808  0.187 17.6  0.110      4.88\n 4 <tokens [2,216]>  2216 0.383  0.840  0.226  4.51 0.0884     5.19\n 5 <tokens [2,244]>  2244 0.320  0.765  0.206  0    0.0936     4.77\n 6 <tokens [2,263]>  2263 0.350  0.813  0.202 53.0  0.114      4.89\n 7 <tokens [2,270]>  2270 0.370  0.828  0.202 17.6  0.105      5.13\n 8 <tokens [2,187]>  2187 0.356  0.810  0.212  4.57 0.0841     4.91\n 9 <tokens [2,234]>  2234 0.379  0.823  0.179 17.9  0.0971     4.94\n10 <tokens [2,282]>  2282 0.368  0.832  0.216  4.38 0.120      4.82\n# … with 490 more rows\n```\n:::\n:::\n\n\nAs the output above shows, the new columns are numeric vectors rather than lists.\n\n## Filter tokens with `re()`\n\nFinally, we want to obtain the number of nominalizations and of different grammatical categories. We can obtain the nominalizations by extracting the word forms in the `all_tokens` column with a regular expression:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfirst_tokens <- d$all_tokens[[1]]\nprint(first_tokens[re(\"..+(tion|ment|ness|ity)$\")], 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nToken sequence of length 80\nidx          token\n--- --------------\n  1  investigation\n  2       election\n  3       election\n  4       election\n  5       election\n  6   registration\n  7       election\n  8 administration\n  9     department\n 10 implementation\n...\n```\n:::\n\n```{.r .cell-code}\nn_tokens(first_tokens[re(\"..+(tion|ment|ness|ity)$\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 80\n```\n:::\n\n```{.r .cell-code}\nn_tokens(first_tokens[re(\"..+(tion|ment|ness|ity)$\")]) / d$n_tok[[1]]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.036\n```\n:::\n\n```{.r .cell-code}\nn_tokens(first_tokens[re(\"..+(tion|ment|ness|ity)$\")]) / d$n_tok[[1]] * 10000\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 357\n```\n:::\n:::\n\n\nThe code below applies such workflow to the `all_tokens` column, obtaining the proportion of tokens that are nominalizations in each of the files, and similar instructions to `pos_codes`, extracting the proportion of tokens that are nouns in each file.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% \n  mutate(\n    p_nomin = map_dbl(all_tokens, ~n_tokens(.x[re(\"..+(tion|ment|ness|ity)$\")])) / n_tok * 10000,\n    p_noun = map_dbl(pos_codes, ~ n_tokens(.x[re(\"NN\")])) / n_tok\n  )\nd %>% select(all_tokens, pos_codes, p_nomin, p_noun)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 4\n   all_tokens       pos_codes        p_nomin p_noun\n   <list>           <list>             <dbl>  <dbl>\n 1 <tokens [2,242]> <tokens [2,242]>    357.  0.244\n 2 <tokens [2,277]> <tokens [2,277]>    211.  0.250\n 3 <tokens [2,275]> <tokens [2,275]>    224.  0.256\n 4 <tokens [2,216]> <tokens [2,216]>    370.  0.225\n 5 <tokens [2,244]> <tokens [2,244]>    241.  0.223\n 6 <tokens [2,263]> <tokens [2,263]>    305.  0.226\n 7 <tokens [2,270]> <tokens [2,270]>    304.  0.224\n 8 <tokens [2,187]> <tokens [2,187]>    251.  0.220\n 9 <tokens [2,234]> <tokens [2,234]>    255.  0.269\n10 <tokens [2,282]> <tokens [2,282]>    153.  0.208\n# … with 490 more rows\n```\n:::\n:::\n\n\n## Build many columns at once and `unnest()`\n\nFor the other POS-tags, we can do the same we did for the nouns. In the case of personal pronouns (\"PPSS\"), adjectives (\"JJ\"), negations (\"*\"), adverbs (\"RB\") and past tenses (\"QL\"), which are very infrequent, we'll multiply the proportions by 10 000. Therefore, given a `tokens` object with POS-tags `pos` and a number of tokens `nt`, for each of these regular expressions `regex` we will run the exact same code:\n`n_tokens(pos[re(regex)])/n_tok * 10000`. In order to avoid redundancy, we can do this for each POS-tag with `map_dbl()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npos <- d$pos_codes[[1]]\nnt <- d$n_tok[[1]]\npos_mapping <- c(p_ppss = \"PPSS\", p_adj = \"JJ\",\n                 p_neg = \"[*]\", p_adv = \"RB\", p_qual = \"QL\")\nmap_dbl(pos_mapping, ~n_tokens(pos[re(.x)])/nt * 10000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\np_ppss  p_adj  p_neg  p_adv p_qual \n    36    464     13    205     27 \n```\n:::\n\n```{.r .cell-code}\nmap_dbl(pos_mapping, ~n_tokens(pos[re(.x)])/nt * 10000) %>% as_tibble_row()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  p_ppss p_adj p_neg p_adv p_qual\n   <dbl> <dbl> <dbl> <dbl>  <dbl>\n1   35.7  464.  13.4  205.   26.8\n```\n:::\n:::\n\n\nIf we wrap this `map_dbl()` call inside a function `pos_proportions()`, we can easily apply ti to any `pos` and `nt` values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npos_proportions <- function(pos, nt) {\n  pos_mapping <- c(p_ppss = \"PPSS\", p_adj = \"JJ\", p_neg = \"[*]\",\n                   p_adv = \"RB\", p_qual = \"QL\")\n  map_dbl(pos_mapping, ~n_tokens(pos[re(.x)])/nt * 10000) %>% \n    as_tibble_row()\n}\npos_proportions(pos, nt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 5\n  p_ppss p_adj p_neg p_adv p_qual\n   <dbl> <dbl> <dbl> <dbl>  <dbl>\n1   35.7  464.  13.4  205.   26.8\n```\n:::\n:::\n\n\nWe can then apply our custom function `pos_proportions` to each element pair made from an item of `pos_codes` and one of `n_tok` using `map2()`, which will create a column *of tibbles*. Afterwards, `unnest()` turns the columns of those mini tibbles into columns of our dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% mutate(pos_prop = map2(pos_codes, n_tok, pos_proportions))\nd %>% select(pos_codes, n_tok, pos_prop)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 3\n   pos_codes        n_tok pos_prop        \n   <list>           <dbl> <list>          \n 1 <tokens [2,242]>  2242 <tibble [1 × 5]>\n 2 <tokens [2,277]>  2277 <tibble [1 × 5]>\n 3 <tokens [2,275]>  2275 <tibble [1 × 5]>\n 4 <tokens [2,216]>  2216 <tibble [1 × 5]>\n 5 <tokens [2,244]>  2244 <tibble [1 × 5]>\n 6 <tokens [2,263]>  2263 <tibble [1 × 5]>\n 7 <tokens [2,270]>  2270 <tibble [1 × 5]>\n 8 <tokens [2,187]>  2187 <tibble [1 × 5]>\n 9 <tokens [2,234]>  2234 <tibble [1 × 5]>\n10 <tokens [2,282]>  2282 <tibble [1 × 5]>\n# … with 490 more rows\n```\n:::\n\n```{.r .cell-code}\nd %>% select(pos_codes, n_tok, pos_prop) %>% unnest(pos_prop)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 7\n   pos_codes        n_tok p_ppss p_adj p_neg p_adv p_qual\n   <list>           <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>\n 1 <tokens [2,242]>  2242   35.7  464. 13.4   205.   26.8\n 2 <tokens [2,277]>  2277   39.5  457.  8.78  193.   13.2\n 3 <tokens [2,275]>  2275   30.8  541. 13.2   163.   39.6\n 4 <tokens [2,216]>  2216   18.1  731.  4.51  311.   99.3\n 5 <tokens [2,244]>  2244   62.4  584.  0     196.   26.7\n 6 <tokens [2,263]>  2263  128.   535.  8.84  163.   13.3\n 7 <tokens [2,270]>  2270   48.5  604.  8.81  163.   26.4\n 8 <tokens [2,187]>  2187   45.7  590.  4.57  306.   77.7\n 9 <tokens [2,234]>  2234   44.8  389.  8.95  179.   22.4\n10 <tokens [2,282]>  2282  101.   482.  4.38  184.   48.2\n# … with 490 more rows\n```\n:::\n\n```{.r .cell-code}\nd <- d %>% unnest(pos_prop) # assign to d\n```\n:::\n\n\n## Obtain matrix {#sec-mat}\n\nThe last we have to do is select the columns we are actually interested in, turning our output into a matrix and setting the rownames with the short version of our filenames.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- d %>% select(ttr, word_len, starts_with(\"p_\"))\ncolnames(d)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] \"ttr\"      \"word_len\" \"p_bigr\"   \"p_pobi\"   \"p_mw\"     \"p_c\"     \n [7] \"p_nomin\"  \"p_noun\"   \"p_ppss\"   \"p_adj\"    \"p_neg\"    \"p_adv\"   \n[13] \"p_qual\"  \n```\n:::\n\n```{.r .cell-code}\nd_mat <- as.matrix(d)\nrownames(d_mat) <- short_names(tei_fnames)\nd_mat[1:10,]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     ttr word_len p_bigr p_pobi p_mw   p_c p_nomin p_noun p_ppss p_adj p_neg\na01 0.36      5.0   0.82   0.21 13.4 0.113     357   0.24     36   464  13.4\na02 0.38      4.9   0.84   0.19 17.6 0.118     211   0.25     40   457   8.8\na03 0.34      4.9   0.81   0.19 17.6 0.110     224   0.26     31   541  13.2\na04 0.38      5.2   0.84   0.23  4.5 0.088     370   0.22     18   731   4.5\na05 0.32      4.8   0.77   0.21  0.0 0.094     241   0.22     62   584   0.0\na06 0.35      4.9   0.81   0.20 53.0 0.114     305   0.23    128   535   8.8\na07 0.37      5.1   0.83   0.20 17.6 0.105     304   0.22     48   604   8.8\na08 0.36      4.9   0.81   0.21  4.6 0.084     251   0.22     46   590   4.6\na09 0.38      4.9   0.82   0.18 17.9 0.097     255   0.27     45   389   9.0\na10 0.37      4.8   0.83   0.22  4.4 0.120     153   0.21    101   482   4.4\n    p_adv p_qual\na01   205     27\na02   193     13\na03   163     40\na04   311     99\na05   196     27\na06   163     13\na07   163     26\na08   306     78\na09   179     22\na10   184     48\n```\n:::\n:::\n\n\n## Summary {#sec-full}\n\nThis section showed the process of creating a matrix with proportions for Factor Analysis from a list of XML files. It paused the workflow at different stages in order to show what each step did and explain what the different functions try to achieve, but in practice it could all be done in one chain. Such code is shown below.\n\n<!-- NOTE we are shortening the names!! -->\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- as_tibble(tei_fnames) %>% \n  mutate(\n    xml = map(filename, read_xml),\n    ns = map(xml, xml_ns),\n    token_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w | //d1:mw | //d1:c\", namespaces = .y)),\n    word_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:w\", namespaces = .y)),\n    mw_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:mw\", namespaces = .y)),\n    punctuation_tags = map2(xml, ns, ~ find_xpath(.x, \"//d1:c\", namespaces = .y)),\n    all_tokens = map(token_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens()),\n    words = map(word_tags, ~ xml_text(.x) %>% tolower() %>% cleanup_spaces() %>% as_tokens()),\n    pos_codes = map(token_tags, function(toks) {\n      most_pos <- xml_attr(toks, \"type\")\n      mw_pos <- xml_attr(toks, \"pos\")\n      most_pos[is.na(most_pos)] <- mw_pos[!is.na(mw_pos)]\n      as_tokens(most_pos)\n    }),\n    bigrams = map(all_tokens, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()),\n    pos_bigrams = map(pos_codes, ~ paste(.x, c(.x[-1], \"EOF\"), sep = \"|\") %>% as_tokens()),\n    n_tok = map_dbl(all_tokens, n_tokens),\n    ttr = map_dbl(all_tokens, n_types)/n_tok,\n    p_bigr = map_dbl(bigrams, n_types) / n_tok,\n    p_pobi = map_dbl(pos_bigrams, n_types) / n_tok,\n    p_mw = map_dbl(mw_tags, length) / n_tok * 10000,\n    p_c = map_dbl(punctuation_tags, length) / n_tok,\n    word_len = map_dbl(words, ~ sum(nchar(.x))/n_tokens(.x)),\n    p_nomin = map_dbl(all_tokens, ~n_tokens(.x[re(\"..+(tion|ment|ness|ity)$\")])) / n_tok * 10000,\n    p_noun = map_dbl(pos_codes, ~ n_tokens(.x[re(\"NN\")])) / n_tok,\n    sp = map2(pos_codes, n_tok, small_proportions)) %>% \n  unnest(sp) %>% \n  mutate(filename = short_names(filename)) %>% \n  select(filename, ttr, word_len, starts_with(\"p_\"))\nd_mat <- data.frame(d, row.names = \"filename\") %>% \n  as.matrix()\n```\n:::\n\n\nAnd we are ready to run Factor Analysis!\n\n::: callout-tip\n### Save and read\n\nYou might have noticed that in the final piece of code we defined the rownames of the matrix in a different way! In @sec-mat, `d` doesn't contain the filenames, and instead we added the rownames with `rownames(d_mat) <- short_names(tei_fnames)`. In @sec-full, instead, since we never removed the `filename` column, we turn it into its short version and then include it as the first column of the final output. If we do this, we can then set the values of the `filename` column as the rownames of the matrix by first turning `d` into a data frame with `data.frame(d, row.names = \"filename\")`.\n\nWhy would we do that? Well, with the new `d` that includes filenames, we can save the tibble for future use and analysis using `readr::write_tsv()`^[Remember that `{readr}` is part of `{tidyverse}`, so it's already loaded!] and then read it with `readr::read_tsv()`. After reading it again, we can turn it into a matrix setting the names of the `filename` column as rownames. This is done in the [Factor Analysis slides](../slides/factor-analysis.qmd) too!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_tsv(d, here::here(\"studies\", \"register-analysis.tsv\"))\nd_mat <- read_tsv(here::here(\"studies\", \"register-analysis.tsv\"), show_col_types = FALSE) %>% \n  data.frame(row.names = \"filename\") %>% \n  as.matrix()\n```\n:::\n\n:::\n\n# Factor Analysis {#sec-fa}\n\nFactor Analysis can be run in R with the base function `factanal()`, which takes a matrix like `d_mat` and a number of factors to reduce it to. An important argument is `rotation`: the default, \"varimax\", assumes that the factors are orthogonal (uncorrelated), whereas \"promax\" allows for some correlation between them. This is the setting recommended by both @levshina_2015 and @biber_1988:\n\n> In the description of textual variation, where the factors represent underlying textual dimensions, there is no reason to assume that the factors are completely uncorrelated, and therefore a Promax rotation is recommended.\n> [@biber_1988 85]\n\nIn addition, if we want to obtain the scores of the different files, we have to ask for it by setting the `scores` argument to either \"regression\" or \"Bartlett\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_fa <- factanal(d_mat, 4, scores = \"regression\", rotation = \"promax\")\nd_fa\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nfactanal(x = d_mat, factors = 4, scores = \"regression\", rotation = \"promax\")\n\nUniquenesses:\n     ttr word_len   p_bigr   p_pobi     p_mw      p_c  p_nomin   p_noun \n   0.014    0.057    0.096    0.241    0.005    0.544    0.245    0.250 \n  p_ppss    p_adj    p_neg    p_adv   p_qual \n   0.422    0.383    0.132    0.414    0.510 \n\nLoadings:\n         Factor1 Factor2 Factor3 Factor4\nttr                       1.006         \nword_len  1.013   0.121   0.163         \np_bigr                    0.888   0.183 \np_pobi   -0.228           0.305   0.607 \np_mw      0.147   1.129                 \np_c      -0.249   0.510                 \np_nomin   0.934   0.108  -0.213         \np_noun    0.563                  -0.431 \np_ppss   -0.272   0.369  -0.120   0.260 \np_adj     0.818                   0.300 \np_neg             0.995                 \np_adv    -0.256          -0.197   0.605 \np_qual    0.311  -0.118           0.780 \n\n               Factor1 Factor2 Factor3 Factor4\nSS loadings       3.27    2.71    2.03    1.74\nProportion Var    0.25    0.21    0.16    0.13\nCumulative Var    0.25    0.46    0.62    0.75\n\nFactor Correlations:\n        Factor1 Factor2 Factor3 Factor4\nFactor1   1.000  0.1266 -0.7496   0.426\nFactor2   0.127  1.0000 -0.0532  -0.262\nFactor3  -0.750 -0.0532  1.0000  -0.387\nFactor4   0.426 -0.2624 -0.3871   1.000\n\nTest of the hypothesis that 4 factors are sufficient.\nThe chi square statistic is 371 on 32 degrees of freedom.\nThe p-value is 2.3e-59 \n```\n:::\n:::\n\n\nThe default printed output includes the call, the uniquenesses of the variables, their loadings in each factor, the variance explained by each factor, the correlation between the factors (when `rotation = \"promax\"`) and the statistic testing that the chosen number of factors is sufficient. In this case, we have a few variables with very high uniqueness (which might lead us to exclude them) and a few that load high in more than one factor, as well as a cumulative variance explained of 0.75. The p-value is really low, indicating that 4 factors are not enough, but increasing them does not really help the model. For such exploratory analysis, our focus lies on the interpretability of the factors.\n\nIn order to easily report the loadings of the factors, we can extract them with `loadings()` and turn them into a tibble with `as_tibble()`, although in the middle we have to remove the \"loadings\" class for `as_tibble()` to work. If we turn irrelevant loadings (here with a threshold of 0.3) to `NA` and we have set `options(knitr.kable.NA = \"\")`, we can hide them.\n\n\n::: {#tbl-loadings .cell tbl-cap='Loadings of the Factor Analysis.'}\n\n```{.r .cell-code}\nd_loadings <- loadings(d_fa) %>% \n  unclass() %>% \n  as_tibble(rownames = \"Variable\") %>% \n  mutate(across(where(is.numeric), ~if_else(abs(.x) < 0.3, NA_real_, .x)))\nkbl(d_loadings) %>% \n  kable_paper(full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Variable </th>\n   <th style=\"text-align:right;\"> Factor1 </th>\n   <th style=\"text-align:right;\"> Factor2 </th>\n   <th style=\"text-align:right;\"> Factor3 </th>\n   <th style=\"text-align:right;\"> Factor4 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> ttr </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 1.01 </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> word_len </td>\n   <td style=\"text-align:right;\"> 1.01 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_bigr </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.89 </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_pobi </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.30 </td>\n   <td style=\"text-align:right;\"> 0.61 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_mw </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 1.13 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_c </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.51 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_nomin </td>\n   <td style=\"text-align:right;\"> 0.93 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_noun </td>\n   <td style=\"text-align:right;\"> 0.56 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> -0.43 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_ppss </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.37 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_adj </td>\n   <td style=\"text-align:right;\"> 0.82 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.30 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_neg </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.99 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_adv </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.60 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> p_qual </td>\n   <td style=\"text-align:right;\"> 0.31 </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\">  </td>\n   <td style=\"text-align:right;\"> 0.78 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nThe scores can be retrieved with `$scores`, and then turned into a tibble with `as_tibble()`. This automatically generates nice column names. We can also add a `Component` column that collects the lowercase name of the filename, which represents the Brown component (or register) the file belongs to.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscores <- d_fa$scores %>% as_tibble(rownames = \"File\") %>% \n  mutate(Component = re_retrieve_first(File, \"[a-z]\"))\nscores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 500 × 6\n   File   Factor1 Factor2 Factor3 Factor4 Component\n   <chr>    <dbl>   <dbl>   <dbl>   <dbl> <chr>    \n 1 a01    0.530     0.432   0.837  -1.09  a        \n 2 a02   -0.148     0.543   1.63   -2.19  a        \n 3 a03   -0.00890   0.230   0.581  -1.47  a        \n 4 a04    1.65      0.532   1.00    0.357 a        \n 5 a05   -0.709    -0.904  -0.169  -0.707 a        \n 6 a06    0.677     1.28    0.917  -1.52  a        \n 7 a07    1.02      0.920   1.18   -1.23  a        \n 8 a08    0.115    -0.402   0.505  -0.122 a        \n 9 a09   -0.0710    0.700   1.63   -2.47  a        \n10 a10   -0.838    -0.734   0.956  -0.791 a        \n# … with 490 more rows\n```\n:::\n:::\n\n\n## Plotting files\n\nOnce we have the `scores` tibble, we can use it to plot the files based on certain factors. For example, @fig-a below plots all files based on their scores in Factors 1 and 2 and highlights the files from the \"a\" component.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncomp <- \"a\"\nthis_comp <- filter(scores, Component == comp)\nother_comps <- filter(scores, Component != comp)\nggplot(other_comps, aes(x = Factor1, y = Factor2)) +\n  geom_point(shape = 3, color = \"gray\") +\n  geom_text(data = this_comp, label = comp, color = \"steelblue\") +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![Scatterplot of Brown files and their scores in the first and second factors, highlighting those from the \"a\" component.](register-analysis_files/figure-html/fig-a-1.png){#fig-a width=672}\n:::\n:::\n\n\nWe could also use `cowplot::plot_grid()` to plot a list of these plots, each of them highlighting a different component. We can create such a list with `map()`, applying a variant of the code from above to each of the components (obtained with `unique(scores$Component)`). We can then give this list of plots as `plotlist` argument to `plot_grid()`, resulting in @fig-plotlist.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cowplot)\nplots <- map(unique(scores$Component), function(comp) {\n  this_comp <- filter(scores, Component == comp)\n  other_comps <- filter(scores, Component != comp)\n  ggplot(other_comps, aes(x = Factor1, y = Factor2)) +\n    geom_point(shape = 3, color = \"gray\") +\n    geom_point(data = this_comp, color = \"steelblue\", alpha = 0.8) +\n    geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n    geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\") +\n    theme(axis.title = element_blank())\n})\nplot_grid(plotlist = plots, labels = unique(scores$Component))\n```\n\n::: {.cell-output-display}\n![Scatterplots of Brown files and their scores in the first and second factors, highlighting one component at a time.](register-analysis_files/figure-html/fig-plotlist-1.png){#fig-plotlist width=672}\n:::\n:::\n\n\n## Plotting registers\n\nAlternatively, we could obtain the centroid of each Brown component by computing the mean of the scores of its components. This can be achieved via `group_by()` combined with `summarize()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nscores %>% \n  group_by(Component) %>% \n  summarize(Factor1 = mean(Factor1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 15 × 2\n   Component  Factor1\n   <chr>        <dbl>\n 1 a         -0.224  \n 2 b          0.138  \n 3 c         -0.00658\n 4 d         -0.418  \n 5 e          0.266  \n 6 f         -0.268  \n 7 g         -0.230  \n 8 h          1.15   \n 9 j          0.904  \n10 k         -1.36   \n11 l         -0.0854 \n12 m         -0.384  \n13 n         -0.724  \n14 p         -0.151  \n15 r         -0.395  \n```\n:::\n:::\n\n\nBy default, `summarize()` only returns the variables requested in its call (here `Factor1`) and the grouping variables (here `Component`). We can compute the mean of all the numeric variables by adding a call to `across()` and selecting all numeric variables with `where(is.numeric)`. The resulting figure is shown in @fig-centroids.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncentroids <- scores %>% \n  group_by(Component) %>% \n  summarize(across(where(is.numeric), mean))\nhead(centroids)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 5\n  Component  Factor1 Factor2  Factor3 Factor4\n  <chr>        <dbl>   <dbl>    <dbl>   <dbl>\n1 a         -0.224    0.154   1.17     -1.10 \n2 b          0.138   -0.0602  0.906     0.221\n3 c         -0.00658 -0.240   1.31      0.100\n4 d         -0.418   -1.21   -0.597     0.930\n5 e          0.266    0.0557 -0.00126  -0.112\n6 f         -0.268   -0.614   0.0908    0.226\n```\n:::\n\n```{.r .cell-code}\nggplot(scores, aes(x = Factor1, y = Factor2, label = Component)) +\n  geom_text(color = \"gray\") +\n  geom_text(data = centroids, size = 6) +\n  geom_hline(yintercept = 0, color = \"black\", linetype = \"dashed\") +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![Scatterplot of component centroids from the Brown corpus and their scores in Factors 1 and 2.](register-analysis_files/figure-html/fig-centroids-1.png){#fig-centroids width=672}\n:::\n:::\n\n\nGoing back to @tbl-components, we see that components \"h\" and \"j\", i.e. Miscellaneous and Learned, have the highest values along the first factor: they tend to have longer words, more nominalizations and more adjectives, whereas component \"k\" (General Fiction) lies on the opposite pole with shorter words and fewer nominalizations and adjectives. Across the second factor, where higher values indicate a stronger tendency towards composite tags (clitics), negation and punctuation marks, we have \"l\" and \"p\" at the top (Mystery and Detective Fiction along with Romance and Love Stories) and \"d\" at the bottom (Religion).\n\nHowever, as we could also see in @fig-plotlist, each register is very broadly distributed across these two factors; some of them have more clear tendencies (\"g\" is fully on the negative side of Factor 1; \"h\" is almost entirely on the positive side of Factor 2 whereas \"k\" is almost entirely on its negative side...) and others much less.\n\nMore remains to be done in terms of description: concretely, the rest of the factors should also be interpreted and plotted to understand how (and if) the registers can be described in terms of them.\n",
    "supporting": [
      "register-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}