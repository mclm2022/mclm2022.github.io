{
  "hash": "1299d4a75f98b77f018ad2bd3c54833f",
  "result": {
    "markdown": "---\ntitle: \"Genitive Alternation II: Analysis\"\nexecute:\n  warning: false\n---\n\n\nThis is the second part of a variation study focusing on the genitive alternation in English, using the Brown corpus. This part illustrates the analysis with conditional inference trees and [mixed-effects logistic regression](../slides/logistic-regression.qmd), whereas the [first part](genitive-alternation-retrieval.qmd) focused on retrieving observations of the alternation.\n\n\n\n\n\n# Setup\n\nFor this study, as always, we need to activate the packages `{tidyverse}` and `{mclm}`. We will also use `{kableExtra}` to print some tables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(mclm)\nlibrary(kableExtra)\n```\n:::\n\n\nIn addition, we'll need the packages `{lmerTest}`, which loads `{lme4}`, for the mixed-effects logistic regression; `{partykit}` for the conditional tree; `{afex}` for comparing models with different predictors and `{ggeffects}` for plotting effects. [If you have not installed these packages, remember to do so with `install.packages()`.]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(lmerTest)\nlibrary(partykit)\nlibrary(afex)\nlibrary(ggeffects)\n```\n:::\n\n\nWe will also set a general theme for all coming plots with `theme_set()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheme_set(theme_minimal(base_size = 18) + theme(legend.position = \"top\"))\n```\n:::\n\n\nThis time, we don't need to read any corpora. Instead, we'll open the concordance file we saved in the [first step](genitive-alternation-retrieval.qmd#save) and work with it. In between, you might also have gone through the examples for manual cleaning or to add other variables, either with a spreadsheet software or with [concAnnotator](https://github.com/montesmariana/concAnnotator/).\n\nAs we read the dataset, we'll also change the type of the categorical variables, i.e. `gen_type` and `possessor_type`, from character vectors to factors. Factors are *actually* numeric vectors with named levels. Crucially, they have predefined levels that keep their order: they will complain if you try to add an item with a different value, will remember an original level that has frequency 0, and will always keep the designed order.[The tidyverse package `{forcats}` has a lot of useful functions to manipulate factors, all starting with `fct_`.]{.aside} Modelling functions such as `lme4::glmer()` and `partykit::ctree()`, which we'll use below, cannot work with character vectors and need factors instead.\n\n::: aside\n![](forcats_logo.png){width=\"6em\" alt-text=\"The forcats logo\"}\n\nThe tidyverse package `{forcats}` has a lot of useful functions to manipulate factors, all starting with `fct_`.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfname <- file.path(data_folder, \"genitive-alternation.tab\") # wherever you stored the file\ncd <- read_conc(fname) %>% \n  mutate(\n    gen_type = factor(gen_type, levels = c(\"of\", \"s\")),\n    possessor_type = factor(possessor_type, levels = c(\"common\", \"proper\"))\n    )\n```\n:::\n\n\nThe first level of a factor is a reference level. For the response variable, in most modelling functions (not in `{tidymodels}`), this counts as the failure outcome; therefore, the model will show the odds of the *second level* (in this case *s*-genitive) occurring.\n\n## Exploring the data\n\nIt is good practice to get familiar with your dataset before delving into the modelling. On the one hand, it may expose annotation errors, such as mistyped values in manually annotated variables. On the other hand, it gives you a foundation for the interpretation of the output and a notion of what you may encounter.\n\nIn the [retrieval section](genitive-alternation-retrieval.qmd#automatic-annotation) we already saw the `xtabs()` function, which quickly returns a contingency table between two variables.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxtabs(~ gen_type + possessor_type, data = cd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        possessor_type\ngen_type common proper\n      of   3151   1006\n      s    1050   2137\n```\n:::\n:::\n\n\nWith `{tidyverse}`, particularly for a more tabular format, you can use `count()` combined with `pivot_wider()` to obtain a similar result, as shown in @tbl-count.\n\n\n::: {#tbl-count .cell tbl-cap='Contingency table between the outcome `gen_type` and the predictor `possessor_type`.'}\n\n```{.r .cell-code}\ncd %>% \n  count(gen_type, possessor_type) %>% \n  pivot_wider(names_from = possessor_type, values_from = n) %>% \n  kbl() %>%\n  kable_paper()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> gen_type </th>\n   <th style=\"text-align:right;\"> common </th>\n   <th style=\"text-align:right;\"> proper </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> of </td>\n   <td style=\"text-align:right;\"> 3151 </td>\n   <td style=\"text-align:right;\"> 1006 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> s </td>\n   <td style=\"text-align:right;\"> 1050 </td>\n   <td style=\"text-align:right;\"> 2137 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nFor quantitative variables, we can extract the corresponding column from the dataset as a vector, e.g. `pull(cd, size_possessor)` or `cd$size_possessor`, and apply `summary()`. In order to correlate this with a categorical variable, we can first split the dataset based on the values of that variable with `split()` and then apply `summary()` on the columns with `map()`.\n\n::: aside\n![](purrr_logo.png){width=\"6em\" alt-text=\"The purrr logo\"}\n\n`{purrr}` functions such as `map()` apply the same function to each element of a list or vector. Here, we take each element of the split dataframe and apply the same code.\n:::\n\n\n::: {.cell .fig-column-margin}\n\n```{.r .cell-code}\ncd %>% \n  split(.$gen_type) %>% \n  map(~ summary(.x$size_possessor))\n## $of\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##    0.00    1.61    2.08    2.08    2.48    4.26 \n## \n## $s\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   0.693   1.609   1.946   1.939   2.197   3.611\n\ncd %>% ggplot(aes(x = gen_type, y = size_possessor)) +\n  geom_boxplot() + theme_minimal(base_size = 30)\n```\n\n::: {.cell-output-display}\n![](genitive-alternation-analysis_files/figure-html/split-1.png){width=672}\n:::\n:::\n\n\nWe can already see here that the tiniest and the longest Possessors tend to go with the *of*-genitive. Remember that `size_possessor` doesn't contain the length in characters but its logarithm. Therefore, the minimum size of 0 corresponds to a character length of 1. @tbl-sizes collects these examples for deeper inspection.\n\n\n::: {#tbl-sizes .cell tbl-cap='Concordances where the Possessor constituent has only one character.'}\n\n```{.r .cell-code}\ncd %>% \n  filter(size_possessor == 0) %>% \n  as_tibble() %>% \n  select(match, possessor) %>% \n  kbl() %>% kable_paper()\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> match </th>\n   <th style=\"text-align:left;\"> possessor </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> the primary decomposition of T </td>\n   <td style=\"text-align:left;\"> t </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> The constancy of P </td>\n   <td style=\"text-align:left;\"> p </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> The choice of P </td>\n   <td style=\"text-align:left;\"> p </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the graph of F </td>\n   <td style=\"text-align:left;\"> f </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the graph of F </td>\n   <td style=\"text-align:left;\"> f </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the multiplicity of F </td>\n   <td style=\"text-align:left;\"> f </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the graph of F </td>\n   <td style=\"text-align:left;\"> f </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the values of S </td>\n   <td style=\"text-align:left;\"> s </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the values of S </td>\n   <td style=\"text-align:left;\"> s </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the image of L </td>\n   <td style=\"text-align:left;\"> l </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the plane of L </td>\n   <td style=\"text-align:left;\"> l </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> the image of L </td>\n   <td style=\"text-align:left;\"> l </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n<!-- In the [last section of this document](#Easystats) we will learn what the `{report}` package can offer us. -->\n\nFinally, we could also do some visualization. We could use barplots for the categorical variables, mosaic plots for the contingency table, histograms for the quantitative variables, scatterplots for the relationship between two quantitative variables or boxplots/violin plots to compare quantities and/or relate them to qualitative variables.\n\n::: panel-tabset\n### Bars\n\nBarplots can be created with the `geom_bar()` function. The `position` argument controls whether the bars of different colors are stacked or put next to each other.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(cd, aes(x = gen_type, fill = possessor_type)) +\n  geom_bar(position = position_dodge()) +\n  scale_fill_manual(values = c(\"orange\", \"darkgreen\"))\n```\n\n::: {.cell-output-display}\n![Example of a barplot visualizing the number of observations with each outcome and Possessor type. If you already have the counts for the y-axis, use `geom_col()` instead.](genitive-alternation-analysis_files/figure-html/fig-barplot-1.png){#fig-barplot width=672}\n:::\n:::\n\n\n### Mosaic\n\nWhile the `{ggmosaic}` package offers a way of creating mosaic plots with `{ggplot2}`, `graphics::mosaic()` has the nice feature of shading the sections based on the residuals (with `shade = TRUE`). Red squares indicate that the frequency of a combination is *significantly* lower than expected (i.e. if the distribution was proportional), and blue squares, that the frequency is significantly higher than expected. In other words, you can already see if a pair of values of two categorical variables are \"attracted\" to each other. Here it is very clear that the Possessor types are not balanced across genitive variants: the common noun Possessor is much more attracted to the *of*-genitive, and the proper noun to the *s*-genitive.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmosaicplot(gen_type ~ possessor_type, data = cd, shade = TRUE)\n```\n\n::: {.cell-output-display}\n![Example of a mosaic plot visualizing the contingency table between the outcome `gen_type` and the predictor `possessor_type`.](genitive-alternation-analysis_files/figure-html/fig-mosaic-1.png){#fig-mosaic width=672}\n:::\n:::\n\n\n### Histograms\n\nHistograms can be generated with `geom_histogram()`; they show the frequencies of different values along a continuous scale.\n\n\n::: {#fig-hist .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nggplot(cd, aes(x = size_possessor)) +\n  geom_histogram()\n\nggplot(cd, aes(x = size_possessed)) +\n  geom_histogram()\n```\n\n::: {.cell-output-display}\n![Size of the possessor slot](genitive-alternation-analysis_files/figure-html/fig-hist-1.png){#fig-hist-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Size of the possessed slot](genitive-alternation-analysis_files/figure-html/fig-hist-2.png){#fig-hist-2 width=672}\n:::\n\nExamples of histograms showing the frequencies of different Possessor and Possessed sizes (log of N of characters).\n:::\n\n\n### Points\n\nA scatterplot of points can contrast two quantitative variables; in this case the log-transformed slot sizes (@fig-scatter-1) and the original sizes in number of characters, retrieved with `exp()` (@fig-scatter-2).\n\n\n::: {#fig-scatter .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nggplot(cd, aes(x = size_possessor, y = size_possessed)) +\n  geom_point(size = 3, alpha = 0.5)\n\nggplot(cd, aes(x = exp(size_possessor), y = exp(size_possessed))) +\n  geom_point(size = 3, alpha = 0.5) +\n  labs(x = \"possessor\", y = \"possessed\")\n```\n\n::: {.cell-output-display}\n![Log-transformed character length](genitive-alternation-analysis_files/figure-html/fig-scatter-1.png){#fig-scatter-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Original character length](genitive-alternation-analysis_files/figure-html/fig-scatter-2.png){#fig-scatter-2 width=672}\n:::\n\nExample of a scatterplot contrasting the size of a Possessor and a Possessed slot in each observation.\n:::\n\n\n### Violin\n\nIn order to compare the ranges of `size_possessor` and `size_possessed` in a violin plot, we need to turn the dataset into a long-form version, so that the value of possessor/possessed is another variable. For that we use `pivot_longer()`, the complement of `pivot_wider()`, with the names of the columns that we want to change and a few useful arguments. First, for the selection of columns, we can also use `{dplyr}` selectors such as `starts_with()`, which selects all columns that start with a given string. Then, `names_to` and `values_to` indicate the new names for the columns to which the original *names* of the columns (here `size_possessor` and `size_possessed`) and their *values* will go. In this case, with `names_to = \"slot\", values_to = \"size\"`, each row of the original dataset will become two rows, and the columns `size_possessor` and `size_possessed` will have disappeared. Instead, there will be a column `slot` with values *size_possessor* and *size_possessed*, and a column `size` with the original values of that row for `size_possessor` and `size_possessed` respectively. This allows us to use `slot` as a variable for the ggplot aesthetics. Finally, the `names_transform` argument allows us to transform the values that will go into the name-column, i.e. `slot`. In particular, this `stringr::str_remove()` call will remove the \"size\\_\" prefix from `size_possessed` and `size_possessor`.\n\nThe function to draw a violin plot is `geom_violin()`.\n\n::: callout-note\n#### Violins and boxes\n\nIf you're wondering why I use violin plots instead of boxplots, I invite you to read the article [*Same Stats, Different Graphs*](https://www.autodesk.com/research/publications/same-stats-different-graphs), by Justin Matejka and George Fitzmaurice.\n\nAnd if you are interested in more advanced graphs, combining half-vioilin plots and half-boxplots or scatterplots, check out [the `{gghalves}` package](https://erocoar.github.io/gghalves/).\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncd %>% as_tibble() %>% \n  select(starts_with(\"size_p\"), gen_type) %>% \n  pivot_longer(starts_with(\"size\"),\n               names_to = \"slot\", values_to = \"size\",\n               names_transform = ~ str_remove(.x, \"size_\")) %>% \n  ggplot(aes(x = slot, y = size, fill = gen_type)) +\n  geom_violin() +\n  scale_fill_manual(values = c(\"orange\", \"darkgreen\"))\n```\n\n::: {.cell-output-display}\n![Example of a violin plot.](genitive-alternation-analysis_files/figure-html/fig-violin-1.png){#fig-violin width=672}\n:::\n:::\n\n:::\n\n# Conditional inference tree\n\nThe first analysis we'll run is a conditional inference tree, which we'll obtain with `partykit::ctree()`. The first argument is a formula, indicating on the left side of the tilde `~` the response variable or outcome, and on the right side the predictors, separated by `+`. The second argument is the dataset from which the variables are taken. The output is an object of class `party`, which can be stored on a file, if needed, with `saveRDS()`.[`saveRDS()` is handy function from base R to write R objects to files; you can restore them with `readRDS()`. The downside is that only R can read these files, so if you have an object that could be stored in a more *interoperable* way, such as tab-separated files or images, then that is preferable.]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncd_ctree <- ctree(gen_type ~ possessor_type + size_possessor + size_possessed + size_diff,\n                  data = cd)\n```\n:::\n\n\nThe easiest way to inspect a tree is by plotting it, as shown in @fig-ctree.\n\n\n::: {.cell .column-page}\n\n```{.r .cell-code}\nplot(cd_ctree)\n```\n\n::: {.cell-output-display}\n![Conditional inference tree predicting the genitive alternation.](genitive-alternation-analysis_files/figure-html/fig-ctree-1.png){#fig-ctree width=960}\n:::\n:::\n\n\nA conditional inference tree first tries to find a variable by which you can split the dataset in significantly different values, and then repeats the procedure iteratively until no other split returns significantly different groups. In the case of a classification task, i.e. when the outcome is categorical, the splits should have significantly different proportions of the success level.\n\nConcretely, @fig-ctree shows that the first split is given by `possessor_type`, followed by `size_possessor` and then `size_possessed`.[The split variable at a given level doesn't have to be the same in all branches.]{.aside} That means that the largest significant difference is given between the observations where the Possessor is a common noun, in which the chances of *of*-genitive are substantially and significantly higher than those of *s*-genitive, against those in which the Possessor is a proper noun, which favor the *s*-genitive a bit more.\n\n\n\n\n\nWithin the split based on `possessor_type`, the next significant split is given by `size_possessor`. When the Possessor is a common noun, if its size is larger than 2.3026 (10 characters) the chances of the *of*-genitive variant increase in relation to observations where the Possessor has fewer characters. Notice than in both cases the *of*-variant is dominant, but the proportion is significantly different. Within the group with 10 characters or fewer, `size_possessed` is now relevant and the *of*-genitive is favored more when it is 1.0986 or lower (3 characters). Notice, however, that the size of the node is much smaller: whereas, within the occurrences with common noun possessor of 10 characters or fewer, more than 75% of those with a shorter Possessed constituent exhibit the *of*-genitive, in absolute numbers there are more longer Possessed constituents with the *of*-genitive.\n\nThe right side of the plot shows the significant splits within the cases with proper nouns in the Possessor slot: this time, if the possessor is longer, with `size_possessor` larger than 2.6391 (14 characters), the chances of the *of*-genitive variant also increase, becoming equal to those of the *s*-genitive within that group.\n\n::: {.callout-tip collapse=\"true\"}\n## Plotting with `{ggparty}`\n\nPlotting with the default method of a `party` object is straightforward and returns the information you want, but it doesn't give you much room for manipulation. If the plotting space is too small, the different parts of the plot may overlap --- if it's way too small, you might even get a warning.\n\nThe `{ggparty}` offers a `{ggplot2}` extension to plot objects from the `{partykit}` package. Designing the plot is now more complicated, but it gives you more freedom, in case you want to manipulate the labels, colors, sizes, etc. For a conditional tree such as the one shown in @fig-ctree, `{ggparty}` might be a bit of an overkill, but @fig-ggparty is an example in which I change the colors in the barplots and color the node labels based on the splitting variables.\n\nA downside of `{ggparty}` is that it doesn't automatically round the numerical values on the edges, but you can copy-paste the code in lines 5:12 to fix that in any other `ggparty` object (just make sure to change `cd_party` to the name of the `ggparty` object). I won't explain the rest of the code; if you're interested in the capabilities of the package, check out [the vignette](https://cran.r-project.org/web/packages/ggparty/vignettes/ggparty-graphic-partying.html).\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"true\"}\nlibrary(ggparty)\n\ncd_party <- ggparty(cd_ctree)\n\nround_edges <- function(break_label) {\n  rounded <- str_extract(break_label, \"[0-9.]+\") %>% \n    as.numeric() %>% \n    formatC(digits = 4)\n  str_replace(break_label, \"[0-9.]+\", rounded)\n}\n\ncd_party$data$breaks_label <- map(cd_party$data$breaks_label, round_edges)\n\ncd_party + # ggparty object\n  geom_edge() + # add edges\n  geom_edge_label(size = 4) + # add edge labels\n  geom_node_label( # add and edit node labels\n    mapping = aes(col = splitvar),\n    line_list = list(\n      aes(label = splitvar),\n      aes(label = sprintf(\"p = %.2e\", p.value))\n    ),\n    line_gpar = list(\n      list(size = 15),\n      list(size = 10)\n    ),\n    ids = \"inner\") +\n  geom_node_label( # add node size to terminal nodes\n    aes(label = paste(\"N= \", nodesize)),\n    ids = \"terminal\"\n  ) +\n  geom_node_plot( # add plots\n    gglist = list(\n      geom_bar(aes(x = \"\", fill = gen_type),\n               position = position_fill()),\n      labs(y = \"Proportion\", x = NULL,\n           fill = \"Genitive variant\"),\n      scale_fill_manual(values = c(\"orange\", \"darkgreen\")),\n      theme_minimal(base_size = 15)),\n    shared_axis_label = TRUE) +\n  scale_color_brewer(palette = \"Dark2\", guide = \"none\") # set node labels color\n```\n\n::: {.cell-output-display}\n![Conditional inference tree predicting the genitive alternation, plotted with `{ggparty}`.](genitive-alternation-analysis_files/figure-html/fig-ggparty-1.png){#fig-ggparty width=672}\n:::\n:::\n\n:::\n\n## Conditional trees in relation to other techniques\n\nUnlike logistic regression analysis, conditional inference trees have the ability to deal with collinearity. This is the case where two variables basically do the same thing, or one variable has the same effect as a combination of two other variables. Therefore, we can use a conditional tree to see which predictors to use in the regression model. For example, in our dataset `size_difference` is redundant with `size_possessor` and `size_possessed` because it depends on them, and @fig-ctree tells us that we can safely discard `size_difference` because it doesn't have a significant effect.\n\nOn the other hand, conditional trees look at one variable at a time, and once a split is performed, the effect of the following variables is examined in relation to the split dataset. That is how the effect of `size_possessor` is different in the subset where `possessor_type` is *common* compared to the subset where `possessor_type` is *proper*, and how `size_possessed` is only a relevant predictor in certain splits. In contrast, regression models look at the effect of each variable on the whole dataset, while accounting for the effects of the rest of the variables.\n\nFinally, the conditional tree chooses the most relevant variable but does not tell us how close other variables were --- it seems that `size_possessor` is not as effective as `possessor_type` when splitting the full dataset, but how less effective is it? In order to obtain a ranking of importance of the variables, we can run random forests. This technique generates a large number of trees with different subsets of the dataset (different subsets of rows and of columns) and checks which variable takes the first place in each of the trees. This is a handy technique when we have many different variables, but it is not very time-efficient and therefore it's overkill for this dataset.\n\n# Logistic regression analysis\n\nBased on the results of the conditional tree, we can decide which predictors to use in the logistic regression model. We'll run it with `lme4::glmer()`, which was loaded when we loaded `{lme4Test}`. We'll obtain a summary of the results with the `summary()` method, which, if `{lme4Test}` is loaded, will include p-values.\n\n`lme4::glmer()` is a function for \"mixed-effects generalized linear models\"; when run with the argument `family = binomial`, it runs mixed-effects logistic regression. A mixed effects model combines fixed effects and random effects. For a model with only fixed effects, we can use the base R function `glm(, family = binomial)`.[`lme4::glmer()` and `glm()` have counterparts for linear regression: `lme4::lmer()` and `lm()`, for mixed and fixed effects, respectively.]{.aside}\n\nA fixed effect is a predictor that we are interested in, such as `possessor_type` and `size_possessor`. We want to know their effect on the response variable and we expect the values attested in our sample (our dataset) to represent the values that occur in the population (language in general). A random effect, in contrast, is a variable that we want to take into account but whose effect on the response variable we are not interested in. The values they take in the sample are only a sample of those that can be attested in the population, and might not be replicable in a different dataset. Examples in linguistic studies are the speaker, the topic and sometimes lexical effects. In our case, we can use the filename (`source`) and the register (`comp`) as a proxy for speakers and topic. We want to take them into account because it could be that a certain speaker prefers to use one variant over the other, but we cannot take them as fixed effects because it's unlikely that a study in a different dataset will have the same speakers. A different dataset, however, can certainly have the same values of `possessor_type` and comparable values in `size_possessor` and `size_possessed`.\n\n::: {.callout-note collapse=\"true\"}\n## Extended formula notation\n\nThe first argument of `lme4::glmer()` is a formula, indicating the response variable and the predictors, as used for `partykit::ctree()`.\n\n> `y ~ x1 + x2`: `y` in function of main effects `x1` and `x2`\n\nHowever, for mixed-effects models we can use an extended notation to refer to interactions and random effects.\n\nInteractions are cases where the effect of a variable depends on the effect on another variable. For example, that would be the case if a higher `size_possessor` favors *of* more intensely when `possessor_type` is *common* than when it is *proper*. It could also be the case that a higher `size_possessor` favors *of* when `possessor_type` is *common* but disfavors it when it is *proper*.\n\n> `y ~ x1 + x2 + x1:x2`: `y` in function of main effects `x1` and `x2` and their interaction\n>\n> `y ~ x1 * x2`: `x1 * x2` is short for `x1 + x2 + x1:x2`\n>\n> `y ~ (x1 + x2 + x3)^2`: `y` in function of main effects `x1`, `x2`, and `x3` and all their two-way interactions; short for `y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3`\n\nRandom effects include random intercepts and random slopes. A random intercept would represent, for this dataset, that different speakers have different baseline preferences for one variant or the other. A random slope for a given predictor, in contrast, indicates that the same predictor may have different effects for different speakers, e.g. that the effect size of `size_possessor` changes somewhat across speakers.\n\n> `y ~ x1 + (1 | r1)`: random intercepts for `r1`\n>\n> `y ~ x1 + (1 + x1 | r1)`: random intercepts and random `x1`-slopes for `r1`\n>\n> `y ~ x1 + (x1 | r1)`: same as (= short notation) previous formula\n\nRandom effects are always crossed but can also be nested, when the names of the levels make the nesting clear. In our dataset, `source` is nested within `comp` because the level names of `source` are unique across levels of `comp`: there is no file name that can belong to different registers. R is smart enough to realize this without us needing to make it explicit.\n\n> `y ~ x1 + (1 | r1) + (1 | r2)`: random intercepts for both `r1` and `r2`\n>\n> `y ~ x1 + (1 | r1) + (1 | r1:r2)`: explicit nesting of `r2` in `r1`; the analysis will treat the same-name levels of `r2` within different levels of `r1` as different levels.\n:::\n\n## Maximal model\n\nThe first model we fit is a maximal model, using all the predictors suggested by the conditional tree. Because complex mixed-effect logistic regression models often have a hard time trying to converge, we add a `lme4::glmerControl()` call to help it. Here we set the optimizer to \"bobyqa\", but we could also increase the number of iteration with the argument `optCtr = list(maxfun = 100000)`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncd_glmer <- glmer(\n    gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +\n               (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd,\n    control = glmerControl(optimizer = \"bobyqa\")\n  )\nsummary(cd_glmer) %>%\n  print(correlation = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +  \n    (1 | comp) + (1 | source)\n   Data: cd\nControl: glmerControl(optimizer = \"bobyqa\")\n\n     AIC      BIC   logLik deviance df.resid \n    7879     7941    -3931     7861     7335 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.630 -0.597 -0.266  0.622  5.265 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n source (Intercept) 0.793    0.891   \n comp   (Intercept) 0.304    0.552   \nNumber of obs: 7344, groups:  source, 499; comp, 15\n\nFixed effects:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                          -0.6953     0.5311   -1.31    0.190    \npossessor_typeproper                  2.5731     0.3299    7.80  6.2e-15 ***\nsize_possessor                       -0.4486     0.2510   -1.79    0.074 .  \nsize_possessed                        0.4472     0.2329    1.92    0.055 .  \npossessor_typeproper:size_possessor   0.0369     0.1240    0.30    0.766    \npossessor_typeproper:size_possessed  -0.2811     0.1130   -2.49    0.013 *  \nsize_possessor:size_possessed        -0.1228     0.1127   -1.09    0.276    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nFor the interpretation of the model, we want to look at:\n\n-   The AIC value: the lower, the better. We can use it to compare models to each other.\n\n-   The variance of the random effects: if it is not 0, it means we should keep them.\n\n-   The p-values and estimates of the intercept and the effects. We do not care about non-significant effects, so we might want to try a new model without those predictors.\n\nAn useful way to check the significance of a predictor on the model is to run different models, each of which has removed one predictor, and compare them to the model with all the predictors. With the likelihood ratio test (LRT) we can see if removing anything from the model makes it significantly worse. We will do this with one handy function in the next section.\n\n::: callout-caution\n### Selection of predictors\n\nWhile p-values and LRT tests are useful statistical guides when selecting or discarding predictors, your selection may also be theoretically motivated. Regardless of these values, if a certain predictor was significant in previous research, you can decide to keep it in your models.\n:::\n\n## Removing one predictor\n\nThe function `afex::mixed()` can take a model, run different iterations in which one predictor was removed, and compare if the model without a predictor is significantly worse than the model with all predictors. For that comparison we'll use likelihood ratio (LRT).[In linear models we would use an F-test instead.]{.aside}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncd_glmer_lrt <- mixed(\n    gen_type ~ (possessor_type + size_possessor + size_possessed)^2 +\n               (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd, \n    method = \"LRT\",\n    control = glmerControl(optimizer = \"bobyqa\")\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nContrasts set to contr.sum for the following variables: gen_type, possessor_type, comp, source\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nNumerical variables NOT centered on 0: size_possessor, size_possessed\nIf in interactions, interpretation of lower order (e.g., main) effects difficult.\n```\n:::\n:::\n\n\n::: callout-note\nThe call to `mixed()` sends us a warning about the distribution of our numerical variables. This is useful but we don't need to heed it as long as we don't try to interpret the main effects of variables in interactions.\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(cd_glmer_lrt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMixed Model Anova Table (Type 3 tests, LRT-method)\n\nModel: gen_type ~ (possessor_type + size_possessor + size_possessed)^2 + \nModel:     (1 | comp) + (1 | source)\nData: cd\nDf full model: 9\n                         Effect df     Chisq p.value\n1                possessor_type  1 61.26 ***   <.001\n2                size_possessor  1    3.14 +    .076\n3                size_possessed  1      1.72    .190\n4 possessor_type:size_possessor  1      0.09    .765\n5 possessor_type:size_possessed  1    6.13 *    .013\n6 size_possessor:size_possessed  1      1.18    .278\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe output confirms that `possessor_type` and its interaction with `size_possessed` are significant. We can interpret as well that the other two interactions are not significant and must be dropped. However, we don't know if the main effect of `size_possessor` is significant, because it's being distorted by the interactions in the model. Therefore, we will try a new model with `possessor_type`, `size_possessor`, `size_possessed` and the interaction `possessor_type:size_possessed`.\n\n::: callout-caution\n### Random effects and variable selection\n\nIf you model an interaction, you also have to keep the main effects of the predictors that participate in the interaction.\n:::\n\n## Second model\n\nSince the new model is not as complex, we don't need `glmerControl()` to help it converge.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncd_glmer2 <- glmer(\n    gen_type ~ possessor_type + size_possessor + size_possessed +\n      possessor_type:size_possessed +\n      (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd\n  )\n\nsummary(cd_glmer2) %>%\n  print(correlation = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nGeneralized linear mixed model fit by maximum likelihood (Laplace\n  Approximation) [glmerMod]\n Family: binomial  ( logit )\nFormula: gen_type ~ possessor_type + size_possessor + size_possessed +  \n    possessor_type:size_possessed + (1 | comp) + (1 | source)\n   Data: cd\n\n     AIC      BIC   logLik deviance df.resid \n    7877     7925    -3931     7863     7337 \n\nScaled residuals: \n   Min     1Q Median     3Q    Max \n-4.632 -0.597 -0.267  0.621  5.332 \n\nRandom effects:\n Groups Name        Variance Std.Dev.\n source (Intercept) 0.794    0.891   \n comp   (Intercept) 0.306    0.553   \nNumber of obs: 7344, groups:  source, 499; comp, 15\n\nFixed effects:\n                                    Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                          -0.2284     0.2478   -0.92   0.3567    \npossessor_typeproper                  2.6650     0.2392   11.14   <2e-16 ***\nsize_possessor                       -0.6880     0.0625  -11.01   <2e-16 ***\nsize_possessed                        0.2055     0.0759    2.71   0.0068 ** \npossessor_typeproper:size_possessed  -0.2893     0.1120   -2.58   0.0098 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nIn this model, the AIC value is slightly lower (so, the model is better) and the variance of the random effects is larger than zero. In addition, all the current predictors and the interaction are significant.\n\nWe could still run `afex::mixed()` to check whether removing any predictor would make the model worse.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncd_glmer2_lrt <- mixed(\n    gen_type ~ possessor_type + size_possessor + size_possessed +\n      possessor_type:size_possessed +\n      (1 | comp) + (1 | source),\n    family = \"binomial\", data = cd,\n    method = \"LRT\")\nprint(cd_glmer2_lrt)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMixed Model Anova Table (Type 3 tests, LRT-method)\n\nModel: gen_type ~ possessor_type + size_possessor + size_possessed + \nModel:     possessor_type:size_possessed + (1 | comp) + (1 | source)\nData: cd\nDf full model: 7\n                         Effect df      Chisq p.value\n1                possessor_type  1 126.10 ***   <.001\n2                size_possessor  1 125.08 ***   <.001\n3                size_possessed  1       1.11    .292\n4 possessor_type:size_possessed  1     6.62 *    .010\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1\n```\n:::\n:::\n\n\nIndeed, the only predictor that we could \"safely\" remove is `size_possessed`, but we need it in order to keep the significant interaction in which it participates.\n\n## Concordance index C\n\nIn order to assess the strength of the predictions of the models, we can use the concordance index C, computed with `Hmisc::somers2()`.\nThe first argument is the fitted values of the models (obtained with `predict()`), whereas the second argument is a numerical form of the observed outcomes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# maximal model\nHmisc::somers2(\n  predict(cd_glmer), as.numeric(cd$gen_type)-1\n)[\"C\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     C \n0.8494 \n```\n:::\n\n```{.r .cell-code}\n# minimal model\nHmisc::somers2(\n  predict(cd_glmer2), as.numeric(cd$gen_type)-1\n)[\"C\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     C \n0.8494 \n```\n:::\n\n```{.r .cell-code}\n# minimal model without random effects in the prediction\nHmisc::somers2(\n  predict(cd_glmer2, re.form = NA), as.numeric(cd$gen_type)-1\n)[\"C\"]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     C \n0.7511 \n```\n:::\n:::\n\n\n::: callout-warning\n### Random effects and `{tidymodels}`\n\nWith basic `predict()` it's possible to obtain the predictions both taking the random effects into account and excluding them. By default, they are taken into account.\n\nIf you use `{tidymodels}` for modelling, this will not be possible and you will have to take some differences into account:\n\n- The predicted value is the reference value.\n\n- There is no \"concordance index C\" measure in `{yardstick}`, the `{tidymodels}` package for evaluation; instead, there is a metric computing \"the area under the ROC curve\" (`yardstick::roc_auc()`), which is the same thing!\n\n- Predictions are always made based on fixed effects only (since the values of your random effects are not supposed to be present in the data you're predicting).\n:::\n\n## Interpretation\n\n\n\n\n\nOne of the first things we see in the output is that, when the Possessor is a proper noun and the Possesses slot has one character, the chances of the *s*-variant are significantly higher --- about 14.3677 times more likely than the alternative, all other things being equal.\n\nMoreover the larger the Possessed constituent, the higher the chances of the *s*-variant. In contrast, all other things being equal, the larger the Possessor constituent, the lower the chances of the *s*-variant. This is consistent with the theoretical assumption that the longer constituent is placed at the end: the longer a constituent, the higher the chances of a construction that places it at the end.\n\nHowever, that effect of the size of the Possessed constituent is valid when the Possessor slot is a common noun, which in general favors the *of* construction (as indicated by the Intercept, although it is not significant). In contrast, when the Possessor is a proper noun (which favors the *s*-construction, all other things being equal), a larger Possessed constituent actually doesn't favor the *s* construction as much.\n\n::: callout-tip\nYou can always retrieve the coefficients with `coefficients(summary(model))`, which is a dataframe. For example `coefficients(summary(cd_glmer2))[\"possessor_typeproper\",\"Estimate\"]` returns the estimate coefficient of `possessor_type`. Adding `exp()`, you can retrieve the odds of the *s*-variant over the *of*-variant when the Possessor is a proper noun, all other things being equal.\n:::\n\n## Plotting effects\n\n@fig-effects plots the effects in the models. @fig-effects-1 shows that the probabilities of the *s*-genitive decrease as the size of the Possessor constituent increases. These probabilities take the other predictors into account by assuming their typical cases.\n\n@fig-effects-2 illustrates the interaction between the type of the Possessor and the size of the Possessed constituent. As we saw in the printed output of the model, when the Possessor is a common noun, a larger size of the Possessed constituent increases the probabilities of the *s*-genitive, whereas the effect inverts when the Possessor is a proper noun. In addition, we see that the probability of the *s*-genitive is always higher for the proper noun, regardless of the size of the Possessed constituent.\n\n\n::: {#fig-effects .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\nggpredict(cd_glmer2, \"size_possessor\") %>%\n  plot()\n\nggpredict(cd_glmer2, c(\"size_possessed\", \"possessor_type\")) %>%\n  plot()\n```\n\n::: {.cell-output-display}\n![Effect of the size of the possessor slot.](genitive-alternation-analysis_files/figure-html/fig-effects-1.png){#fig-effects-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Interaction between size of the possessed slot and type of possessor.](genitive-alternation-analysis_files/figure-html/fig-effects-2.png){#fig-effects-2 width=672}\n:::\n\nEffect plots predicting the probability of the *s*-genitive.\n:::\n\n\n# Easystats {#easystats}\n\nIn this final section we will showcase some functions from the `{easystats}` group of packages for reporting models.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(easystats)\n```\n:::\n\n\n::: panel-tabset\n## Report\n\n::: callout-caution\n### Reporting random effects\n\nNotice that the `{merDeriv}` package needs to be installed to compute confidence intervals for random effect parameters.\n\nALSO: `report()` is printing twice as much text as it should for models with multiple parameters, be careful!\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport(cd_glmer2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a logistic mixed model (estimated using ML and Nelder-Mead optimizer)\nto predict gen_type with possessor_type (formula: gen_type ~ possessor_type +\nsize_possessor + size_possessed + possessor_type:size_possessed). The model\nincluded comp as random effects (formula: list(~1 | comp, ~1 | source)). The\nmodel's total explanatory power is substantial (conditional R2 = 0.41) and the\npart related to the fixed effects alone (marginal R2) is of 0.22. The model's\nintercept, corresponding to possessor_type = common, is at -0.23 (95% CI\n[-0.71, 0.26], p = 0.357). Within this model:\n\n  - The effect of possessor type [proper] is statistically significant and\npositive (beta = 2.66, 95% CI [2.20, 3.13], p < .001; Std. beta = 2.07, 95% CI\n[1.94, 2.20])\n  - The effect of size possessor is statistically significant and negative (beta\n= -0.69, 95% CI [-0.81, -0.57], p < .001; Std. beta = -0.35, 95% CI [-0.42,\n-0.29])\n  - The effect of size possessed is statistically significant and positive (beta\n= 0.21, 95% CI [0.06, 0.35], p = 0.007; Std. beta = 0.11, 95% CI [0.03, 0.19])\n  - The interaction effect of size possessed on possessor type [proper] is\nstatistically significant and negative (beta = -0.29, 95% CI [-0.51, -0.07], p\n= 0.010; Std. beta = -0.15, 95% CI [-0.27, -0.04])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation., We fitted a logistic mixed\nmodel (estimated using ML and Nelder-Mead optimizer) to predict gen_type with\nsize_possessor (formula: gen_type ~ possessor_type + size_possessor +\nsize_possessed + possessor_type:size_possessed). The model included comp as\nrandom effects (formula: list(~1 | comp, ~1 | source)). The model's total\nexplanatory power is substantial (conditional R2 = 0.41) and the part related\nto the fixed effects alone (marginal R2) is of 0.22. The model's intercept,\ncorresponding to size_possessor = 0, is at -0.23 (95% CI [-0.71, 0.26], p =\n0.357). Within this model:\n\n  - The effect of possessor type [proper] is statistically significant and\npositive (beta = 2.66, 95% CI [2.20, 3.13], p < .001; Std. beta = 2.07, 95% CI\n[1.94, 2.20])\n  - The effect of size possessor is statistically significant and negative (beta\n= -0.69, 95% CI [-0.81, -0.57], p < .001; Std. beta = -0.35, 95% CI [-0.42,\n-0.29])\n  - The effect of size possessed is statistically significant and positive (beta\n= 0.21, 95% CI [0.06, 0.35], p = 0.007; Std. beta = 0.11, 95% CI [0.03, 0.19])\n  - The interaction effect of size possessed on possessor type [proper] is\nstatistically significant and negative (beta = -0.29, 95% CI [-0.51, -0.07], p\n= 0.010; Std. beta = -0.15, 95% CI [-0.27, -0.04])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation. and We fitted a logistic\nmixed model (estimated using ML and Nelder-Mead optimizer) to predict gen_type\nwith size_possessed (formula: gen_type ~ possessor_type + size_possessor +\nsize_possessed + possessor_type:size_possessed). The model included comp as\nrandom effects (formula: list(~1 | comp, ~1 | source)). The model's total\nexplanatory power is substantial (conditional R2 = 0.41) and the part related\nto the fixed effects alone (marginal R2) is of 0.22. The model's intercept,\ncorresponding to size_possessed = 0, is at -0.23 (95% CI [-0.71, 0.26], p =\n0.357). Within this model:\n\n  - The effect of possessor type [proper] is statistically significant and\npositive (beta = 2.66, 95% CI [2.20, 3.13], p < .001; Std. beta = 2.07, 95% CI\n[1.94, 2.20])\n  - The effect of size possessor is statistically significant and negative (beta\n= -0.69, 95% CI [-0.81, -0.57], p < .001; Std. beta = -0.35, 95% CI [-0.42,\n-0.29])\n  - The effect of size possessed is statistically significant and positive (beta\n= 0.21, 95% CI [0.06, 0.35], p = 0.007; Std. beta = 0.11, 95% CI [0.03, 0.19])\n  - The interaction effect of size possessed on possessor type [proper] is\nstatistically significant and negative (beta = -0.29, 95% CI [-0.51, -0.07], p\n= 0.010; Std. beta = -0.15, 95% CI [-0.27, -0.04])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n```\n:::\n:::\n\n\n## Parameters\n\n\n::: {#tbl-parameters .cell tbl-cap='Fixed effect parameters.'}\n\n```{.r .cell-code}\nmodel_parameters(cd_glmer2, effects = \"fixed\") %>% \n  print_md()\n```\n\n::: {.cell-output-display}\nTable: Fixed Effects\n\n|Parameter                                | Log-Odds |   SE |         95% CI |      z |      p |\n|:----------------------------------------|:--------:|:----:|:--------------:|:------:|:------:|\n|(Intercept)                              |    -0.23 | 0.25 |  (-0.71, 0.26) |  -0.92 | 0.357  |\n|possessor type (proper)                  |     2.66 | 0.24 |   (2.20, 3.13) |  11.14 | < .001 |\n|size possessor                           |    -0.69 | 0.06 | (-0.81, -0.57) | -11.01 | < .001 |\n|size possessed                           |     0.21 | 0.08 |   (0.06, 0.35) |   2.71 | 0.007  |\n|possessor type (proper) * size possessed |    -0.29 | 0.11 | (-0.51, -0.07) |  -2.58 | 0.010  |\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_parameters(cd_glmer2, effects = \"fixed\") %>% \n  plot()\n```\n\n::: {.cell-output-display}\n![Coefficients of the fixed effect parameters.](genitive-alternation-analysis_files/figure-html/fig-parameters-1.png){#fig-parameters width=672}\n:::\n:::\n\n\n## Performance\n\nThe outliers can be further inspected with `cd %>% filter(check_outliers(cd_glmer2))`.\n\n\n::: {#fig-collinearity .cell layout-ncol=\"2\"}\n\n```{.r .cell-code}\ncheck_collinearity(cd_glmer2) %>% \n  plot()\n\ncheck_outliers(cd_glmer2) %>% plot()\n```\n\n::: {.cell-output-display}\n![VIF, or collinearity](genitive-alternation-analysis_files/figure-html/fig-collinearity-1.png){#fig-collinearity-1 width=672}\n:::\n\n::: {.cell-output-display}\n![Outliers](genitive-alternation-analysis_files/figure-html/fig-collinearity-2.png){#fig-collinearity-2 width=672}\n:::\n\nPerformance checks on minimal model\n:::\n\n\n## Comparison\n\nThe `{easystats}` packages also offer ways of comparing different models. `parameters::compare_models()` compares the effect size of the parameters, whereas `performance::compare_performance()` compares them across metrics. Both objects have `plot()` methods.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nparam_comparison <- compare_models(\"maximal\" = cd_glmer,\n                                   \"minimal\" = cd_glmer2)\nperformance_comparison <- compare_performance(\"maximal\" = cd_glmer,\n                                              \"minimal\" = cd_glmer2,\n                                              metrics = \"common\"\n                                              )\n```\n:::\n\n::: {#tbl-paramcomp .cell tbl-cap='Parameter comparison between maximal and minimal model.'}\n\n```{.r .cell-code}\nprint_md(param_comparison)\n```\n\n::: {.cell-output-display}\n|Parameter                                |                maximal|              minimal|\n|:----------------------------------------|----------------------:|--------------------:|\n|(Intercept)                              |    -0.70 (-1.74, 0.35)|  -0.23 (-0.71, 0.26)|\n|possessor type (proper)                  |      2.57 (1.93, 3.22)|    2.66 (2.20, 3.13)|\n|size possessor                           |    -0.45 (-0.94, 0.04)| -0.69 (-0.81, -0.57)|\n|size possessed                           | 0.45 (-9.35e-03, 0.90)|    0.21 (0.06, 0.35)|\n|possessor type (proper) * size possessed |   -0.28 (-0.50, -0.06)| -0.29 (-0.51, -0.07)|\n|possessor type (proper) * size possessor |     0.04 (-0.21, 0.28)|                     |\n|size possessor * size possessed          |    -0.12 (-0.34, 0.10)|                     |\n|                                         |                       |                     |\n|Observations                             |                   7344|                 7344|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(param_comparison)\n```\n\n::: {.cell-output-display}\n![Parameter comparison between maximal and minimal model.](genitive-alternation-analysis_files/figure-html/fig-paramcomp-1.png){#fig-paramcomp width=672}\n:::\n:::\n\n::: {#tbl-perfcomp .cell tbl-cap='Performance comparison between maximal and minimal model.'}\n\n```{.r .cell-code}\nprint_md(performance_comparison)\n```\n\n::: {.cell-output-display}\nTable: Comparison of Model Performance Indices\n\n|Name      |    Model | AIC (weights) |  BIC (weights) | R2 (cond.) | R2 (marg.) |  ICC | RMSE |\n|:---------|:--------:|:-------------:|:--------------:|:----------:|:----------:|:----:|:----:|\n|cd_glmer  | glmerMod | 7879.3 (0.21) | 7941.4 (<.001) |       0.41 |       0.22 | 0.25 | 0.40 |\n|cd_glmer2 | glmerMod | 7876.6 (0.79) | 7924.9 (>.999) |       0.41 |       0.22 | 0.25 | 0.40 |\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(performance_comparison)\n```\n\n::: {.cell-output-display}\n![Performance comparison between maximal and minimal model.](genitive-alternation-analysis_files/figure-html/fig-perfcomp-1.png){#fig-perfcomp width=672}\n:::\n:::\n\n:::\n",
    "supporting": [
      "genitive-alternation-analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\r\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}