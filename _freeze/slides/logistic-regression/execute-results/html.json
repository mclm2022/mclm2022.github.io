{
  "hash": "1830cc7765352c899936452473b0990d",
  "result": {
    "markdown": "---\ntitle: \"Regression analysis\"\nformat:\n  revealjs:\n    slide-number: true\n    show-slide-number: all\nexecute:\n  echo: true\n---\n\n\n## Linear and logistic regression {.smaller}\n\n\n\n\n\n| | Linear regression | Logistic regression |\n|----------------------|-------------------------|-------------------------|\n| Response variable | Numerical | Categorical |\n| Relationship between predictor estimate and response | Linear | Logistic (linear relationship between estimate and log odds of response) |\n| Fitting function | OSL (ordinary least squares) | MLE (maximum likelihood estimation) |\n| Model comparison | F-test  | AIC |\n| Evaluation metric | $R^2$ | $C$ |\n| Base R function | `lm()` | `glm()` |\n\n# Simple linear regression\n\n## Simple linear regression\n\nSimple\n\n:   one predictor variable\n\nLinear\n\n:   linear relation between estimated parameters and response variable\n\nNotation\n\n:   `y ~ x`\n\nEstimation: OSL\n\n:   Ordinary least squares, minimizing sum of squares of residuals\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|1-2|3|5-8|9-10|12-15\"}\nlibrary(ggplot2)\nlibrary(tibble)\nset.seed(2022)\n\nutt_lengths <- tibble(\n  age = 3:17,\n  utterance_length = 3 + 0.5*seq_along(age) + rnorm(length(age), sd = 0.3)\n)\nm <- lm(utterance_length ~ age, data = utt_lengths)\nutt_lengths$fit <- m$fitted.values\n\ng <- ggplot(utt_lengths, aes(x = age, y = utterance_length)) +\n  labs(x = \"Age\", y = \"Utterance length\") +\n  xlim(c(0,18)) + ylim(c(0,15)) +\n  theme_minimal(base_size = 14) + theme(aspect.ratio = 1)\n```\n:::\n\n\n## Plotted values\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- g + geom_point()\ng\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-3-1.png){width=960}\n:::\n:::\n\n\n## Adding fitted line\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- g + geom_line(aes(y = fit))\ng\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-4-1.png){width=960}\n:::\n:::\n\n\n## Residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- g + geom_segment(aes(xend = age, yend = fit))\ng\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-5-1.png){width=960}\n:::\n:::\n\n\n## Model output\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = utterance_length ~ age, data = utt_lengths)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.71722 -0.15356 -0.01602  0.17418  0.51755 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.69576    0.20355   8.331 1.43e-06 ***\nage          0.51891    0.01869  27.770 5.84e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3127 on 13 degrees of freedom\nMultiple R-squared:  0.9834,\tAdjusted R-squared:  0.9821 \nF-statistic: 771.2 on 1 and 13 DF,  p-value: 5.843e-13\n```\n:::\n:::\n\n\n# Easystats\n\n## Quick reports in text\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(report)\nreport(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a linear model (estimated using OLS) to predict utterance_length with\nage (formula: utterance_length ~ age). The model explains a statistically\nsignificant and substantial proportion of variance (R2 = 0.98, F(1, 13) =\n771.19, p < .001, adj. R2 = 0.98). The model's intercept, corresponding to age\n= 0, is at 1.70 (95% CI [1.26, 2.14], t(13) = 8.33, p < .001). Within this\nmodel:\n\n  - The effect of age is statistically significant and positive (beta = 0.52, 95%\nCI [0.48, 0.56], t(13) = 27.77, p < .001; Std. beta = 0.99, 95% CI [0.91,\n1.07])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald t-distribution approximation.\n```\n:::\n:::\n\n\n::: footer\n[{report} package](https://easystats.github.io/report/)\n:::\n\n## Quickly printing estimates\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(parameters)\nmodel_parameters(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter   | Coefficient |   SE |       95% CI | t(13) |      p\n----------------------------------------------------------------\n(Intercept) |        1.70 | 0.20 | [1.26, 2.14] |  8.33 | < .001\nage         |        0.52 | 0.02 | [0.48, 0.56] | 27.77 | < .001\n```\n:::\n\n```{.r .cell-code}\nprint_md(model_parameters(m))\n```\n\n::: {.cell-output-display}\n|Parameter   | Coefficient |   SE |       95% CI | t(13) |      p |\n|:-----------|:-----------:|:----:|:------------:|:-----:|:------:|\n|(Intercept) |        1.70 | 0.20 | (1.26, 2.14) |  8.33 | < .001 |\n|age         |        0.52 | 0.02 | (0.48, 0.56) | 27.77 | < .001 |\n:::\n:::\n\n\n::: footer\n[{parameters} package](https://easystats.github.io/parameters/)\n:::\n\n## Check and plot assumptions\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(performance)\ncheck_heteroscedasticity(m)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOK: Error variance appears to be homoscedastic (p = 0.237).\n```\n:::\n\n```{.r .cell-code}\ncheck_model(m, check = c(\"qq\", \"ncv\"))\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-9-1.png){width=960}\n:::\n:::\n\n\n::: footer\n[{performance} package](https://easystats.github.io/performance/)\n:::\n\n## Evaluate model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_performance(m) %>% print_md()\n```\n\n::: {.cell-output-display}\nTable: Indices of model performance\n\n|AIC   |   BIC |   R2 | R2 (adj.) | RMSE | Sigma |\n|:-----|:-----:|:----:|:---------:|:----:|:-----:|\n|11.54 | 13.67 | 0.98 |      0.98 | 0.29 |  0.31 |\n:::\n:::\n\n\n::: footer\n[{performance} package](https://easystats.github.io/performance/)\n:::\n\n# Multiple linear regression\n\n## Multiple linear regression\n\nMultiple\n\n:   more than one predictor\n\nNotation\n\n:   `y ~ x1 + x2` (fitting on a plane)\n\n:   `y ~ x1 + x2 + ... + xn` (fitting on a hyperplane)\n\nEstimation: OSL\n\n:   Ordinary least squares, minimizing sum of squares of residuals\n\n## Example\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n\nutt_lengths <- utt_lengths %>% \n  mutate(pages_read = 0 + 0.8*seq_along(age) + rnorm(length(age), sd = 1))\n\nm2 <- glm(utterance_length ~ age + pages_read, data = utt_lengths)\nutt_lengths$fit2 <- m2$fitted.values\nhead(utt_lengths) %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n| age| utterance_length|      fit| pages_read|     fit2|\n|---:|----------------:|--------:|----------:|--------:|\n|   3|         3.770043| 3.252489|   2.393078| 3.247895|\n|   4|         3.647996| 3.771397|   2.645829| 3.772950|\n|   5|         4.230754| 4.290305|   3.507192| 4.289808|\n|   6|         4.566650| 4.809213|   4.444730| 4.805639|\n|   7|         5.400696| 5.328121|   4.297667| 5.336080|\n|   8|         5.129811| 5.847029|   5.284364| 5.851249|\n:::\n:::\n\n\n## Example 3D\n\n\n\n\n```{ojs}\n//| echo: false\nplt = require(\"https://cdn.plot.ly/plotly-latest.min.js\")\npoints = [{x: data.age, y: data.pages_read, z: data.utterance_length,\ntype: \"scatter3d\", mode: \"markers\"}]\nsurface = [{x: data.age, y: data.pages_read, z: surface_fit,\ntype: \"surface\"}]\n```\n\n```{ojs}\n//| panel: input\n//| echo: false\nviewof traces = Inputs.checkbox(\n  [\n    {value: points[0], label: \"Observed values\"},\n    {value: surface[0], label: \"Fitted plane\"}\n  ],\n  {\n    value: [points[0], surface[0]],\n    label: \"Show:\",\n    format: (d) => d.label,\n    valueof: (d) => d.value\n  }\n)\n```\n\n```{ojs}\n//| echo: false\nplt.newPlot(DOM.element(\"div\"), traces, layout)\n```\n\n\n## Categorical predictors\n\n-   With 2 levels, e.g. \"monolingual family\": yes/no\n\n-   Translated into 1 **dummy** predictor with two levels\n\n    -   0: reference level, e.g. *no*\n\n    -   1: other level, e.g. *yes*\n\n-   Line fitted between 0 and 1, slope is the difference in `y` when *yes* (compared to *no*)\n\n## Categorical predictors\n\n-   With more than 2 levels, e.g. \"L1\": EN, Es, FR...\n\n-   Translated into *n*-1 **dummy** predictors with two levels\n\n    -   1: one of the levels, not the reference level, e.g. Es\n\n    -   0: the rest of the levels, including the reference level (e.g. EN)\n\n-   (Hyper)plane fitted between combinations of 0 and 1, slope is the difference in `y` when it is a level compared to the reference level.\n\n## Categorical predictors\n\n-   Comparison is done between each level and the reference level, not in all combinations\n\n-   t-test: does the slope of an individual dummy predictor differ from 0?\n\n-   F-test of nested models: do the dummy predictors jointly reduce unexplained variation?\n\n# Logistic regression\n\n## Logistic regression\n\nLogistic regression explains the probability of success (= a certain outcome)\n\nWe cannot fit a simple straight line <!-- TODO create plot -->\n\n## Probabilities, odds and logit {.smaller}\n\n| Value                                                 | Range              | Neutral value | Description                                                                                                     |\n|-----------------|-----------------|-----------------|---------------------|\n| probabilities <br> $P$                                | 0-1                | 0.5           | Number of successes divided by number of trials                                                                 |\n| odds <br> $\\frac{P}{1-P}$                             | 0-$\\infty$         | 1             | Probability of success divided by the probability of failure. <br> Undefined for $P=1$                          |\n| logit, log odds <br> $\\log\\left(\\frac{P}{1-P}\\right)$ | $-\\infty$-$\\infty$ | 0             | If positive, success is more likely; if negative failure is more likely. <br> Undefined for $P=0$ and for $P=1$ |\n\nHigher $P$ -\\> higher odds -\\> higher logit\n\n## Some examples\n\nWe'll create a vector `probabilities` with the values of fractions from $\\frac{1}{7}$ to $\\frac{1}{2}$ and then from $1-\\frac{1}{3}$ to $1-\\frac{1}{7}$.\n\n`MASS::fractions()` prints them as fractions.\n\nFrom there we compute odds and logit.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-line-numbers=\"|3,5|7|6,8|9\"}\nlibrary(MASS) # to print fractions\n\nprobabilities <- c(1/c(7:2), 1-(1/c(3:7)))\nprobs <- tibble(\n  P = probabilities,\n  P_frac = as.character(fractions(P)),\n  odds = P/(1-P),\n  odds_frac = as.character(fractions(odds)),\n  logit = log(odds)\n)\n```\n:::\n\n\n## Some examples {.smaller}\n\n\n::: {.cell}\n::: {.cell-output-display}\n|     P|P_frac |  odds|odds_frac |  logit|\n|-----:|:------|-----:|:---------|------:|\n| 0.143|1/7    | 0.167|1/6       | -1.792|\n| 0.167|1/6    | 0.200|1/5       | -1.609|\n| 0.200|1/5    | 0.250|1/4       | -1.386|\n| 0.250|1/4    | 0.333|1/3       | -1.099|\n| 0.333|1/3    | 0.500|1/2       | -0.693|\n| 0.500|1/2    | 1.000|1         |  0.000|\n| 0.667|2/3    | 2.000|2         |  0.693|\n| 0.750|3/4    | 3.000|3         |  1.099|\n| 0.800|4/5    | 4.000|4         |  1.386|\n| 0.833|5/6    | 5.000|5         |  1.609|\n| 0.857|6/7    | 6.000|6         |  1.792|\n:::\n:::\n\n\n## Plot code {visibility=\"hidden\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot <- function(df, x, y) {\n  ggplot(df, aes(x = {{ x }}, y = {{ y }})) +\n    geom_line() +\n    geom_point() + \n    theme_minimal(base_size = 20) +\n    theme(aspect.ratio = 1)\n}\n```\n:::\n\n\n## Probabilities, odds, logit\n\n::: panel-tabset\n## `logit ~ P`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot(probs, P, logit)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-19-1.png){width=960}\n:::\n:::\n\n\n## `odds ~ P`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot(probs, P, odds)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-20-1.png){width=960}\n:::\n:::\n\n:::\n\n## Linear/logistic\n\nLinear relation `logit ~ x` entails logistic curve `p ~ x`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith_x <- tibble(x = 1:30, logit = -3.5 + 0.3*x,\n                 odds = exp(logit), P = odds/(1+odds))\n```\n:::\n\n\n::: columns\n::: {.column widht=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot(with_x, x, logit)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-22-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column widht=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot(with_x, x, P)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-23-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Linear/logistic\n\nLinear relation `logit ~ x` entails logistic curve `p ~ x`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith_x2 <- tibble(x = 1:30, logit = 3.5 - 0.3*x,\n                 odds = exp(logit), P = odds/(1+odds))\n```\n:::\n\n\n::: columns\n::: {.column widht=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot(with_x2, x, logit)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-25-1.png){width=960}\n:::\n:::\n\n:::\n\n::: {.column widht=\"50%\"}\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot(with_x2, x, P)\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-26-1.png){width=960}\n:::\n:::\n\n:::\n:::\n\n## Example\n\n\n::: {.cell output-location='slide'}\n\n```{.r .cell-code  code-line-numbers=\"|3|4|6|7|8|9|12-13\"}\nset.seed(0)\nreading_habits <- tibble(\n  L1 = factor(rep(c(\"EN\", \"ES\"), each = 50), levels = c(\"EN\", \"ES\")),\n  age = rep(15:6, 10)) %>%\n  mutate(\n    logit = 0.2 -0.8*as.numeric(L1) + 0.2*age + rnorm(nrow(.)),\n    odds = exp(logit),\n    P = odds/(odds+1),\n    reads_EN = factor(P >= 0.5, c(\"FALSE\", \"TRUE\"))\n    )\n\nm3 <- glm(reads_EN ~ L1 + age, data = reading_habits, family = binomial(logit))\nreading_habits$fit <- m3$fitted.values\n\nreading_habits %>% head() %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n|L1 | age|     logit|       odds|         P|reads_EN |       fit|\n|:--|---:|---------:|----------:|---------:|:--------|---------:|\n|EN |  15|  3.662954| 38.9763201| 0.9749852|TRUE     | 0.9968727|\n|EN |  14|  1.873767|  6.5127816| 0.8668935|TRUE     | 0.9946208|\n|EN |  13|  3.329799| 27.9327340| 0.9654371|TRUE     | 0.9907623|\n|EN |  12|  3.072429| 21.5942985| 0.9557410|TRUE     | 0.9841801|\n|EN |  11|  2.014641|  7.4980384| 0.8823258|TRUE     | 0.9730353|\n|EN |  10| -0.139950|  0.8694017| 0.4650695|FALSE    | 0.9544032|\n:::\n:::\n\n\n## Visual exploration\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(reading_habits, aes(x = age, y = P, color = L1)) +\n  geom_point(size = 3, alpha = 0.6) +\n  scale_color_manual(values=c(\"#E69F00\", \"#56B4E9\")) +\n  theme_minimal(base_size = 20) +\n  labs(x = \"Age\", y = \"Probability of reading in English\")\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-28-1.png){width=960}\n:::\n:::\n\n\n## Default output\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = reads_EN ~ L1 + age, family = binomial(logit), \n    data = reading_habits)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.4851   0.1262   0.2338   0.5153   1.2084  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)  -2.4052     1.4574  -1.650  0.09888 . \nL1ES         -0.9353     0.7086  -1.320  0.18687   \nage           0.5446     0.1744   3.123  0.00179 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 73.385  on 99  degrees of freedom\nResidual deviance: 56.258  on 97  degrees of freedom\nAIC: 62.258\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\n# Easystats - logistic regression\n\n## Quick reports in text\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreport(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe fitted a logistic model (estimated using ML) to predict reads_EN with L1\n(formula: reads_EN ~ L1 + age). The model's explanatory power is moderate\n(Tjur's R2 = 0.19). The model's intercept, corresponding to L1 = EN, is at\n-2.41 (95% CI [-5.52, 0.31], p = 0.099). Within this model:\n\n  - The effect of L1 [ES] is statistically non-significant and negative (beta =\n-0.94, 95% CI [-2.43, 0.41], p = 0.187; Std. beta = -0.94, 95% CI [-2.43,\n0.41])\n  - The effect of age is statistically significant and positive (beta = 0.54, 95%\nCI [0.25, 0.94], p = 0.002; Std. beta = 1.57, 95% CI [0.71, 2.73])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation. and We fitted a logistic\nmodel (estimated using ML) to predict reads_EN with age (formula: reads_EN ~ L1\n+ age). The model's explanatory power is moderate (Tjur's R2 = 0.19). The\nmodel's intercept, corresponding to age = 0, is at -2.41 (95% CI [-5.52, 0.31],\np = 0.099). Within this model:\n\n  - The effect of L1 [ES] is statistically non-significant and negative (beta =\n-0.94, 95% CI [-2.43, 0.41], p = 0.187; Std. beta = -0.94, 95% CI [-2.43,\n0.41])\n  - The effect of age is statistically significant and positive (beta = 0.54, 95%\nCI [0.25, 0.94], p = 0.002; Std. beta = 1.57, 95% CI [0.71, 2.73])\n\nStandardized parameters were obtained by fitting the model on a standardized\nversion of the dataset. 95% Confidence Intervals (CIs) and p-values were\ncomputed using a Wald z-distribution approximation.\n```\n:::\n:::\n\n\n::: footer\n[{report} package](https://easystats.github.io/report/)\n:::\n\n## Quickly printing estimates\n\n::: panel-tabset\n## Table\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint_md(model_parameters(m3))\n```\n\n::: {.cell-output-display}\n|Parameter   | Log-Odds |   SE |        95% CI |     z |     p |\n|:-----------|:--------:|:----:|:-------------:|:-----:|:-----:|\n|(Intercept) |    -2.41 | 1.46 | (-5.52, 0.31) | -1.65 | 0.099 |\n|L1 (ES)     |    -0.94 | 0.71 | (-2.43, 0.41) | -1.32 | 0.187 |\n|age         |     0.54 | 0.17 |  (0.25, 0.94) |  3.12 | 0.002 |\n:::\n:::\n\n\n## Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model_parameters(m3))\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-32-1.png){width=960}\n:::\n:::\n\n:::\n\n::: footer\n[{parameters} package](https://easystats.github.io/parameters/)\n:::\n\n## Check and plot assumptions\n\n::: panel-tabset\n## Written output\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_collinearity(m3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Check for Multicollinearity\n\nLow Correlation\n\n Term  VIF       VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n   L1 1.01 [1.00, 62814.39]         1.01      0.99     [0.00, 1.00]\n  age 1.01 [1.00, 62814.39]         1.01      0.99     [0.00, 1.00]\n```\n:::\n:::\n\n\n## Plots\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_model(m3, check = c(\"binned_residuals\", \"vif\", \"outliers\"))\n```\n\n::: {.cell-output-display}\n![](logistic-regression_files/figure-revealjs/unnamed-chunk-34-1.png){width=960}\n:::\n:::\n\n:::\n\n::: footer\n[{performance} package](https://easystats.github.io/performance/)\n:::\n\n## Evaluate model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_performance(m3, metrics = \"common\") %>% \n  mutate(C = Hmisc::somers2(\n    reading_habits$fit,\n    as.numeric(reading_habits$reads_EN == \"TRUE\")\n  )[[\"C\"]]\n  ) %>% \n  print_md()\n```\n\n::: {.cell-output-display}\nTable: Indices of model performance\n\n|AIC   |   BIC | Tjur's R2 | RMSE |    C |\n|:-----|:-----:|:---------:|:----:|:----:|\n|62.26 | 70.07 |      0.19 | 0.29 | 0.84 |\n:::\n:::\n\n\n::: footer\n[{performance} package](https://easystats.github.io/performance/)\n:::\n\n# Extra code {visibility=\"uncounted\"}\n\n## Plot code {visibility=\"uncounted\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlnplot <- function(df, x, y) {\n  ggplot(df, aes(x = {{ x }}, y = {{ y }})) +\n    geom_line() +\n    geom_point() + \n    theme_minimal(base_size = 20) +\n    theme(aspect.ratio = 1)\n}\n```\n:::\n",
    "supporting": [
      "logistic-regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script type=\"ojs-define\">\r\n{\"contents\":[{\"name\":\"data\",\"value\":{\"age\":[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"utterance_length\":[3.77,3.648,4.2308,4.5666,5.4007,5.1298,6.1822,7.0834,7.7248,8.0725,8.8019,8.9445,9.2055,10.0279,10.4842],\"fit\":[3.2525,3.7714,4.2903,4.8092,5.3281,5.847,6.3659,6.8848,7.4038,7.9227,8.4416,8.9605,9.4794,9.9983,10.5172],\"pages_read\":[2.3931,2.6458,3.5072,4.4447,4.2977,5.2844,7.0285,6.1532,6.84,8.8465,10.194,9.8795,11.4164,10.8829,11.4254],\"fit2\":[3.2479,3.7729,4.2898,4.8056,5.3361,5.8512,6.3562,6.8965,7.4157,7.9171,8.4274,8.9601,9.4679,10.0035,10.5247]}}]}\r\n</script>\r\n<script type=\"ojs-define\">\r\n{\"contents\":[{\"name\":\"surface_fit\",\"value\":[[3.2479,3.7764,4.3048,4.8333,5.3617,5.8902,6.4187,6.9471,7.4756,8.004,8.5325,9.061,9.5894,10.1179,10.6463],[3.2445,3.7729,4.3014,4.8299,5.3583,5.8868,6.4152,6.9437,7.4722,8.0006,8.5291,9.0575,9.586,10.1145,10.6429],[3.2329,3.7613,4.2898,4.8183,5.3467,5.8752,6.4036,6.9321,7.4606,7.989,8.5175,9.0459,9.5744,10.1029,10.6313],[3.2222,3.7507,4.2792,4.8076,5.3361,5.8645,6.393,6.9215,7.4499,7.9784,8.5068,9.0353,9.5638,10.0922,10.6207],[3.2203,3.7487,4.2772,4.8056,5.3341,5.8626,6.391,6.9195,7.4479,7.9764,8.5049,9.0333,9.5618,10.0902,10.6187],[3.209,3.7374,4.2659,4.7943,5.3228,5.8512,6.3797,6.9082,7.4366,7.9651,8.4935,9.022,9.5505,10.0789,10.6074],[3.1972,3.7257,4.2542,4.7826,5.3111,5.8395,6.368,6.8965,7.4249,7.9534,8.4818,9.0103,9.5388,10.0672,10.5957],[3.188,3.7165,4.2449,4.7734,5.3018,5.8303,6.3588,6.8872,7.4157,7.9441,8.4726,9.0011,9.5295,10.058,10.5864],[3.1855,3.7139,4.2424,4.7708,5.2993,5.8278,6.3562,6.8847,7.4131,7.9416,8.4701,8.9985,9.527,10.0554,10.5839],[3.161,3.6894,4.2179,4.7464,5.2748,5.8033,6.3317,6.8602,7.3886,7.9171,8.4456,8.974,9.5025,10.0309,10.5594],[3.1471,3.6755,4.204,4.7324,5.2609,5.7894,6.3178,6.8463,7.3747,7.9032,8.4317,8.9601,9.4886,10.017,10.5455],[3.1428,3.6713,4.1997,4.7282,5.2567,5.7851,6.3136,6.842,7.3705,7.899,8.4274,8.9559,9.4843,10.0128,10.5413],[3.1335,3.662,4.1905,4.7189,5.2474,5.7758,6.3043,6.8328,7.3612,7.8897,8.4181,8.9466,9.4751,10.0035,10.532],[3.1264,3.6548,4.1833,4.7117,5.2402,5.7687,6.2971,6.8256,7.354,7.8825,8.411,8.9394,9.4679,9.9963,10.5248],[3.1262,3.6547,4.1832,4.7116,5.2401,5.7685,6.297,6.8255,7.3539,7.8824,8.4108,8.9393,9.4678,9.9962,10.5247]]}]}\r\n</script>\r\n<script type=\"ojs-define\">\r\n{\"contents\":[{\"name\":\"layout\",\"value\":{\"scene\":{\"xaxis\":{\"title\":\"Pages read\"},\"yaxis\":{\"title\":\"Age\"},\"zaxis\":{\"title\":\"Utterance length\"}},\"margin\":{\"l\":0,\"r\":0,\"t\":0,\"b\":0}}}]}\r\n</script>\r\n"
      ],
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n\r\n      // dispatch for htmlwidgets\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for reveal\r\n    if (window.Reveal) {\r\n      window.Reveal.addEventListener(\"slidechanged\", function(event) {\r\n        fireSlideChanged(event.previousSlide, event.currentSlide);\r\n      });\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}