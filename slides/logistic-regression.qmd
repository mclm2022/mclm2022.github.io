---
title: "Logistic regression analysis"
subtitle: "Methods of Corpus Linguistics (class 6)"
author: "Mariana Montes"
format:
  revealjs:
    preview-links: auto
    multiplex: true
    chalkboard:
        buttons: false
    code-fold: true
    css: styles.css
execute:
  echo: true
from: markdown+emoji
---

```{r}
#| label: setup
#| include: false
headings <- c("Introduction", "Probabilities, odds and logit", "Interpretation and evaluation", "Easystats") 
library(tidyverse)
library(easystats)
library(kableExtra)
options(digits = 3)
lnplot <- function(df, x, y, title) {
  ggplot(df, aes(x = {{ x }}, y = {{ y }})) +
    geom_line() +
    geom_point() + 
    theme_minimal(base_size = 20) +
    theme(aspect.ratio = 1) +
    labs(title = title)
}
kb_style <- function(df, position = "center") {
  df %>% head(10) %>% kbl() %>%
    kable_paper(full_width = FALSE, font_size = 22, position = position)
}
```

## Outline

```{r}
#| label: outline
#| results: asis
#| echo: false
for (heading in headings) {
  cat("-", heading, "\n")
}
i <- 1
```

# `r headings[[i]]`

## Set up code

```{r}
#| label: logistic-code
#| code-fold: false
set.seed(2022)
gonna <- tibble(gt = rep("gonna", 50), comp_length = rnorm(50, 6, 0.8))
going_to <- tibble(gt = rep("going_to", 50), comp_length = rnorm(50, 3, 1.2))
gt <- bind_rows(gonna, going_to) %>%
  mutate(
    comp_length = if_else(comp_length > 1, comp_length, 1),
    gt = fct_relevel(gt, "gonna"), gt_num = as.numeric(gt)-1
    )

m3 <- glm(gt ~ comp_length, data = gt, family = binomial(logit))
gt$fit <- m3$fitted.values

gonna_plot <- ggplot(gt, aes(x = comp_length, y = gt_num)) +
  geom_point(size = 3) +
  labs(x = "Length of complement", y = "Choice of construction") +
  scale_y_continuous(breaks = c(0, 1), limits = c(0, 1),
                     labels = c("gonna", "going to")) +
  theme_minimal(base_size = 20) + theme(aspect.ratio = 1)
```

::: {.footer}
`r headings[[i]]`
:::

## *gonna* dataset

```{r}
#| label: logistic-data
#| layout-ncol: 2
kb_style(filter(gt, gt == "gonna"), "float_left")
kb_style(filter(gt, gt == "going_to"), "left")
```

::: {.footer}
`r headings[[i]]`
:::

## Scatterplot

What is the relationship between the length of the complement and the choice of *going to/gonna*?

```{r}
#| label: logistic-plot
gonna_plot
```

::: {.footer}
`r headings[[i]]`
:::

## Linear fit {chalkboard-buttons="true"}

The relationship is not linear.

:::: {.columns}

::: {.column width="50%"}
```{r}
#| label: logistic-linear
#| fig-height: 8
gonna_plot + 
  geom_smooth(method = "lm", se = FALSE, color = "darkolivegreen4")
```
:::

::: {.column width="50%" .incremental}
- The residuals are not normally distributed.

- Not all values are possible (probabilities go between 0 and 1).
:::

::::

::: {.footer}
`r headings[[i]]`
:::

## Logistic fit

The fit has an S shape.

```{r}
#| label: logistic-fit
gonna_plot +
  geom_line(aes(y = fit), color = "goldenrod", size = 2)
```

::: {.footer}
`r headings[[i]]`
:::

## Model

```{r}
#| label: logistic-model
summary(m3)
```

::: notes
We can turn the intercept estimate into odds and probabilities (of success when all predictors are at 0).
In this case, when comp_length = 0, the probabilities of encountering "success" (going to) are `exp(11.645)/(1+exp(11.645))`, i.e. about 0.9999912.
For other estimates, we can go to odds but not on probabilities because the value also depends on other predictors (?) and it depends on the point on the curve.
Here, for each unit that comp_length increases, the ODDS of going to are lower (because the logit is
negative), i.e. you have to multiply the odds by 0.933 (and therefore it goes down).
In any case, you can compute it for single points too. This is why exp(estimate) gives you the ODDS RATIO,
i.e. the ratio between the odds in one case and in the other case.
:::

::: {.footer}
`r headings[[i]]`
:::

```{r}
#| label: add-heading1
#| include: false
i <- i+1
```

# `r headings[[i]]`

## Probabilities

```{r}
#| label: proboddslog
#| include: false
pol <- tribble(
  ~"Value", ~"Minimum", ~"Neutral value", ~"Maximum", ~"Description",
  r"(probabilities $P$)", "0", "0.5", "1", "Number of successes divided by number of trials.", 
  r"(odds $\frac{P}{1-P}$)", "0", "1", r"($\infty$)", r"(Probability of success divided by the probability of failure. Undefined for $P=1$.)",
  r"[logit, log odds $\log\left(\frac{P}{1-P}\right)$]", r"($-\infty$)", "0", r"($\infty$)", r"(If positive, success is more likely; if negative failure is more likely. Undefined for $P=0$ and for $P=1$.)"
  )
pol_kbl <- function(pol_sub) {
  pol_sub %>% 
    kbl() %>% kable_paper(font_size = 25) %>% 
    column_spec(1, width = "4cm", border_right = TRUE) %>% 
    column_spec(2:4, width = "3cm") %>% 
    column_spec(5, width = "7cm", border_left = TRUE)
}
```

```{r}
#| label: probs
#| echo: false
pol_kbl(slice_head(pol, n=1))
```

::: {.footer}
`r headings[[i]]`
:::

## Odds

```{r}
#| label: odds
#| echo: false
pol_kbl(slice_head(pol, n=2))
```

::: {.footer}
`r headings[[i]]`
:::

## Logit

```{r}
#| label: logit
#| echo: false
pol_kbl(pol)
```

. . .

::: callout-tip
### Same direction!
Higher $P$ &rarr; higher odds &rarr; higher logit
:::

::: {.footer}
`r headings[[i]]`
:::

## Simulation

```{r}
#| code-line-numbers: "|3,4,6|8|7,9|10"
#| output-location: column
library(MASS) # to print fractions

probabilities <- c(1/c(7:2),
                   1-(1/c(3:7)))
probs <- tibble(
  P = probabilities,
  P_frac = as.character(fractions(P)),
  odds = P/(1-P),
  odds_frac = as.character(fractions(odds)),
  logit = log(odds)
)
kbl(probs) %>% kable_paper(font_size = 22) %>% 
  row_spec(6, bold = TRUE)
```

::: notes
We'll create a vector `probabilities` with the values of fractions from $\frac{1}{7}$ to $\frac{1}{2}$ and then from $1-\frac{1}{3}$ to $1-\frac{1}{7}$.

`MASS::fractions()` prints them as fractions.

From there we compute odds and logit.
:::

::: {.footer}
`r headings[[i]]`
:::

## Probabilities, odds and logits: plots {visibility="hidden"}

:::: {.columns}

::: {.column width="50%"}
```{r}
#| fig-height: 8
lnplot(probs, P, logit, "logit ~ P")
```
:::

::: {.column width="50%"}
```{r}
#| fig-height: 8
lnplot(probs, P, odds, "odds ~ P")
```
:::

::::

::: {.footer}
`r headings[[i]]`
:::

## Linear vs logistic relationships

Linear relation `logit ~ x` entails logistic curve `p ~ x`.

```{r}
with_x <- tibble(x = 1:30, logit = -3.5 + 0.3*x, odds = exp(logit), P = odds/(1+odds))
```

:::: columns
::: {.column width="50%"}
```{r}
#| fig-height: 8
lnplot(with_x, x, logit, "logit ~ X")
```
:::

::: {.column width="50%"}
```{r}
#| fig-height: 8
lnplot(with_x, x, P, "P ~ X")
```
:::
::::

::: {.footer}
`r headings[[i]]`
:::

```{r}
#| label: add-heading2
#| include: false
i <- i+1
```

# `r headings[[i]]`

## Model

```{r}
#| label: logistic-model2
summary(m3)
```

::: {.footer}
`r headings[[i]]`
:::

## Deviance {.smaller}

Null deviance

:   discrepancy between data and intercept-only model (like SST)

Residual deviance

:   discrepancy between data and fitted model (like SSE)

::: callout-note
### Computation

$-2 \log(L)$ with $L$ the likelihood of encountering the data if the model is true.
:::

AIC (Akaike Information Criterion)

:   corrected residual deviance (to compare models with different N. of predictors)

Deviance residuals

:   contribution of each observation to the residual deviance.

::: {.footer}
`r headings[[i]]`
:::

## Measures of predictive power {.smaller}

:::: {.columns}

::: {.column width="50%"}
We take all pairs of observations in which the response variable is different (one of *gonna*, one of *going to*).

- `gt0` is what the model predicted for the "gonna" element.

- `gt1` is what the model predicted for the "going to" element.
:::

::: {.column width="50%"}
```{r}
#| label: pred-power
set.seed(2022)
pred_power <- tibble(
  gt0 = sample(filter(gt, gt == "gonna")$fit, 10, replace = FALSE),
  gt1 = sample(filter(gt, gt == "going_to")$fit, 10, replace = FALSE))
kb_style(pred_power)
```
:::

::::


::: {.footer}
`r headings[[i]]`
:::

## C, D, T

:::: {.columns}

::: {.column width="50%"}
- `C` = concordant pair = prediction is higher for "going to" :grin:

- `D` = discordant pair = the prediction is higher for "gonna" :frowning_face:

- `T` = tie = the prediction is the same for both :neutral_face:
:::

::: {.column width="50%"}
```{r}
#| label: pred-power-tbl
pred_power <- pred_power %>%
  mutate(C = gt1 > gt0, D = gt0 > gt1, T = gt1 == gt0)
kb_style(pred_power)
```
:::

::::

::: {.footer}
`r headings[[i]]`
:::

## C-value

```{r}
#| label: pred-power-sum
pred_power <- pred_power %>%
  summarize(C = sum(C), D = sum(D), T = sum(T))
kb_style(pred_power)
```

. . .

$$c = \frac{C+T/2}{C+D+T} = \frac{`r pred_power$C`+`r pred_power$T`/2}{`r pred_power$C`+`r pred_power$D`+`r pred_power$T`} = `r (pred_power$C+pred_power$T/2)/sum(pred_power)`$$

. . .

```{r}
#| label: somers-c
Hmisc::somers2(gt$fit, gt$gt_num)[["C"]]
```

::: {.footer}
`r headings[[i]]`
:::


## Linear vs logistic regression {.smaller}

| | Linear regression | Logistic regression |
|----------------------|-------------------------|-------------------------|
| Response variable | Numerical | Categorical |
| Relationship between predictor estimate and response | Linear | Logistic^[Linear relationship between estimate and log odds of response.] |
| Fitting function | OSL (ordinary least squares) | MLE (maximum likelihood estimation) |
| Model comparison | F-test  | AIC |
| Evaluation metric | $R^2$ | $C$ |
| Base R function | `lm()` | `glm()` |

::: {.footer}
`r headings[[i]]`
:::

## There is more!

With the case study we'll also see:

- Mixed effect models (with random effects)

- Conditional inference trees and random forests

::: {.footer}
`r headings[[i]]`
:::

```{r}
#| label: add-heading4
#| include: false
i <- i+1
```

# `r headings[[i]]`

## {report} package {.smaller}

```{r}
#| label: report-3
report(m3)
```

::: {.footer}
`r headings[[i]]`
:::

## {parameters} package {.smaller}

```{r}
#| label: parameters-3
#| layout-ncol: 2
print_md(model_parameters(m3))

plot(model_parameters(m3))
```

::: {.footer}
`r headings[[i]]`
:::

## {performance} package {.smaller}

```{r}
#| label: performance-3
print_md(model_performance(m3))
```

::: {.footer}
`r headings[[i]]`
:::

<!-- TODO what checks do we need? -->

# Next: Correspondence Analysis